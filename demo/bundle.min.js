/** @type {import('../src/types.d.ts').ParquetType[]} */
const ParquetType = [
  'BOOLEAN',
  'INT32',
  'INT64',
  'INT96', // deprecated
  'FLOAT',
  'DOUBLE',
  'BYTE_ARRAY',
  'FIXED_LEN_BYTE_ARRAY',
];

const Encoding = [
  'PLAIN',
  undefined,
  'PLAIN_DICTIONARY',
  'RLE',
  'BIT_PACKED', // deprecated
  'DELTA_BINARY_PACKED',
  'DELTA_LENGTH_BYTE_ARRAY',
  'DELTA_BYTE_ARRAY',
  'RLE_DICTIONARY',
  'BYTE_STREAM_SPLIT',
];

const FieldRepetitionType = [
  'REQUIRED',
  'OPTIONAL',
  'REPEATED',
];

/** @type {import('../src/types.d.ts').ConvertedType[]} */
const ConvertedType = [
  'UTF8',
  'MAP',
  'MAP_KEY_VALUE',
  'LIST',
  'ENUM',
  'DECIMAL',
  'DATE',
  'TIME_MILLIS',
  'TIME_MICROS',
  'TIMESTAMP_MILLIS',
  'TIMESTAMP_MICROS',
  'UINT_8',
  'UINT_16',
  'UINT_32',
  'UINT_64',
  'INT_8',
  'INT_16',
  'INT_32',
  'INT_64',
  'JSON',
  'BSON',
  'INTERVAL',
];

const CompressionCodec = [
  'UNCOMPRESSED',
  'SNAPPY',
  'GZIP',
  'LZO',
  'BROTLI',
  'LZ4',
  'ZSTD',
  'LZ4_RAW',
];

/** @type {import('../src/types.d.ts').PageType[]} */
const PageType = [
  'DATA_PAGE',
  'INDEX_PAGE',
  'DICTIONARY_PAGE',
  'DATA_PAGE_V2',
];

const dayMillis = 86400000; // 1 day in milliseconds

/**
 * Convert known types from primitive to rich, and dereference dictionary.
 *
 * @import {DecodedArray, Encoding, SchemaElement} from '../src/types.d.ts'
 * @param {DecodedArray} data series of primitive types
 * @param {DecodedArray | undefined} dictionary
 * @param {SchemaElement} schemaElement
 * @param {Encoding} encoding
 * @param {boolean | undefined} utf8 decode bytes as utf8?
 * @returns {DecodedArray} series of rich types
 */
function convertWithDictionary(data, dictionary, schemaElement, encoding, utf8 = true) {
  if (dictionary && encoding.endsWith('_DICTIONARY')) {
    // convert dictionary
    dictionary = convert(dictionary, schemaElement, utf8);
    let output = data;
    if (data instanceof Uint8Array && !(dictionary instanceof Uint8Array)) {
      // @ts-expect-error upgrade data to match dictionary type with fancy constructor
      output = new dictionary.constructor(data.length);
    }
    for (let i = 0; i < data.length; i++) {
      output[i] = dictionary[data[i]];
    }
    return output
  } else {
    return convert(data, schemaElement, utf8)
  }
}

/**
 * Convert known types from primitive to rich.
 *
 * @param {DecodedArray} data series of primitive types
 * @param {SchemaElement} schemaElement
 * @param {boolean | undefined} utf8 decode bytes as utf8?
 * @returns {DecodedArray} series of rich types
 */
function convert(data, schemaElement, utf8 = true) {
  const ctype = schemaElement.converted_type;
  if (ctype === 'DECIMAL') {
    const scale = schemaElement.scale || 0;
    const factor = Math.pow(10, -scale);
    const arr = new Array(data.length);
    for (let i = 0; i < arr.length; i++) {
      if (data[0] instanceof Uint8Array) {
        arr[i] = parseDecimal(data[i]) * factor;
      } else {
        arr[i] = Number(data[i]) * factor;
      }
    }
    return arr
  }
  if (ctype === undefined && schemaElement.type === 'INT96') {
    return Array.from(data).map(parseInt96Date)
  }
  if (ctype === 'DATE') {
    const arr = new Array(data.length);
    for (let i = 0; i < arr.length; i++) {
      arr[i] = new Date(data[i] * dayMillis);
    }
    return arr
  }
  if (ctype === 'TIMESTAMP_MILLIS') {
    const arr = new Array(data.length);
    for (let i = 0; i < arr.length; i++) {
      arr[i] = new Date(Number(data[i]));
    }
    return arr
  }
  if (ctype === 'TIMESTAMP_MICROS') {
    const arr = new Array(data.length);
    for (let i = 0; i < arr.length; i++) {
      arr[i] = new Date(Number(data[i] / 1000n));
    }
    return arr
  }
  if (ctype === 'JSON') {
    const decoder = new TextDecoder();
    return data.map(v => JSON.parse(decoder.decode(v)))
  }
  if (ctype === 'BSON') {
    throw new Error('parquet bson not supported')
  }
  if (ctype === 'INTERVAL') {
    throw new Error('parquet interval not supported')
  }
  if (ctype === 'UTF8' || utf8 && schemaElement.type === 'BYTE_ARRAY') {
    const decoder = new TextDecoder();
    const arr = new Array(data.length);
    for (let i = 0; i < arr.length; i++) {
      arr[i] = data[i] && decoder.decode(data[i]);
    }
    return arr
  }
  if (ctype === 'UINT_64') {
    const arr = new BigUint64Array(data.length);
    for (let i = 0; i < arr.length; i++) {
      arr[i] = BigInt(data[i]);
    }
    return arr
  }
  if (schemaElement.logical_type?.type === 'FLOAT16') {
    return Array.from(data).map(parseFloat16)
  }
  if (schemaElement.logical_type?.type === 'TIMESTAMP') {
    const { unit } = schemaElement.logical_type;
    let factor = 1n;
    if (unit === 'MICROS') factor = 1000n;
    if (unit === 'NANOS') factor = 1000000n;
    const arr = new Array(data.length);
    for (let i = 0; i < arr.length; i++) {
      arr[i] = new Date(Number(data[i] / factor));
    }
    return arr
  }
  return data
}

/**
 * @param {Uint8Array} bytes
 * @returns {number}
 */
function parseDecimal(bytes) {
  // TODO: handle signed
  let value = 0;
  for (const byte of bytes) {
    value = value << 8 | byte;
  }
  return value
}

/**
 * @param {bigint} value
 * @returns {Date}
 */
function parseInt96Date(value) {
  const days = Number((value >> 64n) - 2440588n);
  const nano = Number((value & 0xffffffffffffffffn) / 1000000n);
  const millis = days * dayMillis + nano;
  return new Date(millis)
}

/**
 * @param {Uint8Array | undefined} bytes
 * @returns {number | undefined}
 */
function parseFloat16(bytes) {
  if (!bytes) return undefined
  const int16 = bytes[1] << 8 | bytes[0];
  const sign = int16 >> 15 ? -1 : 1;
  const exp = int16 >> 10 & 0x1f;
  const frac = int16 & 0x3ff;
  if (exp === 0) return sign * Math.pow(2, -14) * (frac / 1024) // subnormals
  if (exp === 0x1f) return frac ? NaN : sign * Infinity
  return sign * Math.pow(2, exp - 15) * (1 + frac / 1024)
}

/**
 * Build a tree from the schema elements.
 *
 * @import {SchemaElement, SchemaTree} from '../src/types.d.ts'
 * @param {SchemaElement[]} schema
 * @param {number} rootIndex index of the root element
 * @param {string[]} path path to the element
 * @returns {SchemaTree} tree of schema elements
 */
function schemaTree(schema, rootIndex, path) {
  const element = schema[rootIndex];
  const children = [];
  let count = 1;

  // Read the specified number of children
  if (element.num_children) {
    while (children.length < element.num_children) {
      const childElement = schema[rootIndex + count];
      const child = schemaTree(schema, rootIndex + count, [...path, childElement.name]);
      count += child.count;
      children.push(child);
    }
  }

  return { count, element, children, path }
}

/**
 * Get schema elements from the root to the given element name.
 *
 * @param {SchemaElement[]} schema
 * @param {string[]} name path to the element
 * @returns {SchemaTree[]} list of schema elements
 */
function getSchemaPath(schema, name) {
  let tree = schemaTree(schema, 0, []);
  const path = [tree];
  for (const part of name) {
    const child = tree.children.find(child => child.element.name === part);
    if (!child) throw new Error(`parquet schema element not found: ${name}`)
    path.push(child);
    tree = child;
  }
  return path
}

/**
 * Get the max repetition level for a given schema path.
 *
 * @param {SchemaTree[]} schemaPath
 * @returns {number} max repetition level
 */
function getMaxRepetitionLevel(schemaPath) {
  let maxLevel = 0;
  for (const { element } of schemaPath) {
    if (element.repetition_type === 'REPEATED') {
      maxLevel++;
    }
  }
  return maxLevel
}

/**
 * Get the max definition level for a given schema path.
 *
 * @param {SchemaTree[]} schemaPath
 * @returns {number} max definition level
 */
function getMaxDefinitionLevel(schemaPath) {
  let maxLevel = 0;
  for (const { element } of schemaPath.slice(1)) {
    if (element.repetition_type !== 'REQUIRED') {
      maxLevel++;
    }
  }
  return maxLevel
}

/**
 * Check if a column is list-like.
 *
 * @param {SchemaTree} schema
 * @returns {boolean} true if list-like
 */
function isListLike(schema) {
  if (!schema) return false
  if (schema.element.converted_type !== 'LIST') return false
  if (schema.children.length > 1) return false

  const firstChild = schema.children[0];
  if (firstChild.children.length > 1) return false
  if (firstChild.element.repetition_type !== 'REPEATED') return false

  return true
}

/**
 * Check if a column is map-like.
 *
 * @param {SchemaTree} schema
 * @returns {boolean} true if map-like
 */
function isMapLike(schema) {
  if (!schema) return false
  if (schema.element.converted_type !== 'MAP') return false
  if (schema.children.length > 1) return false

  const firstChild = schema.children[0];
  if (firstChild.children.length !== 2) return false
  if (firstChild.element.repetition_type !== 'REPEATED') return false

  const keyChild = firstChild.children.find(child => child.element.name === 'key');
  if (keyChild?.element.repetition_type === 'REPEATED') return false

  const valueChild = firstChild.children.find(child => child.element.name === 'value');
  if (valueChild?.element.repetition_type === 'REPEATED') return false

  return true
}

// TCompactProtocol types
const CompactType = {
  STOP: 0,
  TRUE: 1,
  FALSE: 2,
  BYTE: 3,
  I16: 4,
  I32: 5,
  I64: 6,
  DOUBLE: 7,
  BINARY: 8,
  LIST: 9,
  SET: 10,
  MAP: 11,
  STRUCT: 12,
  UUID: 13,
};

/**
 * Parse TCompactProtocol
 *
 * @import {DataReader} from '../src/types.d.ts'
 * @param {DataReader} reader
 * @returns {Record<string, any>}
 */
function deserializeTCompactProtocol(reader) {
  let lastFid = 0;
  /** @type {Record<string, any>} */
  const value = {};

  while (reader.offset < reader.view.byteLength) {
    // Parse each field based on its type and add to the result object
    const [type, fid, newLastFid] = readFieldBegin(reader, lastFid);
    lastFid = newLastFid;

    if (type === CompactType.STOP) {
      break
    }

    // Handle the field based on its type
    value[`field_${fid}`] = readElement(reader, type);
  }

  return value
}

/**
 * Read a single element based on its type
 *
 * @param {DataReader} reader
 * @param {number} type
 * @returns {any} value
 */
function readElement(reader, type) {
  switch (type) {
  case CompactType.TRUE:
    return true
  case CompactType.FALSE:
    return false
  case CompactType.BYTE:
    // read byte directly
    return reader.view.getInt8(reader.offset++)
  case CompactType.I16:
  case CompactType.I32:
    return readZigZag(reader)
  case CompactType.I64:
    return readZigZagBigInt(reader)
  case CompactType.DOUBLE: {
    const value = reader.view.getFloat64(reader.offset, true);
    reader.offset += 8;
    return value
  }
  case CompactType.BINARY: {
    const stringLength = readVarInt(reader);
    const strBytes = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, stringLength);
    reader.offset += stringLength;
    return strBytes
  }
  case CompactType.LIST: {
    const [elemType, listSize] = readCollectionBegin(reader);
    const boolType = elemType === CompactType.TRUE || elemType === CompactType.FALSE;
    const values = new Array(listSize);
    for (let i = 0; i < listSize; i++) {
      values[i] = boolType ? readElement(reader, CompactType.BYTE) === 1 : readElement(reader, elemType);
    }
    return values
  }
  case CompactType.STRUCT: {
    /** @type {Record<string, any>} */
    const structValues = {};
    let structLastFid = 0;
    while (true) {
      let structFieldType, structFid;
      [structFieldType, structFid, structLastFid] = readFieldBegin(reader, structLastFid);
      if (structFieldType === CompactType.STOP) {
        break
      }
      structValues[`field_${structFid}`] = readElement(reader, structFieldType);
    }
    return structValues
  }
  // TODO: MAP and SET
  case CompactType.UUID: {
    // Read 16 bytes to uuid string
    let uuid = '';
    for (let i = 0; i < 16; i++) {
      uuid += reader.view.getUint8(reader.offset++).toString(16).padStart(2, '0');
    }
    return uuid
  }
  default:
    throw new Error(`thrift unhandled type: ${type}`)
  }
}

/**
 * Var int, also known as Unsigned LEB128.
 * Var ints take 1 to 5 bytes (int32) or 1 to 10 bytes (int64).
 * Reads groups of 7 low bits until high bit is 0.
 *
 * @param {DataReader} reader
 * @returns {number} value
 */
function readVarInt(reader) {
  let result = 0;
  let shift = 0;
  while (true) {
    const byte = reader.view.getUint8(reader.offset++);
    result |= (byte & 0x7f) << shift;
    if (!(byte & 0x80)) {
      return result
    }
    shift += 7;
  }
}

/**
 * Read a varint as a bigint.
 *
 * @param {DataReader} reader
 * @returns {bigint} value
 */
function readVarBigInt(reader) {
  let result = 0n;
  let shift = 0n;
  while (true) {
    const byte = reader.view.getUint8(reader.offset++);
    result |= BigInt(byte & 0x7f) << shift;
    if (!(byte & 0x80)) {
      return result
    }
    shift += 7n;
  }
}

/**
 * Values of type int32 and int64 are transformed to a zigzag int.
 * A zigzag int folds positive and negative numbers into the positive number space.
 *
 * @param {DataReader} reader
 * @returns {number} value
 */
function readZigZag(reader) {
  const zigzag = readVarInt(reader);
  // convert zigzag to int
  return zigzag >>> 1 ^ -(zigzag & 1)
}

/**
 * A zigzag int folds positive and negative numbers into the positive number space.
 * This version returns a BigInt.
 *
 * @param {DataReader} reader
 * @returns {bigint} value
 */
function readZigZagBigInt(reader) {
  const zigzag = readVarBigInt(reader);
  // convert zigzag to int
  return zigzag >> BigInt(1) ^ -(zigzag & BigInt(1))
}

/**
 * Get thrift type from half a byte
 *
 * @param {number} byte
 * @returns {number}
 */
function getCompactType(byte) {
  return byte & 0x0f
}

/**
 * Read field type and field id
 *
 * @param {DataReader} reader
 * @param {number} lastFid
 * @returns {[number, number, number]} [type, fid, newLastFid]
 */
function readFieldBegin(reader, lastFid) {
  const type = reader.view.getUint8(reader.offset++);
  if ((type & 0x0f) === CompactType.STOP) {
    // STOP also ends a struct
    return [0, 0, lastFid]
  }
  const delta = type >> 4;
  let fid; // field id
  if (delta) {
    // add delta to last field id
    fid = lastFid + delta;
  } else {
    throw new Error('non-delta field id not supported')
  }
  return [getCompactType(type), fid, fid]
}

/**
 * Read collection type and size
 *
 * @param {DataReader} reader
 * @returns {[number, number]} [type, size]
 */
function readCollectionBegin(reader) {
  const sizeType = reader.view.getUint8(reader.offset++);
  const size = sizeType >> 4;
  const type = getCompactType(sizeType);
  if (size === 15) {
    const newSize = readVarInt(reader);
    return [type, newSize]
  }
  return [type, size]
}

/**
 * Read parquet metadata from an async buffer.
 *
 * An AsyncBuffer is like an ArrayBuffer, but the slices are loaded
 * asynchronously, possibly over the network.
 *
 * You must provide the byteLength of the buffer, typically from a HEAD request.
 *
 * In theory, you could use suffix-range requests to fetch the end of the file,
 * and save a round trip. But in practice, this doesn't work because chrome
 * deems suffix-range requests as a not-safe-listed header, and will require
 * a pre-flight. So the byteLength is required.
 *
 * To make this efficient, we initially request the last 512kb of the file,
 * which is likely to contain the metadata. If the metadata length exceeds the
 * initial fetch, 512kb, we request the rest of the metadata from the AsyncBuffer.
 *
 * This ensures that we either make one 512kb initial request for the metadata,
 * or a second request for up to the metadata size.
 *
 * @param {AsyncBuffer} asyncBuffer parquet file contents
 * @param {number} initialFetchSize initial fetch size in bytes (default 512kb)
 * @returns {Promise<FileMetaData>} parquet metadata object
 */
async function parquetMetadataAsync(asyncBuffer, initialFetchSize = 1 << 19 /* 512kb */) {
  if (!asyncBuffer) throw new Error('parquet file is required')
  if (!(asyncBuffer.byteLength >= 0)) throw new Error('parquet file byteLength is required')

  // fetch last bytes (footer) of the file
  const footerOffset = Math.max(0, asyncBuffer.byteLength - initialFetchSize);
  const footerBuffer = await asyncBuffer.slice(footerOffset, asyncBuffer.byteLength);

  // Check for parquet magic number "PAR1"
  const footerView = new DataView(footerBuffer);
  if (footerView.getUint32(footerBuffer.byteLength - 4, true) !== 0x31524150) {
    throw new Error('parquet file invalid (footer != PAR1)')
  }

  // Parquet files store metadata at the end of the file
  // Metadata length is 4 bytes before the last PAR1
  const metadataLength = footerView.getUint32(footerBuffer.byteLength - 8, true);
  if (metadataLength > asyncBuffer.byteLength - 8) {
    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${asyncBuffer.byteLength - 8}`)
  }

  // check if metadata size fits inside the initial fetch
  if (metadataLength + 8 > initialFetchSize) {
    // fetch the rest of the metadata
    const metadataOffset = asyncBuffer.byteLength - metadataLength - 8;
    const metadataBuffer = await asyncBuffer.slice(metadataOffset, footerOffset);
    // combine initial fetch with the new slice
    const combinedBuffer = new ArrayBuffer(metadataLength + 8);
    const combinedView = new Uint8Array(combinedBuffer);
    combinedView.set(new Uint8Array(metadataBuffer));
    combinedView.set(new Uint8Array(footerBuffer), footerOffset - metadataOffset);
    return parquetMetadata(combinedBuffer)
  } else {
    // parse metadata from the footer
    return parquetMetadata(footerBuffer)
  }
}

/**
 * Read parquet metadata from a buffer synchronously.
 *
 * @param {ArrayBuffer} arrayBuffer parquet file contents
 * @returns {FileMetaData} parquet metadata object
 */
function parquetMetadata(arrayBuffer) {
  if (!arrayBuffer) throw new Error('parquet file is required')
  const view = new DataView(arrayBuffer);

  // Validate footer magic number "PAR1"
  if (view.byteLength < 8) {
    throw new Error('parquet file is too short')
  }
  if (view.getUint32(view.byteLength - 4, true) !== 0x31524150) {
    throw new Error('parquet file invalid (footer != PAR1)')
  }

  // Parquet files store metadata at the end of the file
  // Metadata length is 4 bytes before the last PAR1
  const metadataLengthOffset = view.byteLength - 8;
  const metadataLength = view.getUint32(metadataLengthOffset, true);
  if (metadataLength > view.byteLength - 8) {
    // {metadata}, metadata_length, PAR1
    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${view.byteLength - 8}`)
  }

  const metadataOffset = metadataLengthOffset - metadataLength;
  const reader = { view, offset: metadataOffset };
  const metadata = deserializeTCompactProtocol(reader);
  const decoder = new TextDecoder();
  function decode(/** @type {Uint8Array} */ value) {
    return value && decoder.decode(value)
  }

  // Parse metadata from thrift data
  const version = metadata.field_1;
  /** @type {SchemaElement[]} */
  const schema = metadata.field_2.map((/** @type {any} */ field) => ({
    type: ParquetType[field.field_1],
    type_length: field.field_2,
    repetition_type: FieldRepetitionType[field.field_3],
    name: decode(field.field_4),
    num_children: field.field_5,
    converted_type: ConvertedType[field.field_6],
    scale: field.field_7,
    precision: field.field_8,
    field_id: field.field_9,
    logical_type: logicalType(field.field_10),
  }));
  // schema element per column index
  const columnSchema = schema.filter(e => e.type);
  const num_rows = metadata.field_3;
  const row_groups = metadata.field_4.map((/** @type {any} */ rowGroup) => ({
    columns: rowGroup.field_1.map((/** @type {any} */ column, /** @type {number} */ columnIndex) => ({
      file_path: decode(column.field_1),
      file_offset: column.field_2,
      meta_data: column.field_3 && {
        type: ParquetType[column.field_3.field_1],
        encodings: column.field_3.field_2?.map((/** @type {number} */ e) => Encoding[e]),
        path_in_schema: column.field_3.field_3.map(decode),
        codec: CompressionCodec[column.field_3.field_4],
        num_values: column.field_3.field_5,
        total_uncompressed_size: column.field_3.field_6,
        total_compressed_size: column.field_3.field_7,
        key_value_metadata: column.field_3.field_8,
        data_page_offset: column.field_3.field_9,
        index_page_offset: column.field_3.field_10,
        dictionary_page_offset: column.field_3.field_11,
        statistics: convertStats(column.field_3.field_12, columnSchema[columnIndex]),
        encoding_stats: column.field_3.field_13?.map((/** @type {any} */ encodingStat) => ({
          page_type: PageType[encodingStat.field_1],
          encoding: Encoding[encodingStat.field_2],
          count: encodingStat.field_3,
        })),
        bloom_filter_offset: column.field_3.field_14,
        bloom_filter_length: column.field_3.field_15,
        size_statistics: column.field_3.field_16 && {
          unencoded_byte_array_data_bytes: column.field_3.field_16.field_1,
          repetition_level_histogram: column.field_3.field_16.field_2,
          definition_level_histogram: column.field_3.field_16.field_3,
        },
      },
      offset_index_offset: column.field_4,
      offset_index_length: column.field_5,
      column_index_offset: column.field_6,
      column_index_length: column.field_7,
      crypto_metadata: column.field_7,
      encrypted_column_metadata: column.field_8,
    })),
    total_byte_size: rowGroup.field_2,
    num_rows: rowGroup.field_3,
    sorting_columns: rowGroup.field_4?.map((/** @type {any} */ sortingColumn) => ({
      column_idx: sortingColumn.field_1,
      descending: sortingColumn.field_2,
      nulls_first: sortingColumn.field_3,
    })),
    file_offset: rowGroup.field_5,
    total_compressed_size: rowGroup.field_6,
    ordinal: rowGroup.field_7,
  }));
  const key_value_metadata = metadata.field_5?.map((/** @type {any} */ keyValue) => ({
    key: decode(keyValue.field_1),
    value: decode(keyValue.field_2),
  }));
  const created_by = decode(metadata.field_6);

  return {
    version,
    schema,
    num_rows,
    row_groups,
    key_value_metadata,
    created_by,
    metadata_length: metadataLength,
  }
}

/**
 * @param {any} logicalType
 * @returns {LogicalType | undefined}
 */
function logicalType(logicalType) {
  if (logicalType?.field_1) return { type: 'STRING' }
  if (logicalType?.field_2) return { type: 'MAP' }
  if (logicalType?.field_3) return { type: 'LIST' }
  if (logicalType?.field_4) return { type: 'ENUM' }
  if (logicalType?.field_5) return {
    type: 'DECIMAL',
    scale: logicalType.field_5.field_1,
    precision: logicalType.field_5.field_2,
  }
  if (logicalType?.field_6) return { type: 'DATE' }
  if (logicalType?.field_7) return {
    type: 'TIME',
    isAdjustedToUTC: logicalType.field_7.field_1,
    unit: timeUnit(logicalType.field_7.field_2),
  }
  if (logicalType?.field_8) return {
    type: 'TIMESTAMP',
    isAdjustedToUTC: logicalType.field_8.field_1,
    unit: timeUnit(logicalType.field_8.field_2),
  }
  if (logicalType?.field_10) return {
    type: 'INTEGER',
    bitWidth: logicalType.field_10.field_1,
    isSigned: logicalType.field_10.field_2,
  }
  if (logicalType?.field_11) return { type: 'NULL' }
  if (logicalType?.field_12) return { type: 'JSON' }
  if (logicalType?.field_13) return { type: 'BSON' }
  if (logicalType?.field_14) return { type: 'UUID' }
  if (logicalType?.field_15) return { type: 'FLOAT16' }
  return logicalType
}

/**
 * @param {any} unit
 * @returns {TimeUnit}
 */
function timeUnit(unit) {
  if (unit.field_1) return 'MILLIS'
  if (unit.field_2) return 'MICROS'
  if (unit.field_3) return 'NANOS'
  throw new Error('parquet time unit required')
}

/**
 * Convert column statistics based on column type.
 *
 * @import {AsyncBuffer, FileMetaData, LogicalType, MinMaxType, SchemaElement, SchemaTree, Statistics, TimeUnit} from '../src/types.d.ts'
 * @param {any} stats
 * @param {SchemaElement} schema
 * @returns {Statistics}
 */
function convertStats(stats, schema) {
  return stats && {
    max: convertMetadata(stats.field_1, schema),
    min: convertMetadata(stats.field_2, schema),
    null_count: stats.field_3,
    distinct_count: stats.field_4,
    max_value: convertMetadata(stats.field_5, schema),
    min_value: convertMetadata(stats.field_6, schema),
    is_max_value_exact: stats.field_7,
    is_min_value_exact: stats.field_8,
  }
}

/**
 * @param {Uint8Array | undefined} value
 * @param {SchemaElement} schema
 * @returns {MinMaxType | undefined}
 */
function convertMetadata(value, schema) {
  const { type, converted_type, logical_type } = schema;
  if (value === undefined) return value
  if (type === 'BOOLEAN') return value[0] === 1
  if (type === 'BYTE_ARRAY') return new TextDecoder().decode(value)
  const view = new DataView(value.buffer, value.byteOffset, value.byteLength);
  if (type === 'FLOAT' && view.byteLength === 4) return view.getFloat32(0, true)
  if (type === 'DOUBLE' && view.byteLength === 8) return view.getFloat64(0, true)
  if (type === 'INT32' && converted_type === 'DATE') return new Date(view.getInt32(0, true) * 86400000)
  if (type === 'INT64' && converted_type === 'TIMESTAMP_MICROS') return new Date(Number(view.getBigInt64(0, true) / 1000n))
  if (type === 'INT64' && converted_type === 'TIMESTAMP_MILLIS') return new Date(Number(view.getBigInt64(0, true)))
  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'NANOS') return new Date(Number(view.getBigInt64(0, true) / 1000000n))
  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'MICROS') return new Date(Number(view.getBigInt64(0, true) / 1000n))
  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP') return new Date(Number(view.getBigInt64(0, true)))
  if (type === 'INT32' && view.byteLength === 4) return view.getInt32(0, true)
  if (type === 'INT64' && view.byteLength === 8) return view.getBigInt64(0, true)
  if (converted_type === 'DECIMAL') return parseDecimal(value) * Math.pow(10, -(schema.scale || 0))
  if (logical_type?.type === 'FLOAT16') return parseFloat16(value)
  if (type === 'FIXED_LEN_BYTE_ARRAY') return value
  // assert(false)
  return value
}

/**
 * Dremel-assembly of arrays of values into lists
 *
 * Reconstructs a complex nested structure from flat arrays of definition and repetition levels,
 * according to Dremel encoding.
 *
 * @import {DecodedArray, FieldRepetitionType} from '../src/types.d.ts'
 * @param {any[]} output
 * @param {number[] | undefined} definitionLevels
 * @param {number[]} repetitionLevels
 * @param {DecodedArray} values
 * @param {(FieldRepetitionType | undefined)[]} repetitionPath
 * @param {number} maxDefinitionLevel definition level that corresponds to non-null
 * @returns {any[]}
 */
function assembleLists(
  output, definitionLevels, repetitionLevels, values, repetitionPath, maxDefinitionLevel
) {
  const n = definitionLevels?.length || repetitionLevels.length;
  let valueIndex = 0;

  // Track state of nested structures
  const containerStack = [output];
  let currentContainer = output;
  let currentDepth = 0; // schema depth
  let currentDefLevel = 0; // list depth
  let currentRepLevel = 0;

  if (repetitionLevels[0]) {
    // continue previous row
    while (currentDepth < repetitionPath.length - 2 && currentRepLevel < repetitionLevels[0]) {
      // go into last list
      currentContainer = currentContainer.at(-1);
      containerStack.push(currentContainer);
      currentDepth++;
      if (repetitionPath[currentDepth] !== 'REQUIRED') currentDefLevel++;
      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++;
    }
  }

  for (let i = 0; i < n; i++) {
    // assert(currentDefLevel === containerStack.length - 1)
    const def = definitionLevels?.length ? definitionLevels[i] : maxDefinitionLevel;
    const rep = repetitionLevels[i];

    // Pop up to start of rep level
    while (currentDepth && (rep < currentRepLevel || repetitionPath[currentDepth] !== 'REPEATED')) {
      if (repetitionPath[currentDepth] !== 'REQUIRED') {
        containerStack.pop();
        currentDefLevel--;
      }
      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel--;
      currentDepth--;
    }
    // @ts-expect-error won't be empty
    currentContainer = containerStack.at(-1);

    // Go deeper to end of definition level
    while (
      (currentDepth < repetitionPath.length - 2 || repetitionPath[currentDepth + 1] === 'REPEATED') &&
      (currentDefLevel < def || repetitionPath[currentDepth + 1] === 'REQUIRED')
    ) {
      currentDepth++;
      if (repetitionPath[currentDepth] !== 'REQUIRED') {
        /** @type {any[]} */
        const newList = [];
        currentContainer.push(newList);
        currentContainer = newList;
        containerStack.push(newList);
        currentDefLevel++;
      }
      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++;
    }

    // Add value or null based on definition level
    if (def === maxDefinitionLevel) {
      // assert(currentDepth === maxDefinitionLevel || currentDepth === repetitionPath.length - 2)
      currentContainer.push(values[valueIndex++]);
    } else if (currentDepth === repetitionPath.length - 2) {
      currentContainer.push(null);
    } else {
      currentContainer.push([]);
    }
  }

  // Handle edge cases for empty inputs or single-level data
  if (!output.length) {
    // return max definition level of nested lists
    for (let i = 0; i < maxDefinitionLevel; i++) {
      /** @type {any[]} */
      const newList = [];
      currentContainer.push(newList);
      currentContainer = newList;
    }
  }

  return output
}

/**
 * Assemble a nested structure from subcolumn data.
 * https://github.com/apache/parquet-format/blob/apache-parquet-format-2.10.0/LogicalTypes.md#nested-types
 *
 * @import {SchemaTree} from '../src/types.d.ts'
 * @param {Map<string, any[]>} subcolumnData
 * @param {SchemaTree} schema top-level schema element
 * @param {number} [depth] depth of nested structure
 */
function assembleNested(subcolumnData, schema, depth = 0) {
  const path = schema.path.join('.');
  const optional = schema.element.repetition_type === 'OPTIONAL';
  const nextDepth = optional ? depth + 1 : depth;

  if (isListLike(schema)) {
    let sublist = schema.children[0];
    let subDepth = nextDepth;
    if (sublist.children.length === 1) {
      sublist = sublist.children[0];
      subDepth++;
    }
    assembleNested(subcolumnData, sublist, subDepth);

    const subcolumn = sublist.path.join('.');
    const values = subcolumnData.get(subcolumn);
    if (!values) throw new Error('parquet list column missing values')
    if (optional) flattenAtDepth(values, depth);
    subcolumnData.set(path, values);
    subcolumnData.delete(subcolumn);
    return
  }

  if (isMapLike(schema)) {
    const mapName = schema.children[0].element.name;

    // Assemble keys and values
    assembleNested(subcolumnData, schema.children[0].children[0], nextDepth + 1);
    assembleNested(subcolumnData, schema.children[0].children[1], nextDepth + 1);

    const keys = subcolumnData.get(`${path}.${mapName}.key`);
    const values = subcolumnData.get(`${path}.${mapName}.value`);

    if (!keys) throw new Error('parquet map column missing keys')
    if (!values) throw new Error('parquet map column missing values')
    if (keys.length !== values.length) {
      throw new Error('parquet map column key/value length mismatch')
    }

    const out = assembleMaps(keys, values, nextDepth);
    if (optional) flattenAtDepth(out, depth);

    subcolumnData.delete(`${path}.${mapName}.key`);
    subcolumnData.delete(`${path}.${mapName}.value`);
    subcolumnData.set(path, out);
    return
  }

  // Struct-like column
  if (schema.children.length) {
    // construct a meta struct and then invert
    const invertDepth = schema.element.repetition_type === 'REQUIRED' ? depth : depth + 1;
    /** @type {Record<string, any>} */
    const struct = {};
    for (const child of schema.children) {
      assembleNested(subcolumnData, child, invertDepth);
      const childData = subcolumnData.get(child.path.join('.'));
      if (!childData) throw new Error('parquet struct missing child data')
      struct[child.element.name] = childData;
    }
    // remove children
    for (const child of schema.children) {
      subcolumnData.delete(child.path.join('.'));
    }
    // invert struct by depth
    const inverted = invertStruct(struct, invertDepth);
    if (optional) flattenAtDepth(inverted, depth);
    subcolumnData.set(path, inverted);
  }
}

/**
 * @param {any[]} arr
 * @param {number} depth
 */
function flattenAtDepth(arr, depth) {
  for (let i = 0; i < arr.length; i++) {
    if (depth) {
      flattenAtDepth(arr[i], depth - 1);
    } else {
      arr[i] = arr[i][0];
    }
  }
}

/**
 * @param {any[]} keys
 * @param {any[]} values
 * @param {number} depth
 * @returns {any[]}
 */
function assembleMaps(keys, values, depth) {
  const out = [];
  for (let i = 0; i < keys.length; i++) {
    if (depth) {
      out.push(assembleMaps(keys[i], values[i], depth - 1)); // go deeper
    } else {
      if (keys[i]) {
        /** @type {Record<string, any>} */
        const obj = {};
        for (let j = 0; j < keys[i].length; j++) {
          const value = values[i][j];
          obj[keys[i][j]] = value === undefined ? null : value;
        }
        out.push(obj);
      } else {
        out.push(undefined);
      }
    }
  }
  return out
}

/**
 * Invert a struct-like object by depth.
 *
 * @param {Record<string, any[]>} struct
 * @param {number} depth
 * @returns {any[]}
 */
function invertStruct(struct, depth) {
  const keys = Object.keys(struct);
  const length = struct[keys[0]]?.length;
  const out = [];
  for (let i = 0; i < length; i++) {
    /** @type {Record<string, any>} */
    const obj = {};
    for (const key of keys) {
      if (struct[key].length !== length) throw new Error('parquet struct parsing error')
      obj[key] = struct[key][i];
    }
    if (depth) {
      out.push(invertStruct(obj, depth - 1)); // deeper
    } else {
      out.push(obj);
    }
  }
  return out
}

/**
 * @import {DataReader} from '../src/types.d.ts'
 * @param {DataReader} reader
 * @param {number} count number of values to read
 * @param {Int32Array | BigInt64Array} output
 */
function deltaBinaryUnpack(reader, count, output) {
  const int32 = output instanceof Int32Array;
  const blockSize = readVarInt(reader);
  const miniblockPerBlock = readVarInt(reader);
  readVarInt(reader); // assert(=== count)
  let value = readZigZagBigInt(reader); // first value
  let outputIndex = 0;
  output[outputIndex++] = int32 ? Number(value) : value;

  const valuesPerMiniblock = blockSize / miniblockPerBlock;

  while (outputIndex < count) {
    // new block
    const minDelta = readZigZagBigInt(reader);
    const bitWidths = new Uint8Array(miniblockPerBlock);
    for (let i = 0; i < miniblockPerBlock; i++) {
      bitWidths[i] = reader.view.getUint8(reader.offset++);
    }

    for (let i = 0; i < miniblockPerBlock && outputIndex < count; i++) {
      // new miniblock
      const bitWidth = BigInt(bitWidths[i]);
      if (bitWidth) {
        let bitpackPos = 0n;
        let miniblockCount = valuesPerMiniblock;
        const mask = (1n << bitWidth) - 1n;
        while (miniblockCount && outputIndex < count) {
          let bits = BigInt(reader.view.getUint8(reader.offset)) >> bitpackPos & mask; // TODO: don't re-read value every time
          bitpackPos += bitWidth;
          while (bitpackPos >= 8) {
            bitpackPos -= 8n;
            reader.offset++;
            if (bitpackPos) {
              bits |= BigInt(reader.view.getUint8(reader.offset)) << bitWidth - bitpackPos & mask;
            }
          }
          const delta = minDelta + bits;
          value += delta;
          output[outputIndex++] = int32 ? Number(value) : value;
          miniblockCount--;
        }
        if (miniblockCount) {
          // consume leftover miniblock
          reader.offset += Math.ceil((miniblockCount * Number(bitWidth) + Number(bitpackPos)) / 8);
        }
      } else {
        for (let j = 0; j < valuesPerMiniblock && outputIndex < count; j++) {
          value += minDelta;
          output[outputIndex++] = int32 ? Number(value) : value;
        }
      }
    }
  }
}

/**
 * @param {DataReader} reader
 * @param {number} count
 * @param {Uint8Array[]} output
 */
function deltaLengthByteArray(reader, count, output) {
  const lengths = new Int32Array(count);
  deltaBinaryUnpack(reader, count, lengths);
  for (let i = 0; i < count; i++) {
    output[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, lengths[i]);
    reader.offset += lengths[i];
  }
}

/**
 * @param {DataReader} reader
 * @param {number} count
 * @param {Uint8Array[]} output
 */
function deltaByteArray(reader, count, output) {
  const prefixData = new Int32Array(count);
  deltaBinaryUnpack(reader, count, prefixData);
  const suffixData = new Int32Array(count);
  deltaBinaryUnpack(reader, count, suffixData);

  for (let i = 0; i < count; i++) {
    const suffix = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, suffixData[i]);
    if (prefixData[i]) {
      // copy from previous value
      output[i] = new Uint8Array(prefixData[i] + suffixData[i]);
      output[i].set(output[i - 1].subarray(0, prefixData[i]));
      output[i].set(suffix, prefixData[i]);
    } else {
      output[i] = suffix;
    }
    reader.offset += suffixData[i];
  }
}

/**
 * Minimum bits needed to store value.
 *
 * @param {number} value
 * @returns {number}
 */
function bitWidth(value) {
  return 32 - Math.clz32(value)
}

/**
 * Read values from a run-length encoded/bit-packed hybrid encoding.
 *
 * If length is zero, then read int32 length at the start.
 *
 * @param {DataReader} reader
 * @param {number} width - width of each bit-packed group
 * @param {number} length - length of the encoded data
 * @param {DecodedArray} output
 */
function readRleBitPackedHybrid(reader, width, length, output) {
  if (!length) {
    // length = reader.view.getUint32(reader.offset, true)
    reader.offset += 4;
  }
  let seen = 0;
  while (seen < output.length) {
    const header = readVarInt(reader);
    if (header & 1) {
      // bit-packed
      seen = readBitPacked(reader, header, width, output, seen);
    } else {
      // rle
      const count = header >>> 1;
      readRle(reader, count, width, output, seen);
      seen += count;
    }
  }
  // assert(reader.offset - startOffset === length)
}

/**
 * Run-length encoding: read value with bitWidth and repeat it count times.
 *
 * @param {DataReader} reader
 * @param {number} count
 * @param {number} bitWidth
 * @param {DecodedArray} output
 * @param {number} seen
 */
function readRle(reader, count, bitWidth, output, seen) {
  const width = bitWidth + 7 >> 3;
  let value = 0;
  for (let i = 0; i < width; i++) {
    value |= reader.view.getUint8(reader.offset++) << (i << 3);
  }
  // assert(value < 1 << bitWidth)

  // repeat value count times
  for (let i = 0; i < count; i++) {
    output[seen + i] = value;
  }
}

/**
 * Read a bit-packed run of the rle/bitpack hybrid.
 * Supports width > 8 (crossing bytes).
 *
 * @param {DataReader} reader
 * @param {number} header - bit-pack header
 * @param {number} bitWidth
 * @param {DecodedArray} output
 * @param {number} seen
 * @returns {number} total output values so far
 */
function readBitPacked(reader, header, bitWidth, output, seen) {
  let count = header >> 1 << 3; // values to read
  const mask = (1 << bitWidth) - 1;

  let data = 0;
  if (reader.offset < reader.view.byteLength) {
    data = reader.view.getUint8(reader.offset++);
  } else if (mask) {
    // sometimes out-of-bounds reads are masked out
    throw new Error(`parquet bitpack offset ${reader.offset} out of range`)
  }
  let left = 8;
  let right = 0;

  // read values
  while (count) {
    // if we have crossed a byte boundary, shift the data
    if (right > 8) {
      right -= 8;
      left -= 8;
      data >>>= 8;
    } else if (left - right < bitWidth) {
      // if we don't have bitWidth number of bits to read, read next byte
      data |= reader.view.getUint8(reader.offset) << left;
      reader.offset++;
      left += 8;
    } else {
      if (seen < output.length) {
        // emit value
        output[seen++] = data >> right & mask;
      }
      count--;
      right += bitWidth;
    }
  }

  return seen
}

/**
 * @param {DataReader} reader
 * @param {number} count
 * @param {ParquetType} type
 * @param {number | undefined} typeLength
 * @returns {DecodedArray}
 */
function byteStreamSplit(reader, count, type, typeLength) {
  const width = byteWidth(type, typeLength);
  const bytes = new Uint8Array(count * width);
  for (let b = 0; b < width; b++) {
    for (let i = 0; i < count; i++) {
      bytes[i * width + b] = reader.view.getUint8(reader.offset++);
    }
  }
  // interpret bytes as typed array
  if (type === 'FLOAT') return new Float32Array(bytes.buffer)
  else if (type === 'DOUBLE') return new Float64Array(bytes.buffer)
  else if (type === 'INT32') return new Int32Array(bytes.buffer)
  else if (type === 'INT64') return new BigInt64Array(bytes.buffer)
  else if (type === 'FIXED_LEN_BYTE_ARRAY') {
    // split into arrays of typeLength
    const split = new Array(count);
    for (let i = 0; i < count; i++) {
      split[i] = bytes.subarray(i * width, (i + 1) * width);
    }
    return split
  }
  throw new Error(`parquet byte_stream_split unsupported type: ${type}`)
}

/**
 * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'
 * @param {ParquetType} type
 * @param {number | undefined} typeLength
 * @returns {number}
 */
function byteWidth(type, typeLength) {
  switch (type) {
  case 'INT32':
  case 'FLOAT':
    return 4
  case 'INT64':
  case 'DOUBLE':
    return 8
  case 'FIXED_LEN_BYTE_ARRAY':
    if (!typeLength) throw new Error('parquet byteWidth missing type_length')
    return typeLength
  default:
    throw new Error(`parquet unsupported type: ${type}`)
  }
}

/**
 * Read `count` values of the given type from the reader.view.
 *
 * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'
 * @param {DataReader} reader - buffer to read data from
 * @param {ParquetType} type - parquet type of the data
 * @param {number} count - number of values to read
 * @param {number | undefined} fixedLength - length of each fixed length byte array
 * @returns {DecodedArray} array of values
 */
function readPlain(reader, type, count, fixedLength) {
  if (count === 0) return []
  if (type === 'BOOLEAN') {
    return readPlainBoolean(reader, count)
  } else if (type === 'INT32') {
    return readPlainInt32(reader, count)
  } else if (type === 'INT64') {
    return readPlainInt64(reader, count)
  } else if (type === 'INT96') {
    return readPlainInt96(reader, count)
  } else if (type === 'FLOAT') {
    return readPlainFloat(reader, count)
  } else if (type === 'DOUBLE') {
    return readPlainDouble(reader, count)
  } else if (type === 'BYTE_ARRAY') {
    return readPlainByteArray(reader, count)
  } else if (type === 'FIXED_LEN_BYTE_ARRAY') {
    if (!fixedLength) throw new Error('parquet missing fixed length')
    return readPlainByteArrayFixed(reader, count, fixedLength)
  } else {
    throw new Error(`parquet unhandled type: ${type}`)
  }
}

/**
 * Read `count` boolean values.
 *
 * @param {DataReader} reader
 * @param {number} count
 * @returns {boolean[]}
 */
function readPlainBoolean(reader, count) {
  const values = new Array(count);
  for (let i = 0; i < count; i++) {
    const byteOffset = reader.offset + (i / 8 | 0);
    const bitOffset = i % 8;
    const byte = reader.view.getUint8(byteOffset);
    values[i] = (byte & 1 << bitOffset) !== 0;
  }
  reader.offset += Math.ceil(count / 8);
  return values
}

/**
 * Read `count` int32 values.
 *
 * @param {DataReader} reader
 * @param {number} count
 * @returns {Int32Array}
 */
function readPlainInt32(reader, count) {
  const values = (reader.view.byteOffset + reader.offset) % 4
    ? new Int32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))
    : new Int32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count);
  reader.offset += count * 4;
  return values
}

/**
 * Read `count` int64 values.
 *
 * @param {DataReader} reader
 * @param {number} count
 * @returns {BigInt64Array}
 */
function readPlainInt64(reader, count) {
  const values = (reader.view.byteOffset + reader.offset) % 8
    ? new BigInt64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))
    : new BigInt64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count);
  reader.offset += count * 8;
  return values
}

/**
 * Read `count` int96 values.
 *
 * @param {DataReader} reader
 * @param {number} count
 * @returns {bigint[]}
 */
function readPlainInt96(reader, count) {
  const values = new Array(count);
  for (let i = 0; i < count; i++) {
    const low = reader.view.getBigInt64(reader.offset + i * 12, true);
    const high = reader.view.getInt32(reader.offset + i * 12 + 8, true);
    values[i] = BigInt(high) << 64n | low;
  }
  reader.offset += count * 12;
  return values
}

/**
 * Read `count` float values.
 *
 * @param {DataReader} reader
 * @param {number} count
 * @returns {Float32Array}
 */
function readPlainFloat(reader, count) {
  const values = (reader.view.byteOffset + reader.offset) % 4
    ? new Float32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))
    : new Float32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count);
  reader.offset += count * 4;
  return values
}

/**
 * Read `count` double values.
 *
 * @param {DataReader} reader
 * @param {number} count
 * @returns {Float64Array}
 */
function readPlainDouble(reader, count) {
  const values = (reader.view.byteOffset + reader.offset) % 8
    ? new Float64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))
    : new Float64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count);
  reader.offset += count * 8;
  return values
}

/**
 * Read `count` byte array values.
 *
 * @param {DataReader} reader
 * @param {number} count
 * @returns {Uint8Array[]}
 */
function readPlainByteArray(reader, count) {
  const values = new Array(count);
  for (let i = 0; i < count; i++) {
    const length = reader.view.getInt32(reader.offset, true);
    reader.offset += 4;
    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, length);
    reader.offset += length;
  }
  return values
}

/**
 * Read a fixed length byte array.
 *
 * @param {DataReader} reader
 * @param {number} count
 * @param {number} fixedLength
 * @returns {Uint8Array[]}
 */
function readPlainByteArrayFixed(reader, count, fixedLength) {
  // assert(reader.view.byteLength - reader.offset >= count * fixedLength)
  const values = new Array(count);
  for (let i = 0; i < count; i++) {
    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, fixedLength);
    reader.offset += fixedLength;
  }
  return values
}

/**
 * Create a new buffer with the offset and size.
 *
 * @param {ArrayBufferLike} buffer
 * @param {number} offset
 * @param {number} size
 * @returns {ArrayBuffer}
 */
function align(buffer, offset, size) {
  const aligned = new ArrayBuffer(size);
  new Uint8Array(aligned).set(new Uint8Array(buffer, offset, size));
  return aligned
}

/**
 * The MIT License (MIT)
 * Copyright (c) 2016 Zhipeng Jia
 * https://github.com/zhipeng-jia/snappyjs
 */

const WORD_MASK = [0, 0xff, 0xffff, 0xffffff, 0xffffffff];

/**
 * Copy bytes from one array to another
 *
 * @param {Uint8Array} fromArray source array
 * @param {number} fromPos source position
 * @param {Uint8Array} toArray destination array
 * @param {number} toPos destination position
 * @param {number} length number of bytes to copy
 * @returns {void}
 */
function copyBytes(fromArray, fromPos, toArray, toPos, length) {
  for (let i = 0; i < length; i++) {
    toArray[toPos + i] = fromArray[fromPos + i];
  }
}

/**
 * Copy bytes within an array
 *
 * @param {Uint8Array} array source and destination array
 * @param {number} pos source position
 * @param {number} offset offset back from current position to read
 * @param {number} length number of bytes to copy
 * @returns {void}
 */
function selfCopyBytes(array, pos, offset, length) {
  for (let i = 0; i < length; i++) {
    array[pos + i] = array[pos - offset + i];
  }
}

/**
 * Decompress snappy data.
 * Accepts an output buffer to avoid allocating a new buffer for each call.
 *
 * @param {Uint8Array} input compressed data
 * @param {Uint8Array} output output buffer
 * @returns {void}
 */
function snappyUncompress(input, output) {
  const inputLength = input.byteLength;
  const outputLength = output.byteLength;
  let pos = 0;
  let outPos = 0;

  // skip preamble (contains uncompressed length as varint)
  while (pos < inputLength) {
    const c = input[pos];
    pos++;
    if (c < 128) {
      break
    }
  }
  if (outputLength && pos >= inputLength) {
    throw new Error('invalid snappy length header')
  }

  while (pos < inputLength) {
    const c = input[pos];
    let len = 0;
    pos++;

    if (pos >= inputLength) {
      throw new Error('missing eof marker')
    }

    // There are two types of elements, literals and copies (back references)
    if ((c & 0x3) === 0) {
      // Literals are uncompressed data stored directly in the byte stream
      let len = (c >>> 2) + 1;
      // Longer literal length is encoded in multiple bytes
      if (len > 60) {
        if (pos + 3 >= inputLength) {
          throw new Error('snappy error literal pos + 3 >= inputLength')
        }
        const lengthSize = len - 60; // length bytes - 1
        len = input[pos]
          + (input[pos + 1] << 8)
          + (input[pos + 2] << 16)
          + (input[pos + 3] << 24);
        len = (len & WORD_MASK[lengthSize]) + 1;
        pos += lengthSize;
      }
      if (pos + len > inputLength) {
        throw new Error('snappy error literal exceeds input length')
      }
      copyBytes(input, pos, output, outPos, len);
      pos += len;
      outPos += len;
    } else {
      // Copy elements
      let offset = 0; // offset back from current position to read
      switch (c & 0x3) {
      case 1:
        // Copy with 1-byte offset
        len = (c >>> 2 & 0x7) + 4;
        offset = input[pos] + (c >>> 5 << 8);
        pos++;
        break
      case 2:
        // Copy with 2-byte offset
        if (inputLength <= pos + 1) {
          throw new Error('snappy error end of input')
        }
        len = (c >>> 2) + 1;
        offset = input[pos] + (input[pos + 1] << 8);
        pos += 2;
        break
      case 3:
        // Copy with 4-byte offset
        if (inputLength <= pos + 3) {
          throw new Error('snappy error end of input')
        }
        len = (c >>> 2) + 1;
        offset = input[pos]
          + (input[pos + 1] << 8)
          + (input[pos + 2] << 16)
          + (input[pos + 3] << 24);
        pos += 4;
        break
      }
      if (offset === 0 || isNaN(offset)) {
        throw new Error(`invalid offset ${offset} pos ${pos} inputLength ${inputLength}`)
      }
      if (offset > outPos) {
        throw new Error('cannot copy from before start of buffer')
      }
      selfCopyBytes(output, outPos, offset, len);
      outPos += len;
    }
  }

  if (outPos !== outputLength) throw new Error('premature end of input')
}

/**
 * Read a data page from uncompressed reader.
 *
 * @param {Uint8Array} bytes raw page data (should already be decompressed)
 * @param {DataPageHeader} daph data page header
 * @param {SchemaTree[]} schemaPath
 * @param {ColumnMetaData} columnMetadata
 * @returns {DataPage} definition levels, repetition levels, and array of values
 */
function readDataPage(bytes, daph, schemaPath, { type }) {
  const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength);
  const reader = { view, offset: 0 };
  /** @type {DecodedArray} */
  let dataPage;

  // repetition and definition levels
  const repetitionLevels = readRepetitionLevels(reader, daph, schemaPath);
  // assert(!repetitionLevels.length || repetitionLevels.length === daph.num_values)
  const { definitionLevels, numNulls } = readDefinitionLevels(reader, daph, schemaPath);
  // assert(!definitionLevels.length || definitionLevels.length === daph.num_values)

  // read values based on encoding
  const nValues = daph.num_values - numNulls;
  if (daph.encoding === 'PLAIN') {
    const { type_length } = schemaPath[schemaPath.length - 1].element;
    dataPage = readPlain(reader, type, nValues, type_length);
  } else if (
    daph.encoding === 'PLAIN_DICTIONARY' ||
    daph.encoding === 'RLE_DICTIONARY' ||
    daph.encoding === 'RLE'
  ) {
    const bitWidth = type === 'BOOLEAN' ? 1 : view.getUint8(reader.offset++);
    if (bitWidth) {
      dataPage = new Array(nValues);
      readRleBitPackedHybrid(reader, bitWidth, view.byteLength - reader.offset, dataPage);
    } else {
      dataPage = new Uint8Array(nValues); // nValue zeroes
    }
  } else if (daph.encoding === 'BYTE_STREAM_SPLIT') {
    const { type_length } = schemaPath[schemaPath.length - 1].element;
    dataPage = byteStreamSplit(reader, nValues, type, type_length);
  } else {
    throw new Error(`parquet unsupported encoding: ${daph.encoding}`)
  }

  return { definitionLevels, repetitionLevels, dataPage }
}

/**
 * @param {Uint8Array} bytes raw page data
 * @param {DictionaryPageHeader} diph dictionary page header
 * @param {ColumnMetaData} columnMetadata
 * @param {number | undefined} typeLength - type_length from schema
 * @returns {DecodedArray}
 */
function readDictionaryPage(bytes, diph, columnMetadata, typeLength) {
  const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength);
  const reader = { view, offset: 0 };
  return readPlain(reader, columnMetadata.type, diph.num_values, typeLength)
}

/**
 * @import {ColumnMetaData, CompressionCodec, Compressors, DataPage, DataPageHeader, DataPageHeaderV2, DataReader, DecodedArray, DictionaryPageHeader, PageHeader, SchemaTree} from '../src/types.d.ts'
 * @param {DataReader} reader data view for the page
 * @param {DataPageHeader} daph data page header
 * @param {SchemaTree[]} schemaPath
 * @returns {any[]} repetition levels and number of bytes read
 */
function readRepetitionLevels(reader, daph, schemaPath) {
  if (schemaPath.length > 1) {
    const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath);
    if (maxRepetitionLevel) {
      const values = new Array(daph.num_values);
      readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), 0, values);
      return values
    }
  }
  return []
}

/**
 * @param {DataReader} reader data view for the page
 * @param {DataPageHeader} daph data page header
 * @param {SchemaTree[]} schemaPath
 * @returns {{ definitionLevels: number[], numNulls: number }} definition levels
 */
function readDefinitionLevels(reader, daph, schemaPath) {
  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath);
  if (!maxDefinitionLevel) return { definitionLevels: [], numNulls: 0 }

  const definitionLevels = new Array(daph.num_values);
  readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), 0, definitionLevels);

  // count nulls
  let numNulls = daph.num_values;
  for (const def of definitionLevels) {
    if (def === maxDefinitionLevel) numNulls--;
  }
  if (numNulls === 0) definitionLevels.length = 0;

  return { definitionLevels, numNulls }
}

/**
 * @param {Uint8Array} compressedBytes
 * @param {number} uncompressed_page_size
 * @param {CompressionCodec} codec
 * @param {Compressors | undefined} compressors
 * @returns {Uint8Array}
 */
function decompressPage(compressedBytes, uncompressed_page_size, codec, compressors) {
  /** @type {Uint8Array} */
  let page;
  const customDecompressor = compressors?.[codec];
  if (codec === 'UNCOMPRESSED') {
    page = compressedBytes;
  } else if (customDecompressor) {
    page = customDecompressor(compressedBytes, uncompressed_page_size);
  } else if (codec === 'SNAPPY') {
    page = new Uint8Array(uncompressed_page_size);
    snappyUncompress(compressedBytes, page);
  } else {
    throw new Error(`parquet unsupported compression codec: ${codec}`)
  }
  if (page?.length !== uncompressed_page_size) {
    throw new Error(`parquet decompressed page length ${page?.length} does not match header ${uncompressed_page_size}`)
  }
  return page
}


/**
 * Read a data page from the given Uint8Array.
 *
 * @param {Uint8Array} compressedBytes raw page data
 * @param {PageHeader} ph page header
 * @param {SchemaTree[]} schemaPath
 * @param {ColumnMetaData} columnMetadata
 * @param {Compressors | undefined} compressors
 * @returns {DataPage} definition levels, repetition levels, and array of values
 */
function readDataPageV2(compressedBytes, ph, schemaPath, columnMetadata, compressors) {
  const view = new DataView(compressedBytes.buffer, compressedBytes.byteOffset, compressedBytes.byteLength);
  const reader = { view, offset: 0 };
  const { codec, type } = columnMetadata;
  const daph2 = ph.data_page_header_v2;
  if (!daph2) throw new Error('parquet data page header v2 is undefined')

  // repetition levels
  const repetitionLevels = readRepetitionLevelsV2(reader, daph2, schemaPath);
  reader.offset = daph2.repetition_levels_byte_length; // readVarInt() => len for boolean v2?

  // definition levels
  const definitionLevels = readDefinitionLevelsV2(reader, daph2, schemaPath);
  // assert(reader.offset === daph2.repetition_levels_byte_length + daph2.definition_levels_byte_length)

  const uncompressedPageSize = ph.uncompressed_page_size - daph2.definition_levels_byte_length - daph2.repetition_levels_byte_length;

  let page = compressedBytes.subarray(reader.offset);
  if (daph2.is_compressed !== false) {
    page = decompressPage(page, uncompressedPageSize, codec, compressors);
  }
  const pageView = new DataView(page.buffer, page.byteOffset, page.byteLength);
  const pageReader = { view: pageView, offset: 0 };

  // read values based on encoding
  /** @type {DecodedArray} */
  let dataPage;
  const nValues = daph2.num_values - daph2.num_nulls;
  if (daph2.encoding === 'PLAIN') {
    const { type_length } = schemaPath[schemaPath.length - 1].element;
    dataPage = readPlain(pageReader, type, nValues, type_length);
  } else if (daph2.encoding === 'RLE') {
    // assert(columnMetadata.type === 'BOOLEAN')
    dataPage = new Array(nValues);
    readRleBitPackedHybrid(pageReader, 1, 0, dataPage);
    dataPage = dataPage.map(x => !!x);
  } else if (
    daph2.encoding === 'PLAIN_DICTIONARY' ||
    daph2.encoding === 'RLE_DICTIONARY'
  ) {
    const bitWidth = pageView.getUint8(pageReader.offset++);
    dataPage = new Array(nValues);
    readRleBitPackedHybrid(pageReader, bitWidth, uncompressedPageSize - 1, dataPage);
  } else if (daph2.encoding === 'DELTA_BINARY_PACKED') {
    const int32 = type === 'INT32';
    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues);
    deltaBinaryUnpack(pageReader, nValues, dataPage);
  } else if (daph2.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {
    dataPage = new Array(nValues);
    deltaLengthByteArray(pageReader, nValues, dataPage);
  } else if (daph2.encoding === 'DELTA_BYTE_ARRAY') {
    dataPage = new Array(nValues);
    deltaByteArray(pageReader, nValues, dataPage);
  } else if (daph2.encoding === 'BYTE_STREAM_SPLIT') {
    const { type_length } = schemaPath[schemaPath.length - 1].element;
    dataPage = byteStreamSplit(reader, nValues, type, type_length);
  } else {
    throw new Error(`parquet unsupported encoding: ${daph2.encoding}`)
  }

  return { definitionLevels, repetitionLevels, dataPage }
}

/**
 * @param {DataReader} reader
 * @param {DataPageHeaderV2} daph2 data page header v2
 * @param {SchemaTree[]} schemaPath
 * @returns {any[]} repetition levels
 */
function readRepetitionLevelsV2(reader, daph2, schemaPath) {
  const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath);
  if (!maxRepetitionLevel) return []

  const values = new Array(daph2.num_values);
  readRleBitPackedHybrid(
    reader, bitWidth(maxRepetitionLevel), daph2.repetition_levels_byte_length, values
  );
  return values
}

/**
 * @param {DataReader} reader
 * @param {DataPageHeaderV2} daph2 data page header v2
 * @param {SchemaTree[]} schemaPath
 * @returns {number[] | undefined} definition levels
 */
function readDefinitionLevelsV2(reader, daph2, schemaPath) {
  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath);
  if (maxDefinitionLevel) {
    // V2 we know the length
    const values = new Array(daph2.num_values);
    readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), daph2.definition_levels_byte_length, values);
    return values
  }
}

/**
 * Replace bigint, date, etc with legal JSON types.
 * When parsing parquet files, bigints are used to represent 64-bit integers.
 * However, JSON does not support bigints, so it's helpful to convert to numbers.
 *
 * @param {any} obj object to convert
 * @returns {unknown} converted object
 */
function toJson(obj) {
  if (obj === undefined) return null
  if (typeof obj === 'bigint') return Number(obj)
  if (Array.isArray(obj)) return obj.map(toJson)
  if (obj instanceof Uint8Array) return Array.from(obj)
  if (obj instanceof Date) return obj.toISOString()
  if (obj instanceof Object) {
    /** @type {Record<string, unknown>} */
    const newObj = {};
    for (const key of Object.keys(obj)) {
      if (obj[key] === undefined) continue
      newObj[key] = toJson(obj[key]);
    }
    return newObj
  }
  return obj
}

/**
 * Concatenate two arrays fast.
 *
 * @param {any[]} aaa first array
 * @param {DecodedArray} bbb second array
 */
function concat(aaa, bbb) {
  const chunk = 10000;
  for (let i = 0; i < bbb.length; i += chunk) {
    aaa.push(...bbb.slice(i, i + chunk));
  }
}

/**
 * Get the byte length of a URL using a HEAD request.
 * If requestInit is provided, it will be passed to fetch.
 *
 * @param {string} url
 * @param {RequestInit} [requestInit] fetch options
 * @returns {Promise<number>}
 */
async function byteLengthFromUrl(url, requestInit) {
  return await fetch(url, { ...requestInit, method: 'HEAD' })
    .then(res => {
      if (!res.ok) throw new Error(`fetch head failed ${res.status}`)
      const length = res.headers.get('Content-Length');
      if (!length) throw new Error('missing content length')
      return parseInt(length)
    })
}

/**
 * Construct an AsyncBuffer for a URL.
 * If byteLength is not provided, will make a HEAD request to get the file size.
 * If requestInit is provided, it will be passed to fetch.
 *
 * @param {object} options
 * @param {string} options.url
 * @param {number} [options.byteLength]
 * @param {RequestInit} [options.requestInit]
 * @returns {Promise<AsyncBuffer>}
 */
async function asyncBufferFromUrl({ url, byteLength, requestInit }) {
  // byte length from HEAD request
  byteLength ||= await byteLengthFromUrl(url, requestInit);
  const init = requestInit || {};
  return {
    byteLength,
    async slice(start, end) {
      // fetch byte range from url
      const headers = new Headers(init.headers);
      const endStr = end === undefined ? '' : end - 1;
      headers.set('Range', `bytes=${start}-${endStr}`);
      const res = await fetch(url, { ...init, headers });
      if (!res.ok || !res.body) throw new Error(`fetch failed ${res.status}`)
      return res.arrayBuffer()
    },
  }
}

/**
 * Parse column data from a buffer.
 *
 * @param {DataReader} reader
 * @param {number} rowLimit maximum number of rows to read
 * @param {ColumnMetaData} columnMetadata column metadata
 * @param {SchemaTree[]} schemaPath schema path for the column
 * @param {ParquetReadOptions} options read options
 * @returns {any[]} array of values
 */
function readColumn(reader, rowLimit, columnMetadata, schemaPath, { compressors, utf8 }) {
  const { element } = schemaPath[schemaPath.length - 1];
  /** @type {DecodedArray | undefined} */
  let dictionary = undefined;
  /** @type {any[]} */
  const rowData = [];

  while (rowData.length < rowLimit) {
    // parse column header
    const header = parquetHeader(reader);
    // assert(header.compressed_page_size !== undefined)

    // read compressed_page_size bytes starting at offset
    const compressedBytes = new Uint8Array(
      reader.view.buffer, reader.view.byteOffset + reader.offset, header.compressed_page_size
    );

    // parse page data by type
    /** @type {DecodedArray} */
    let values;
    if (header.type === 'DATA_PAGE') {
      const daph = header.data_page_header;
      if (!daph) throw new Error('parquet data page header is undefined')

      const page = decompressPage(compressedBytes, Number(header.uncompressed_page_size), columnMetadata.codec, compressors);
      const { definitionLevels, repetitionLevels, dataPage } = readDataPage(page, daph, schemaPath, columnMetadata);
      // assert(!daph.statistics?.null_count || daph.statistics.null_count === BigInt(daph.num_values - dataPage.length))

      // convert types, dereference dictionary, and assemble lists
      values = convertWithDictionary(dataPage, dictionary, element, daph.encoding, utf8);
      if (repetitionLevels.length || definitionLevels?.length) {
        const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath);
        const repetitionPath = schemaPath.map(({ element }) => element.repetition_type);
        assembleLists(
          rowData, definitionLevels, repetitionLevels, values, repetitionPath, maxDefinitionLevel
        );
      } else {
        // wrap nested flat data by depth
        for (let i = 2; i < schemaPath.length; i++) {
          if (schemaPath[i].element.repetition_type !== 'REQUIRED') {
            values = Array.from(values, e => [e]);
          }
        }
        concat(rowData, values);
      }
    } else if (header.type === 'DATA_PAGE_V2') {
      const daph2 = header.data_page_header_v2;
      if (!daph2) throw new Error('parquet data page header v2 is undefined')

      const { definitionLevels, repetitionLevels, dataPage } = readDataPageV2(
        compressedBytes, header, schemaPath, columnMetadata, compressors
      );

      // convert types, dereference dictionary, and assemble lists
      values = convertWithDictionary(dataPage, dictionary, element, daph2.encoding, utf8);
      if (repetitionLevels.length || definitionLevels?.length) {
        const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath);
        const repetitionPath = schemaPath.map(({ element }) => element.repetition_type);
        assembleLists(
          rowData, definitionLevels, repetitionLevels, values, repetitionPath, maxDefinitionLevel
        );
      } else {
        concat(rowData, values);
      }
    } else if (header.type === 'DICTIONARY_PAGE') {
      const diph = header.dictionary_page_header;
      if (!diph) throw new Error('parquet dictionary page header is undefined')

      const page = decompressPage(
        compressedBytes, Number(header.uncompressed_page_size), columnMetadata.codec, compressors
      );
      dictionary = readDictionaryPage(page, diph, columnMetadata, element.type_length);
    } else {
      throw new Error(`parquet unsupported page type: ${header.type}`)
    }
    reader.offset += header.compressed_page_size;
  }
  if (rowData.length < rowLimit) {
    throw new Error(`parquet row data length ${rowData.length} does not match row group limit ${rowLimit}}`)
  }
  if (rowData.length > rowLimit) {
    rowData.length = rowLimit; // truncate to row limit
  }
  return rowData
}

/**
 * Find the start byte offset for a column chunk.
 *
 * @param {ColumnMetaData} columnMetadata
 * @returns {[bigint, bigint]} byte offset range
 */
function getColumnRange({ dictionary_page_offset, data_page_offset, total_compressed_size }) {
  let columnOffset = dictionary_page_offset;
  if (!columnOffset || data_page_offset < columnOffset) {
    columnOffset = data_page_offset;
  }
  return [columnOffset, columnOffset + total_compressed_size]
}

/**
 * Read parquet header from a buffer.
 *
 * @import {ColumnMetaData, DecodedArray, DataReader, PageHeader, ParquetReadOptions, SchemaTree} from '../src/types.d.ts'
 * @param {DataReader} reader - parquet file reader
 * @returns {PageHeader} metadata object and bytes read
 */
function parquetHeader(reader) {
  const header = deserializeTCompactProtocol(reader);

  // Parse parquet header from thrift data
  const type = PageType[header.field_1];
  const uncompressed_page_size = header.field_2;
  const compressed_page_size = header.field_3;
  const crc = header.field_4;
  const data_page_header = header.field_5 && {
    num_values: header.field_5.field_1,
    encoding: Encoding[header.field_5.field_2],
    definition_level_encoding: Encoding[header.field_5.field_3],
    repetition_level_encoding: Encoding[header.field_5.field_4],
    statistics: header.field_5.field_5 && {
      max: header.field_5.field_5.field_1,
      min: header.field_5.field_5.field_2,
      null_count: header.field_5.field_5.field_3,
      distinct_count: header.field_5.field_5.field_4,
      max_value: header.field_5.field_5.field_5,
      min_value: header.field_5.field_5.field_6,
    },
  };
  const index_page_header = header.field_6;
  const dictionary_page_header = header.field_7 && {
    num_values: header.field_7.field_1,
    encoding: Encoding[header.field_7.field_2],
    is_sorted: header.field_7.field_3,
  };
  const data_page_header_v2 = header.field_8 && {
    num_values: header.field_8.field_1,
    num_nulls: header.field_8.field_2,
    num_rows: header.field_8.field_3,
    encoding: Encoding[header.field_8.field_4],
    definition_levels_byte_length: header.field_8.field_5,
    repetition_levels_byte_length: header.field_8.field_6,
    is_compressed: header.field_8.field_7 === undefined ? true : header.field_8.field_7, // default true
    statistics: header.field_8.field_8,
  };

  return {
    type,
    uncompressed_page_size,
    compressed_page_size,
    crc,
    data_page_header,
    index_page_header,
    dictionary_page_header,
    data_page_header_v2,
  }
}

/**
 * Read parquet data rows from a file-like object.
 * Reads the minimal number of row groups and columns to satisfy the request.
 *
 * Returns a void promise when complete, and to throw errors.
 * Data is returned in onComplete, not the return promise, because
 * if onComplete is undefined, we parse the data, and emit chunks, but skip
 * computing the row view directly. This saves on allocation if the caller
 * wants to cache the full chunks, and make their own view of the data from
 * the chunks.
 *
 * @param {ParquetReadOptions} options read options
 * @returns {Promise<void>} resolves when all requested rows and columns are parsed
 */
async function parquetRead(options) {
  if (!options.file) throw new Error('parquet file is required')

  // load metadata if not provided
  options.metadata ||= await parquetMetadataAsync(options.file);
  if (!options.metadata) throw new Error('parquet metadata not found')

  const { metadata, onComplete, rowEnd } = options;
  const rowStart = options.rowStart || 0;
  /** @type {any[][]} */
  const rowData = [];

  // find which row groups to read
  let groupStart = 0; // first row index of the current group
  for (const rowGroup of metadata.row_groups) {
    // number of rows in this row group
    const groupRows = Number(rowGroup.num_rows);
    // if row group overlaps with row range, read it
    if (groupStart + groupRows >= rowStart && (rowEnd === undefined || groupStart < rowEnd)) {
      // read row group
      const rowLimit = rowEnd && rowEnd - groupStart;
      const groupData = await readRowGroup(options, rowGroup, groupStart, rowLimit);
      if (onComplete) {
        // filter to rows in range
        const start = Math.max(rowStart - groupStart, 0);
        const end = rowEnd === undefined ? undefined : rowEnd - groupStart;
        concat(rowData, groupData.slice(start, end));
      }
    }
    groupStart += groupRows;
  }

  if (onComplete) onComplete(rowData);
}

/**
 * Read a row group from a file-like object.
 *
 * @param {ParquetReadOptions} options read options
 * @param {RowGroup} rowGroup row group to read
 * @param {number} groupStart row index of the first row in the group
 * @param {number} [rowLimit] max rows to read from this group
 * @returns {Promise<any[][]>} resolves to row data
 */
async function readRowGroup(options, rowGroup, groupStart, rowLimit) {
  const { file, metadata, columns } = options;
  if (!metadata) throw new Error('parquet metadata not found')
  if (rowLimit === undefined || rowLimit > rowGroup.num_rows) rowLimit = Number(rowGroup.num_rows);

  // loop through metadata to find min/max bytes to read
  let [groupStartByte, groupEndByte] = [file.byteLength, 0];
  rowGroup.columns.forEach(({ meta_data: columnMetadata }) => {
    if (!columnMetadata) throw new Error('parquet column metadata is undefined')
    // skip columns that are not requested
    if (columns && !columns.includes(columnMetadata.path_in_schema[0])) return

    const [columnStartByte, columnEndByte] = getColumnRange(columnMetadata).map(Number);
    groupStartByte = Math.min(groupStartByte, columnStartByte);
    groupEndByte = Math.max(groupEndByte, columnEndByte);
  });
  if (groupStartByte >= groupEndByte && columns?.length) {
    // TODO: should throw if any column is missing
    throw new Error(`parquet columns not found: ${columns.join(', ')}`)
  }
  // if row group size is less than 32mb, pre-load in one read
  let groupBuffer;
  if (groupEndByte - groupStartByte <= 1 << 25) {
    // pre-load row group byte data in one big read,
    // otherwise read column data individually
    groupBuffer = await file.slice(groupStartByte, groupEndByte);
  }

  const promises = [];
  // Top-level columns to assemble
  const { children } = getSchemaPath(metadata.schema, [])[0];
  const subcolumnNames = new Map(children.map(child => [child.element.name, getSubcolumns(child)]));
  const subcolumnData = new Map(); // columns to assemble as maps
  // read column data
  for (let columnIndex = 0; columnIndex < rowGroup.columns.length; columnIndex++) {
    const columnMetadata = rowGroup.columns[columnIndex].meta_data;
    if (!columnMetadata) throw new Error('parquet column metadata is undefined')

    // skip columns that are not requested
    const columnName = columnMetadata.path_in_schema[0];
    if (columns && !columns.includes(columnName)) continue

    const [columnStartByte, columnEndByte] = getColumnRange(columnMetadata).map(Number);
    const columnBytes = columnEndByte - columnStartByte;

    // skip columns larger than 1gb
    // TODO: stream process the data, returning only the requested rows
    if (columnBytes > 1 << 30) {
      console.warn(`parquet skipping huge column "${columnMetadata.path_in_schema}" ${columnBytes.toLocaleString()} bytes`);
      // TODO: set column to new Error('parquet column too large')
      continue
    }

    // use pre-loaded row group byte data if available, else read column data
    /** @type {Promise<ArrayBuffer>} */
    let buffer;
    let bufferOffset = 0;
    if (groupBuffer) {
      buffer = Promise.resolve(groupBuffer);
      bufferOffset = columnStartByte - groupStartByte;
    } else {
      // wrap awaitable to ensure it's a promise
      buffer = Promise.resolve(file.slice(columnStartByte, columnEndByte));
    }

    // read column data async
    promises.push(buffer.then(arrayBuffer => {
      const schemaPath = getSchemaPath(metadata.schema, columnMetadata.path_in_schema);
      const reader = { view: new DataView(arrayBuffer), offset: bufferOffset };
      /** @type {any[] | undefined} */
      let columnData = readColumn(reader, rowLimit, columnMetadata, schemaPath, options);
      // assert(columnData.length === Number(rowGroup.num_rows)

      // TODO: fast path for non-nested columns
      // Save column data for assembly
      const subcolumn = columnMetadata.path_in_schema.join('.');
      subcolumnData.set(subcolumn, columnData);
      columnData = undefined;

      const subcolumns = subcolumnNames.get(columnName);
      if (subcolumns?.every(name => subcolumnData.has(name))) {
        // We have all data needed to assemble a top level column
        assembleNested(subcolumnData, schemaPath[1]);
        columnData = subcolumnData.get(columnName);
        if (!columnData) {
          throw new Error(`parquet column data not assembled: ${columnName}`)
        }
      }

      // do not emit column data until structs are fully parsed
      if (!columnData) return
      // notify caller of column data
      options.onChunk?.({
        columnName,
        columnData,
        rowStart: groupStart,
        rowEnd: groupStart + columnData.length,
      });
    }));
  }
  await Promise.all(promises);
  if (options.onComplete) {
    // transpose columns into rows
    const groupData = new Array(rowLimit);
    const includedColumnNames = children
      .map(child => child.element.name)
      .filter(name => !columns || columns.includes(name));
    const columnOrder = columns || includedColumnNames;
    const includedColumns = columnOrder
      .map(name => includedColumnNames.includes(name) ? subcolumnData.get(name) : undefined);

    for (let row = 0; row < rowLimit; row++) {
      if (options.rowFormat === 'object') {
        // return each row as an object
        /** @type {Record<string, any>} */
        const rowData = {};
        columnOrder.forEach((name, index) => {
          rowData[name] = includedColumns[index]?.[row];
        });
        groupData[row] = rowData;
      } else {
        // return each row as an array
        groupData[row] = includedColumns.map(column => column?.[row]);
      }
    }
    return groupData
  }
  return []
}


/**
 * Return a list of sub-columns needed to construct a top-level column.
 *
 * @import {ParquetReadOptions, RowGroup, SchemaTree} from '../src/types.d.ts'
 * @param {SchemaTree} schema
 * @param {string[]} output
 * @returns {string[]}
 */
function getSubcolumns(schema, output = []) {
  if (schema.children.length) {
    for (const child of schema.children) {
      getSubcolumns(child, output);
    }
  } else {
    output.push(schema.path.join('.'));
  }
  return output
}

/**
 * Wraps parquetRead with orderBy support.
 * This is a parquet-aware query engine that can read a subset of rows and columns.
 * Accepts an optional orderBy column name to sort the results.
 * Note that using orderBy may SIGNIFICANTLY increase the query time.
 *
 * @param {ParquetReadOptions & { orderBy?: string }} options
 * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed
 */
async function parquetQuery(options) {
  const { file, rowStart, rowEnd, orderBy } = options;
  options.metadata ||= await parquetMetadataAsync(file);

  // TODO: Faster path for: no orderBy, no rowStart/rowEnd, one row group

  if (typeof orderBy === 'string') {
    // Fetch orderBy column first
    const orderColumn = await parquetReadObjects({ ...options, rowStart: undefined, rowEnd: undefined, columns: [orderBy] });

    // Compute row groups to fetch
    const sortedIndices = Array.from(orderColumn, (_, index) => index)
      .sort((a, b) => compare(orderColumn[a][orderBy], orderColumn[b][orderBy]))
      .slice(rowStart, rowEnd);

    const sparseData = await parquetReadRows({ ...options, rows: sortedIndices });
    const data = sortedIndices.map(index => sparseData[index]);
    return data
  } else {
    return await parquetReadObjects(options)
  }
}

/**
 * Reads a list rows from a parquet file, reading only the row groups that contain the rows.
 * Returns a sparse array of rows.
 * @import {ParquetReadOptions} from '../src/types.d.ts'
 * @param {ParquetReadOptions & { rows: number[] }} options
 * @returns {Promise<Record<string, any>[]>}
 */
async function parquetReadRows(options) {
  const { file, rows } = options;
  options.metadata ||= await parquetMetadataAsync(file);
  const { row_groups: rowGroups } = options.metadata;
  // Compute row groups to fetch
  const groupIncluded = Array(rowGroups.length).fill(false);
  let groupStart = 0;
  const groupEnds = rowGroups.map(group => groupStart += Number(group.num_rows));
  for (const index of rows) {
    const groupIndex = groupEnds.findIndex(end => index < end);
    groupIncluded[groupIndex] = true;
  }

  // Compute row ranges to fetch
  const rowRanges = [];
  let rangeStart;
  groupStart = 0;
  for (let i = 0; i < groupIncluded.length; i++) {
    const groupEnd = groupStart + Number(rowGroups[i].num_rows);
    if (groupIncluded[i]) {
      if (rangeStart === undefined) {
        rangeStart = groupStart;
      }
    } else {
      if (rangeStart !== undefined) {
        rowRanges.push([rangeStart, groupEnd]);
        rangeStart = undefined;
      }
    }
    groupStart = groupEnd;
  }
  if (rangeStart !== undefined) {
    rowRanges.push([rangeStart, groupStart]);
  }

  // Fetch by row group and map to rows
  const sparseData = new Array(Number(options.metadata.num_rows));
  for (const [rangeStart, rangeEnd] of rowRanges) {
    // TODO: fetch in parallel
    const groupData = await parquetReadObjects({ ...options, rowStart: rangeStart, rowEnd: rangeEnd });
    for (let i = rangeStart; i < rangeEnd; i++) {
      sparseData[i] = groupData[i - rangeStart];
      sparseData[i].__index__ = i;
    }
  }
  return sparseData
}

/**
 * @param {any} a
 * @param {any} b
 * @returns {number}
 */
function compare(a, b) {
  if (a < b) return -1
  if (a > b) return 1
  return 1 // TODO: how to handle nulls?
}

/**
 * @param {ParquetReadOptions} options
 * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed
*/
function parquetReadObjects(options) {
  return new Promise((onComplete, reject) => {
    parquetRead({
      rowFormat: 'object',
      ...options,
      onComplete,
    }).catch(reject);
  })
}
/**
 * Explicitly export types for use in downstream typescript projects through
 * `import { ParquetReadOptions } from 'hyparquet'` for example.
 *
 * @template {any} T
 * @typedef {import('../src/types.d.ts').Awaitable<T>} Awaitable<T>
 */

/**
 * @typedef {import('../src/types.d.ts').AsyncBuffer} AsyncBuffer
 * @typedef {import('../src/types.d.ts').DataReader} DataReader
 * @typedef {import('../src/types.d.ts').FileMetaData} FileMetaData
 * @typedef {import('../src/types.d.ts').SchemaTree} SchemaTree
 * @typedef {import('../src/types.d.ts').SchemaElement} SchemaElement
 * @typedef {import('../src/types.d.ts').ParquetType} ParquetType
 * @typedef {import('../src/types.d.ts').FieldRepetitionType} FieldRepetitionType
 * @typedef {import('../src/types.d.ts').ConvertedType} ConvertedType
 * @typedef {import('../src/types.d.ts').TimeUnit} TimeUnit
 * @typedef {import('../src/types.d.ts').LogicalType} LogicalType
 * @typedef {import('../src/types.d.ts').LogicalTypeType} LogicalTypeType
 * @typedef {import('../src/types.d.ts').RowGroup} RowGroup
 * @typedef {import('../src/types.d.ts').ColumnChunk} ColumnChunk
 * @typedef {import('../src/types.d.ts').ColumnMetaData} ColumnMetaData
 * @typedef {import('../src/types.d.ts').Encoding} Encoding
 * @typedef {import('../src/types.d.ts').CompressionCodec} CompressionCodec
 * @typedef {import('../src/types.d.ts').Compressors} Compressors
 * @typedef {import('../src/types.d.ts').Statistics} Statistics
 * @typedef {import('../src/types.d.ts').PageType} PageType
 * @typedef {import('../src/types.d.ts').PageHeader} PageHeader
 * @typedef {import('../src/types.d.ts').DataPageHeader} DataPageHeader
 * @typedef {import('../src/types.d.ts').DictionaryPageHeader} DictionaryPageHeader
 * @typedef {import('../src/types.d.ts').DecodedArray} DecodedArray
 * @typedef {import('../src/types.d.ts').OffsetIndex} OffsetIndex
 * @typedef {import('../src/types.d.ts').ColumnIndex} ColumnIndex
 * @typedef {import('../src/types.d.ts').BoundaryOrder} BoundaryOrder
 * @typedef {import('../src/types.d.ts').ColumnData} ColumnData
 * @typedef {import('../src/types.d.ts').ParquetReadOptions} ParquetReadOptions
 */

const geometryTypePoint = 1;
const geometryTypeLineString = 2;
const geometryTypePolygon = 3;
const geometryTypeMultiPolygon = 6;

/**
 * Minimal WKB (Well Known Binary) decoder supporting Polygon and MultiPolygon.
 * Supports both big-endian (byteOrder=0) and little-endian (byteOrder=1).
 * @import { Geometry } from './geojson.js'
 * @param {Uint8Array} wkb 
 * @returns {Geometry} GeoJSON geometry object
 */
function decodeWKB(wkb) {
  let offset = 0;

  // Byte order: 0 = big-endian, 1 = little-endian
  const byteOrder = wkb[offset]; offset += 1;
  const isLittleEndian = (byteOrder === 1);

  // Helper functions
  /**
   * Read a 32-bit unsigned integer from buffer at given offset
   * @param {Uint8Array} buf
   * @param {number} off
   */
  function readUInt32(buf, off) {
    const dv = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return dv.getUint32(off, isLittleEndian)
  }

  /**
   * Read a 64-bit double from buffer at given offset
   * @param {Uint8Array} buf
   * @param {number} off
   */
  function readDouble(buf, off) {
    const dv = new DataView(buf.buffer, buf.byteOffset, buf.byteLength);
    return dv.getFloat64(off, isLittleEndian)
  }

  // Read geometry type
  const geometryType = readUInt32(wkb, offset);
  offset += 4;

  // WKB geometry types (OGC):
  if (geometryType === geometryTypePoint) {
    // Point
    const x = readDouble(wkb, offset); offset += 8;
    const y = readDouble(wkb, offset); offset += 8;
    return { type: 'Point', coordinates: [x,y] }
  } else if (geometryType === geometryTypeLineString) {
    // LineString
    const numPoints = readUInt32(wkb, offset); offset += 4;
    const coords = [];
    for (let i = 0; i < numPoints; i++) {
      const x = readDouble(wkb, offset); offset += 8;
      const y = readDouble(wkb, offset); offset += 8;
      coords.push([x,y]);
    }
    return { type: 'LineString', coordinates: coords }
  } else if (geometryType === geometryTypePolygon) {
    // Polygon
    const numRings = readUInt32(wkb, offset); offset += 4;
    const coords = [];
    for (let r = 0; r < numRings; r++) {
      const numPoints = readUInt32(wkb, offset); offset += 4;
      const ring = [];
      for (let p = 0; p < numPoints; p++) {
        const x = readDouble(wkb, offset); offset += 8;
        const y = readDouble(wkb, offset); offset += 8;
        ring.push([x,y]);
      }
      coords.push(ring);
    }
    return { type: 'Polygon', coordinates: coords }

  } else if (geometryType === geometryTypeMultiPolygon) {
    // MultiPolygon
    const numPolygons = readUInt32(wkb, offset); offset += 4;
    const polygons = [];
    for (let i = 0; i < numPolygons; i++) {
      // Each polygon has its own byte order & geometry type
      const pgByteOrder = wkb[offset]; offset += 1;
      const pgIsLittleEndian = (pgByteOrder === 1);
      const pgType = (function() {
        const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength);
        const val = dv.getUint32(offset, pgIsLittleEndian);
        offset += 4;
        return val
      })();

      if (pgType !== 3) throw new Error(`Expected Polygon in MultiPolygon, got ${pgType}`)

      const numRings = (function() {
        const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength);
        const val = dv.getUint32(offset, pgIsLittleEndian);
        offset += 4;
        return val
      })();

      const pgCoords = [];
      for (let r = 0; r < numRings; r++) {
        const numPoints = (function() {
          const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength);
          const val = dv.getUint32(offset, pgIsLittleEndian);
          offset += 4;
          return val
        })();
        const ring = [];
        for (let p = 0; p < numPoints; p++) {
          const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength);
          const x = dv.getFloat64(offset, pgIsLittleEndian); offset += 8;
          const y = dv.getFloat64(offset, pgIsLittleEndian); offset += 8;
          ring.push([x,y]);
        }
        pgCoords.push(ring);
      }
      polygons.push(pgCoords);
    }
    return { type: 'MultiPolygon', coordinates: polygons }
  } else {
    throw new Error("Unsupported geometry type: " + geometryType)
  }
}

/**
 * @import { AsyncBuffer } from 'hyparquet'
 * @import { GeoJSON } from './geojson.js'
 * @param {AsyncBuffer} asyncBuffer 
 * @returns {Promise<GeoJSON>}
 */
async function geoparquet2geojson(asyncBuffer) {
  const metadata = await parquetMetadataAsync(asyncBuffer);
  const geoMetadata = metadata.key_value_metadata?.find(kv => kv.key === 'geo');
  if (!geoMetadata) {
    throw new Error('Invalid GeoParquet file: missing "geo" metadata')
  }
  const geoSchema = JSON.parse(geoMetadata.value || '{}');
  console.log('Geoparquet schema:', geoSchema);

  // Read all parquet data
  const data = await parquetQuery({ file: asyncBuffer });
  console.log('Geoparquet data:', toJson(data));

  // Convert parquet data to GeoJSON
  /**
   * @import { Feature } from './geojson.js'
   * @type {Feature[]}
   */
  const features = [];

  // According to the schema, the primary geometry column is 'geometry'
  // We'll assume WKB encoding, and other columns are properties
  const primaryColumn = geoSchema.primary_column || 'geometry';

  for (const row of data) {
    const wkbStr = row[primaryColumn];
    if (!wkbStr) {
      // No geometry
      continue
    }

    // Convert the UTF-8 string with weird chars back to binary
    // The parquetQuery returns strings. We'll treat as binary data with char codes.
    const binary = new Uint8Array(wkbStr.length);
    for (let i = 0; i < wkbStr.length; i++) {
      binary[i] = wkbStr.charCodeAt(i);
    }

    // const geom2 = wkx.parse(binary.buffer)
    // console.log('WKB:', binary, 'WKX:', geom2)

    const geometry = decodeWKB(binary);

    // Extract properties (all fields except geometry)
    /** @type {Record<string, any>} */
    const properties = {};
    for (const key of Object.keys(row)) {
      if (key !== primaryColumn) {
        properties[key] = row[key];
      }
    }

    /** @type {Feature} */
    const feature = {
      type: 'Feature',
      geometry,
      properties
    };

    features.push(feature);
  }

  return {
    type: 'FeatureCollection',
    features,
  }
}

let map;

window.initMap = async function loadGeoParquet() {
  // Create a new map centered on a default location
  const div = document.getElementById('map');
  map = new google.maps.Map(div, {
    center: { lat: 40.7128, lng: -74.0060 },
    zoom: 5,
  });

  // URL or path to your GeoParquet file
  const parquetUrl = 'examples/example.parquet';

  try {
    // Read the GeoParquet file and convert to GeoJSON
    const asyncBuffer = await asyncBufferFromUrl({ url: parquetUrl });
    const geojson = await geoparquet2geojson(asyncBuffer);

    // Add the GeoJSON data to the map
    map.data.addGeoJson(geojson);
  } catch (error) {
    console.error('Error loading or parsing GeoParquet file:', error);
  }
};
//# sourceMappingURL=bundle.min.js.map
