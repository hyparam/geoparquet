{"version":3,"file":"bundle.min.js","sources":["../node_modules/hyparquet/src/constants.js","../node_modules/hyparquet/src/convert.js","../node_modules/hyparquet/src/schema.js","../node_modules/hyparquet/src/thrift.js","../node_modules/hyparquet/src/metadata.js","../node_modules/hyparquet/src/utils.js","../node_modules/hyparquet/src/plan.js","../node_modules/hyparquet/src/assemble.js","../node_modules/hyparquet/src/delta.js","../node_modules/hyparquet/src/encoding.js","../node_modules/hyparquet/src/plain.js","../node_modules/hyparquet/src/snappy.js","../node_modules/hyparquet/src/datapage.js","../node_modules/hyparquet/src/column.js","../node_modules/hyparquet/src/rowgroup.js","../node_modules/hyparquet/src/read.js","../node_modules/hyparquet/src/query.js","../node_modules/hyparquet/src/index.js","../src/wkb.js","demo.js","../src/toGeoJson.js"],"sourcesContent":["/** @type {import('../src/types.d.ts').ParquetType[]} */\nexport const ParquetType = [\n  'BOOLEAN',\n  'INT32',\n  'INT64',\n  'INT96', // deprecated\n  'FLOAT',\n  'DOUBLE',\n  'BYTE_ARRAY',\n  'FIXED_LEN_BYTE_ARRAY',\n]\n\n/** @type {import('../src/types.d.ts').Encoding[]} */\nexport const Encoding = [\n  'PLAIN',\n  'GROUP_VAR_INT', // deprecated\n  'PLAIN_DICTIONARY',\n  'RLE',\n  'BIT_PACKED', // deprecated\n  'DELTA_BINARY_PACKED',\n  'DELTA_LENGTH_BYTE_ARRAY',\n  'DELTA_BYTE_ARRAY',\n  'RLE_DICTIONARY',\n  'BYTE_STREAM_SPLIT',\n]\n\n/** @type {import('../src/types.d.ts').FieldRepetitionType[]} */\nexport const FieldRepetitionType = [\n  'REQUIRED',\n  'OPTIONAL',\n  'REPEATED',\n]\n\n/** @type {import('../src/types.d.ts').ConvertedType[]} */\nexport const ConvertedType = [\n  'UTF8',\n  'MAP',\n  'MAP_KEY_VALUE',\n  'LIST',\n  'ENUM',\n  'DECIMAL',\n  'DATE',\n  'TIME_MILLIS',\n  'TIME_MICROS',\n  'TIMESTAMP_MILLIS',\n  'TIMESTAMP_MICROS',\n  'UINT_8',\n  'UINT_16',\n  'UINT_32',\n  'UINT_64',\n  'INT_8',\n  'INT_16',\n  'INT_32',\n  'INT_64',\n  'JSON',\n  'BSON',\n  'INTERVAL',\n]\n\n/** @type {import('../src/types.d.ts').CompressionCodec[]} */\nexport const CompressionCodec = [\n  'UNCOMPRESSED',\n  'SNAPPY',\n  'GZIP',\n  'LZO',\n  'BROTLI',\n  'LZ4',\n  'ZSTD',\n  'LZ4_RAW',\n]\n\n/** @type {import('../src/types.d.ts').PageType[]} */\nexport const PageType = [\n  'DATA_PAGE',\n  'INDEX_PAGE',\n  'DICTIONARY_PAGE',\n  'DATA_PAGE_V2',\n]\n\n/** @type {import('../src/types.d.ts').BoundaryOrder[]} */\nexport const BoundaryOrder = [\n  'UNORDERED',\n  'ASCENDING',\n  'DESCENDING',\n]\n","/**\n * @import {ColumnDecoder, DecodedArray, Encoding, ParquetParsers, SchemaElement} from '../src/types.d.ts'\n */\n\n/**\n * Default type parsers when no custom ones are given\n * @type ParquetParsers\n */\nexport const DEFAULT_PARSERS = {\n  timestampFromMilliseconds(millis) {\n    return new Date(Number(millis))\n  },\n  timestampFromMicroseconds(micros) {\n    return new Date(Number(micros / 1000n))\n  },\n  timestampFromNanoseconds(nanos) {\n    return new Date(Number(nanos / 1000000n))\n  },\n  dateFromDays(days) {\n    const dayInMillis = 86400000\n    return new Date(days * dayInMillis)\n  },\n}\n\n/**\n * Convert known types from primitive to rich, and dereference dictionary.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {DecodedArray | undefined} dictionary\n * @param {Encoding} encoding\n * @param {ColumnDecoder} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convertWithDictionary(data, dictionary, encoding, columnDecoder) {\n  if (dictionary && encoding.endsWith('_DICTIONARY')) {\n    let output = data\n    if (data instanceof Uint8Array && !(dictionary instanceof Uint8Array)) {\n      // @ts-expect-error upgrade data to match dictionary type with fancy constructor\n      output = new dictionary.constructor(data.length)\n    }\n    for (let i = 0; i < data.length; i++) {\n      output[i] = dictionary[data[i]]\n    }\n    return output\n  } else {\n    return convert(data, columnDecoder)\n  }\n}\n\n/**\n * Convert known types from primitive to rich.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {Pick<ColumnDecoder, \"element\" | \"utf8\" | \"parsers\">} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convert(data, columnDecoder) {\n  const { element, parsers, utf8 = true } = columnDecoder\n  const { type, converted_type: ctype, logical_type: ltype } = element\n  if (ctype === 'DECIMAL') {\n    const scale = element.scale || 0\n    const factor = 10 ** -scale\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      if (data[0] instanceof Uint8Array) {\n        arr[i] = parseDecimal(data[i]) * factor\n      } else {\n        arr[i] = Number(data[i]) * factor\n      }\n    }\n    return arr\n  }\n  if (!ctype && type === 'INT96') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parsers.timestampFromNanoseconds(parseInt96Nanos(data[i]))\n    }\n    return arr\n  }\n  if (ctype === 'DATE') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parsers.dateFromDays(data[i])\n    }\n    return arr\n  }\n  if (ctype === 'TIMESTAMP_MILLIS') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parsers.timestampFromMilliseconds(data[i])\n    }\n    return arr\n  }\n  if (ctype === 'TIMESTAMP_MICROS') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parsers.timestampFromMicroseconds(data[i])\n    }\n    return arr\n  }\n  if (ctype === 'JSON') {\n    const decoder = new TextDecoder()\n    return data.map(v => JSON.parse(decoder.decode(v)))\n  }\n  if (ctype === 'BSON') {\n    throw new Error('parquet bson not supported')\n  }\n  if (ctype === 'INTERVAL') {\n    throw new Error('parquet interval not supported')\n  }\n  if (ctype === 'UTF8' || ltype?.type === 'STRING' || utf8 && type === 'BYTE_ARRAY') {\n    const decoder = new TextDecoder()\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = data[i] && decoder.decode(data[i])\n    }\n    return arr\n  }\n  if (ctype === 'UINT_64' || ltype?.type === 'INTEGER' && ltype.bitWidth === 64 && !ltype.isSigned) {\n    if (data instanceof BigInt64Array) {\n      return new BigUint64Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new BigUint64Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = BigInt(data[i])\n    return arr\n  }\n  if (ctype === 'UINT_32' || ltype?.type === 'INTEGER' && ltype.bitWidth === 32 && !ltype.isSigned) {\n    if (data instanceof Int32Array) {\n      return new Uint32Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new Uint32Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = data[i]\n    return arr\n  }\n  if (ltype?.type === 'FLOAT16') {\n    return Array.from(data).map(parseFloat16)\n  }\n  if (ltype?.type === 'TIMESTAMP') {\n    const { unit } = ltype\n    /** @type {ParquetParsers[keyof ParquetParsers]} */\n    let parser = parsers.timestampFromMilliseconds\n    if (unit === 'MICROS') parser = parsers.timestampFromMicroseconds\n    if (unit === 'NANOS') parser = parsers.timestampFromNanoseconds\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parser(data[i])\n    }\n    return arr\n  }\n  return data\n}\n\n/**\n * @param {Uint8Array} bytes\n * @returns {number}\n */\nexport function parseDecimal(bytes) {\n  let value = 0\n  for (const byte of bytes) {\n    value = value * 256 + byte\n  }\n\n  // handle signed\n  const bits = bytes.length * 8\n  if (value >= 2 ** (bits - 1)) {\n    value -= 2 ** bits\n  }\n\n  return value\n}\n\n/**\n * Converts INT96 date format (hi 32bit days, lo 64bit nanos) to nanos since epoch\n * @param {bigint} value\n * @returns {bigint}\n */\nfunction parseInt96Nanos(value) {\n  const days = (value >> 64n) - 2440588n\n  const nano = value & 0xffffffffffffffffn\n  return days * 86400000000000n + nano\n}\n\n/**\n * @param {Uint8Array | undefined} bytes\n * @returns {number | undefined}\n */\nexport function parseFloat16(bytes) {\n  if (!bytes) return undefined\n  const int16 = bytes[1] << 8 | bytes[0]\n  const sign = int16 >> 15 ? -1 : 1\n  const exp = int16 >> 10 & 0x1f\n  const frac = int16 & 0x3ff\n  if (exp === 0) return sign * 2 ** -14 * (frac / 1024) // subnormals\n  if (exp === 0x1f) return frac ? NaN : sign * Infinity\n  return sign * 2 ** (exp - 15) * (1 + frac / 1024)\n}\n","/**\n * Build a tree from the schema elements.\n *\n * @import {SchemaElement, SchemaTree} from '../src/types.d.ts'\n * @param {SchemaElement[]} schema\n * @param {number} rootIndex index of the root element\n * @param {string[]} path path to the element\n * @returns {SchemaTree} tree of schema elements\n */\nfunction schemaTree(schema, rootIndex, path) {\n  const element = schema[rootIndex]\n  const children = []\n  let count = 1\n\n  // Read the specified number of children\n  if (element.num_children) {\n    while (children.length < element.num_children) {\n      const childElement = schema[rootIndex + count]\n      const child = schemaTree(schema, rootIndex + count, [...path, childElement.name])\n      count += child.count\n      children.push(child)\n    }\n  }\n\n  return { count, element, children, path }\n}\n\n/**\n * Get schema elements from the root to the given element name.\n *\n * @param {SchemaElement[]} schema\n * @param {string[]} name path to the element\n * @returns {SchemaTree[]} list of schema elements\n */\nexport function getSchemaPath(schema, name) {\n  let tree = schemaTree(schema, 0, [])\n  const path = [tree]\n  for (const part of name) {\n    const child = tree.children.find(child => child.element.name === part)\n    if (!child) throw new Error(`parquet schema element not found: ${name}`)\n    path.push(child)\n    tree = child\n  }\n  return path\n}\n\n/**\n * Get the max repetition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max repetition level\n */\nexport function getMaxRepetitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath) {\n    if (element.repetition_type === 'REPEATED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Get the max definition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max definition level\n */\nexport function getMaxDefinitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath.slice(1)) {\n    if (element.repetition_type !== 'REQUIRED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Check if a column is list-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if list-like\n */\nexport function isListLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'LIST') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length > 1) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Check if a column is map-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if map-like\n */\nexport function isMapLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'MAP') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length !== 2) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  const keyChild = firstChild.children.find(child => child.element.name === 'key')\n  if (keyChild?.element.repetition_type === 'REPEATED') return false\n\n  const valueChild = firstChild.children.find(child => child.element.name === 'value')\n  if (valueChild?.element.repetition_type === 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Returns true if a column is non-nested.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {boolean}\n */\nexport function isFlatColumn(schemaPath) {\n  if (schemaPath.length !== 2) return false\n  const [, column] = schemaPath\n  if (column.element.repetition_type === 'REPEATED') return false\n  if (column.children.length) return false\n  return true\n}\n","// TCompactProtocol types\nexport const CompactType = {\n  STOP: 0,\n  TRUE: 1,\n  FALSE: 2,\n  BYTE: 3,\n  I16: 4,\n  I32: 5,\n  I64: 6,\n  DOUBLE: 7,\n  BINARY: 8,\n  LIST: 9,\n  SET: 10,\n  MAP: 11,\n  STRUCT: 12,\n  UUID: 13,\n}\n\n/**\n * Parse TCompactProtocol\n *\n * @param {DataReader} reader\n * @returns {{ [key: `field_${number}`]: any }}\n */\nexport function deserializeTCompactProtocol(reader) {\n  let lastFid = 0\n  /** @type {ThriftObject} */\n  const value = {}\n\n  while (reader.offset < reader.view.byteLength) {\n    // Parse each field based on its type and add to the result object\n    const [type, fid, newLastFid] = readFieldBegin(reader, lastFid)\n    lastFid = newLastFid\n\n    if (type === CompactType.STOP) {\n      break\n    }\n\n    // Handle the field based on its type\n    value[`field_${fid}`] = readElement(reader, type)\n  }\n\n  return value\n}\n\n/**\n * Read a single element based on its type\n *\n * @import {DataReader, ThriftObject, ThriftType} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} type\n * @returns {ThriftType}\n */\nfunction readElement(reader, type) {\n  switch (type) {\n  case CompactType.TRUE:\n    return true\n  case CompactType.FALSE:\n    return false\n  case CompactType.BYTE:\n    // read byte directly\n    return reader.view.getInt8(reader.offset++)\n  case CompactType.I16:\n  case CompactType.I32:\n    return readZigZag(reader)\n  case CompactType.I64:\n    return readZigZagBigInt(reader)\n  case CompactType.DOUBLE: {\n    const value = reader.view.getFloat64(reader.offset, true)\n    reader.offset += 8\n    return value\n  }\n  case CompactType.BINARY: {\n    const stringLength = readVarInt(reader)\n    const strBytes = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, stringLength)\n    reader.offset += stringLength\n    return strBytes\n  }\n  case CompactType.LIST: {\n    const [elemType, listSize] = readCollectionBegin(reader)\n    const boolType = elemType === CompactType.TRUE || elemType === CompactType.FALSE\n    const values = new Array(listSize)\n    for (let i = 0; i < listSize; i++) {\n      values[i] = boolType ? readElement(reader, CompactType.BYTE) === 1 : readElement(reader, elemType)\n    }\n    return values\n  }\n  case CompactType.STRUCT: {\n    /** @type {ThriftObject} */\n    const structValues = {}\n    let structLastFid = 0\n    while (true) {\n      let structFieldType, structFid\n      [structFieldType, structFid, structLastFid] = readFieldBegin(reader, structLastFid)\n      if (structFieldType === CompactType.STOP) {\n        break\n      }\n      structValues[`field_${structFid}`] = readElement(reader, structFieldType)\n    }\n    return structValues\n  }\n  // TODO: MAP, SET, UUID\n  default:\n    throw new Error(`thrift unhandled type: ${type}`)\n  }\n}\n\n/**\n * Var int, also known as Unsigned LEB128.\n * Var ints take 1 to 5 bytes (int32) or 1 to 10 bytes (int64).\n * Reads groups of 7 low bits until high bit is 0.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readVarInt(reader) {\n  let result = 0\n  let shift = 0\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= (byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7\n  }\n}\n\n/**\n * Read a varint as a bigint.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nfunction readVarBigInt(reader) {\n  let result = 0n\n  let shift = 0n\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= BigInt(byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7n\n  }\n}\n\n/**\n * Values of type int32 and int64 are transformed to a zigzag int.\n * A zigzag int folds positive and negative numbers into the positive number space.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readZigZag(reader) {\n  const zigzag = readVarInt(reader)\n  // convert zigzag to int\n  return zigzag >>> 1 ^ -(zigzag & 1)\n}\n\n/**\n * A zigzag int folds positive and negative numbers into the positive number space.\n * This version returns a BigInt.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nexport function readZigZagBigInt(reader) {\n  const zigzag = readVarBigInt(reader)\n  // convert zigzag to int\n  return zigzag >> 1n ^ -(zigzag & 1n)\n}\n\n/**\n * Get thrift type from half a byte\n *\n * @param {number} byte\n * @returns {number}\n */\nfunction getCompactType(byte) {\n  return byte & 0x0f\n}\n\n/**\n * Read field type and field id\n *\n * @param {DataReader} reader\n * @param {number} lastFid\n * @returns {[number, number, number]} [type, fid, newLastFid]\n */\nfunction readFieldBegin(reader, lastFid) {\n  const type = reader.view.getUint8(reader.offset++)\n  if ((type & 0x0f) === CompactType.STOP) {\n    // STOP also ends a struct\n    return [0, 0, lastFid]\n  }\n  const delta = type >> 4\n  let fid // field id\n  if (delta) {\n    // add delta to last field id\n    fid = lastFid + delta\n  } else {\n    throw new Error('non-delta field id not supported')\n  }\n  return [getCompactType(type), fid, fid]\n}\n\n/**\n * Read collection type and size\n *\n * @param {DataReader} reader\n * @returns {[number, number]} [type, size]\n */\nfunction readCollectionBegin(reader) {\n  const sizeType = reader.view.getUint8(reader.offset++)\n  const size = sizeType >> 4\n  const type = getCompactType(sizeType)\n  if (size === 15) {\n    const newSize = readVarInt(reader)\n    return [type, newSize]\n  }\n  return [type, size]\n}\n","import { CompressionCodec, ConvertedType, Encoding, FieldRepetitionType, PageType, ParquetType } from './constants.js'\nimport { DEFAULT_PARSERS, parseDecimal, parseFloat16 } from './convert.js'\nimport { getSchemaPath } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\n\nexport const defaultInitialFetchSize = 1 << 19 // 512kb\n\n/**\n * Read parquet metadata from an async buffer.\n *\n * An AsyncBuffer is like an ArrayBuffer, but the slices are loaded\n * asynchronously, possibly over the network.\n *\n * You must provide the byteLength of the buffer, typically from a HEAD request.\n *\n * In theory, you could use suffix-range requests to fetch the end of the file,\n * and save a round trip. But in practice, this doesn't work because chrome\n * deems suffix-range requests as a not-safe-listed header, and will require\n * a pre-flight. So the byteLength is required.\n *\n * To make this efficient, we initially request the last 512kb of the file,\n * which is likely to contain the metadata. If the metadata length exceeds the\n * initial fetch, 512kb, we request the rest of the metadata from the AsyncBuffer.\n *\n * This ensures that we either make one 512kb initial request for the metadata,\n * or a second request for up to the metadata size.\n *\n * @param {AsyncBuffer} asyncBuffer parquet file contents\n * @param {MetadataOptions & { initialFetchSize?: number }} options initial fetch size in bytes (default 512kb)\n * @returns {Promise<FileMetaData>} parquet metadata object\n */\nexport async function parquetMetadataAsync(asyncBuffer, { parsers, initialFetchSize = defaultInitialFetchSize } = {}) {\n  if (!asyncBuffer || !(asyncBuffer.byteLength >= 0)) throw new Error('parquet expected AsyncBuffer')\n\n  // fetch last bytes (footer) of the file\n  const footerOffset = Math.max(0, asyncBuffer.byteLength - initialFetchSize)\n  const footerBuffer = await asyncBuffer.slice(footerOffset, asyncBuffer.byteLength)\n\n  // Check for parquet magic number \"PAR1\"\n  const footerView = new DataView(footerBuffer)\n  if (footerView.getUint32(footerBuffer.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLength = footerView.getUint32(footerBuffer.byteLength - 8, true)\n  if (metadataLength > asyncBuffer.byteLength - 8) {\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${asyncBuffer.byteLength - 8}`)\n  }\n\n  // check if metadata size fits inside the initial fetch\n  if (metadataLength + 8 > initialFetchSize) {\n    // fetch the rest of the metadata\n    const metadataOffset = asyncBuffer.byteLength - metadataLength - 8\n    const metadataBuffer = await asyncBuffer.slice(metadataOffset, footerOffset)\n    // combine initial fetch with the new slice\n    const combinedBuffer = new ArrayBuffer(metadataLength + 8)\n    const combinedView = new Uint8Array(combinedBuffer)\n    combinedView.set(new Uint8Array(metadataBuffer))\n    combinedView.set(new Uint8Array(footerBuffer), footerOffset - metadataOffset)\n    return parquetMetadata(combinedBuffer, { parsers })\n  } else {\n    // parse metadata from the footer\n    return parquetMetadata(footerBuffer, { parsers })\n  }\n}\n\n/**\n * Read parquet metadata from a buffer synchronously.\n *\n * @param {ArrayBuffer} arrayBuffer parquet file footer\n * @param {MetadataOptions} options metadata parsing options\n * @returns {FileMetaData} parquet metadata object\n */\nexport function parquetMetadata(arrayBuffer, { parsers } = {}) {\n  if (!(arrayBuffer instanceof ArrayBuffer)) throw new Error('parquet expected ArrayBuffer')\n  const view = new DataView(arrayBuffer)\n\n  // Use default parsers if not given\n  parsers = { ...DEFAULT_PARSERS, ...parsers }\n\n  // Validate footer magic number \"PAR1\"\n  if (view.byteLength < 8) {\n    throw new Error('parquet file is too short')\n  }\n  if (view.getUint32(view.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLengthOffset = view.byteLength - 8\n  const metadataLength = view.getUint32(metadataLengthOffset, true)\n  if (metadataLength > view.byteLength - 8) {\n    // {metadata}, metadata_length, PAR1\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${view.byteLength - 8}`)\n  }\n\n  const metadataOffset = metadataLengthOffset - metadataLength\n  const reader = { view, offset: metadataOffset }\n  const metadata = deserializeTCompactProtocol(reader)\n  const decoder = new TextDecoder()\n  function decode(/** @type {Uint8Array} */ value) {\n    return value && decoder.decode(value)\n  }\n\n  // Parse metadata from thrift data\n  const version = metadata.field_1\n  /** @type {SchemaElement[]} */\n  const schema = metadata.field_2.map((/** @type {any} */ field) => ({\n    type: ParquetType[field.field_1],\n    type_length: field.field_2,\n    repetition_type: FieldRepetitionType[field.field_3],\n    name: decode(field.field_4),\n    num_children: field.field_5,\n    converted_type: ConvertedType[field.field_6],\n    scale: field.field_7,\n    precision: field.field_8,\n    field_id: field.field_9,\n    logical_type: logicalType(field.field_10),\n  }))\n  // schema element per column index\n  const columnSchema = schema.filter(e => e.type)\n  const num_rows = metadata.field_3\n  const row_groups = metadata.field_4.map((/** @type {any} */ rowGroup) => ({\n    columns: rowGroup.field_1.map((/** @type {any} */ column, /** @type {number} */ columnIndex) => ({\n      file_path: decode(column.field_1),\n      file_offset: column.field_2,\n      meta_data: column.field_3 && {\n        type: ParquetType[column.field_3.field_1],\n        encodings: column.field_3.field_2?.map((/** @type {number} */ e) => Encoding[e]),\n        path_in_schema: column.field_3.field_3.map(decode),\n        codec: CompressionCodec[column.field_3.field_4],\n        num_values: column.field_3.field_5,\n        total_uncompressed_size: column.field_3.field_6,\n        total_compressed_size: column.field_3.field_7,\n        key_value_metadata: column.field_3.field_8,\n        data_page_offset: column.field_3.field_9,\n        index_page_offset: column.field_3.field_10,\n        dictionary_page_offset: column.field_3.field_11,\n        statistics: convertStats(column.field_3.field_12, columnSchema[columnIndex], parsers),\n        encoding_stats: column.field_3.field_13?.map((/** @type {any} */ encodingStat) => ({\n          page_type: PageType[encodingStat.field_1],\n          encoding: Encoding[encodingStat.field_2],\n          count: encodingStat.field_3,\n        })),\n        bloom_filter_offset: column.field_3.field_14,\n        bloom_filter_length: column.field_3.field_15,\n        size_statistics: column.field_3.field_16 && {\n          unencoded_byte_array_data_bytes: column.field_3.field_16.field_1,\n          repetition_level_histogram: column.field_3.field_16.field_2,\n          definition_level_histogram: column.field_3.field_16.field_3,\n        },\n      },\n      offset_index_offset: column.field_4,\n      offset_index_length: column.field_5,\n      column_index_offset: column.field_6,\n      column_index_length: column.field_7,\n      crypto_metadata: column.field_8,\n      encrypted_column_metadata: column.field_9,\n    })),\n    total_byte_size: rowGroup.field_2,\n    num_rows: rowGroup.field_3,\n    sorting_columns: rowGroup.field_4?.map((/** @type {any} */ sortingColumn) => ({\n      column_idx: sortingColumn.field_1,\n      descending: sortingColumn.field_2,\n      nulls_first: sortingColumn.field_3,\n    })),\n    file_offset: rowGroup.field_5,\n    total_compressed_size: rowGroup.field_6,\n    ordinal: rowGroup.field_7,\n  }))\n  const key_value_metadata = metadata.field_5?.map((/** @type {any} */ keyValue) => ({\n    key: decode(keyValue.field_1),\n    value: decode(keyValue.field_2),\n  }))\n  const created_by = decode(metadata.field_6)\n\n  return {\n    version,\n    schema,\n    num_rows,\n    row_groups,\n    key_value_metadata,\n    created_by,\n    metadata_length: metadataLength,\n  }\n}\n\n/**\n * Return a tree of schema elements from parquet metadata.\n *\n * @param {{schema: SchemaElement[]}} metadata parquet metadata object\n * @returns {SchemaTree} tree of schema elements\n */\nexport function parquetSchema({ schema }) {\n  return getSchemaPath(schema, [])[0]\n}\n\n/**\n * @param {any} logicalType\n * @returns {LogicalType | undefined}\n */\nfunction logicalType(logicalType) {\n  if (logicalType?.field_1) return { type: 'STRING' }\n  if (logicalType?.field_2) return { type: 'MAP' }\n  if (logicalType?.field_3) return { type: 'LIST' }\n  if (logicalType?.field_4) return { type: 'ENUM' }\n  if (logicalType?.field_5) return {\n    type: 'DECIMAL',\n    scale: logicalType.field_5.field_1,\n    precision: logicalType.field_5.field_2,\n  }\n  if (logicalType?.field_6) return { type: 'DATE' }\n  if (logicalType?.field_7) return {\n    type: 'TIME',\n    isAdjustedToUTC: logicalType.field_7.field_1,\n    unit: timeUnit(logicalType.field_7.field_2),\n  }\n  if (logicalType?.field_8) return {\n    type: 'TIMESTAMP',\n    isAdjustedToUTC: logicalType.field_8.field_1,\n    unit: timeUnit(logicalType.field_8.field_2),\n  }\n  if (logicalType?.field_10) return {\n    type: 'INTEGER',\n    bitWidth: logicalType.field_10.field_1,\n    isSigned: logicalType.field_10.field_2,\n  }\n  if (logicalType?.field_11) return { type: 'NULL' }\n  if (logicalType?.field_12) return { type: 'JSON' }\n  if (logicalType?.field_13) return { type: 'BSON' }\n  if (logicalType?.field_14) return { type: 'UUID' }\n  if (logicalType?.field_15) return { type: 'FLOAT16' }\n  return logicalType\n}\n\n/**\n * @param {any} unit\n * @returns {TimeUnit}\n */\nfunction timeUnit(unit) {\n  if (unit.field_1) return 'MILLIS'\n  if (unit.field_2) return 'MICROS'\n  if (unit.field_3) return 'NANOS'\n  throw new Error('parquet time unit required')\n}\n\n/**\n * Convert column statistics based on column type.\n *\n * @import {AsyncBuffer, FileMetaData, LogicalType, MetadataOptions, MinMaxType, ParquetParsers, SchemaElement, SchemaTree, Statistics, TimeUnit} from '../src/types.d.ts'\n * @param {any} stats\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {Statistics}\n */\nfunction convertStats(stats, schema, parsers) {\n  return stats && {\n    max: convertMetadata(stats.field_1, schema, parsers),\n    min: convertMetadata(stats.field_2, schema, parsers),\n    null_count: stats.field_3,\n    distinct_count: stats.field_4,\n    max_value: convertMetadata(stats.field_5, schema, parsers),\n    min_value: convertMetadata(stats.field_6, schema, parsers),\n    is_max_value_exact: stats.field_7,\n    is_min_value_exact: stats.field_8,\n  }\n}\n\n/**\n * @param {Uint8Array | undefined} value\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {MinMaxType | undefined}\n */\nexport function convertMetadata(value, schema, parsers) {\n  const { type, converted_type, logical_type } = schema\n  if (value === undefined) return value\n  if (type === 'BOOLEAN') return value[0] === 1\n  if (type === 'BYTE_ARRAY') return new TextDecoder().decode(value)\n  const view = new DataView(value.buffer, value.byteOffset, value.byteLength)\n  if (type === 'FLOAT' && view.byteLength === 4) return view.getFloat32(0, true)\n  if (type === 'DOUBLE' && view.byteLength === 8) return view.getFloat64(0, true)\n  if (type === 'INT32' && converted_type === 'DATE') return parsers.dateFromDays(view.getInt32(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MILLIS') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'NANOS') return parsers.timestampFromNanoseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT32' && view.byteLength === 4) return view.getInt32(0, true)\n  if (type === 'INT64' && view.byteLength === 8) return view.getBigInt64(0, true)\n  if (converted_type === 'DECIMAL') return parseDecimal(value) * 10 ** -(schema.scale || 0)\n  if (logical_type?.type === 'FLOAT16') return parseFloat16(value)\n  if (type === 'FIXED_LEN_BYTE_ARRAY') return value\n  // assert(false)\n  return value\n}\n","import { defaultInitialFetchSize } from './metadata.js'\n\n/**\n * Replace bigint, date, etc with legal JSON types.\n *\n * @param {any} obj object to convert\n * @returns {unknown} converted object\n */\nexport function toJson(obj) {\n  if (obj === undefined) return null\n  if (typeof obj === 'bigint') return Number(obj)\n  if (Array.isArray(obj)) return obj.map(toJson)\n  if (obj instanceof Uint8Array) return Array.from(obj)\n  if (obj instanceof Date) return obj.toISOString()\n  if (obj instanceof Object) {\n    /** @type {Record<string, unknown>} */\n    const newObj = {}\n    for (const key of Object.keys(obj)) {\n      if (obj[key] === undefined) continue\n      newObj[key] = toJson(obj[key])\n    }\n    return newObj\n  }\n  return obj\n}\n\n/**\n * Concatenate two arrays fast.\n *\n * @param {any[]} aaa first array\n * @param {DecodedArray} bbb second array\n */\nexport function concat(aaa, bbb) {\n  const chunk = 10000\n  for (let i = 0; i < bbb.length; i += chunk) {\n    aaa.push(...bbb.slice(i, i + chunk))\n  }\n}\n\n/**\n * Deep equality comparison\n *\n * @param {any} a First object to compare\n * @param {any} b Second object to compare\n * @returns {boolean} true if objects are equal\n */\nexport function equals(a, b) {\n  if (a === b) return true\n  if (a instanceof Uint8Array && b instanceof Uint8Array) return equals(Array.from(a), Array.from(b))\n  if (!a || !b || typeof a !== typeof b) return false\n  return Array.isArray(a) && Array.isArray(b)\n    ? a.length === b.length && a.every((v, i) => equals(v, b[i]))\n    : typeof a === 'object' && Object.keys(a).length === Object.keys(b).length && Object.keys(a).every(k => equals(a[k], b[k]))\n}\n\n/**\n * Get the byte length of a URL using a HEAD request.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {string} url\n * @param {RequestInit} [requestInit] fetch options\n * @param {typeof globalThis.fetch} [customFetch] fetch function to use\n * @returns {Promise<number>}\n */\nexport async function byteLengthFromUrl(url, requestInit, customFetch) {\n  const fetch = customFetch ?? globalThis.fetch\n  return await fetch(url, { ...requestInit, method: 'HEAD' })\n    .then(res => {\n      if (!res.ok) throw new Error(`fetch head failed ${res.status}`)\n      const length = res.headers.get('Content-Length')\n      if (!length) throw new Error('missing content length')\n      return parseInt(length)\n    })\n}\n\n/**\n * Construct an AsyncBuffer for a URL.\n * If byteLength is not provided, will make a HEAD request to get the file size.\n * If fetch is provided, it will be used instead of the global fetch.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {object} options\n * @param {string} options.url\n * @param {number} [options.byteLength]\n * @param {typeof globalThis.fetch} [options.fetch] fetch function to use\n * @param {RequestInit} [options.requestInit]\n * @returns {Promise<AsyncBuffer>}\n */\nexport async function asyncBufferFromUrl({ url, byteLength, requestInit, fetch: customFetch }) {\n  if (!url) throw new Error('missing url')\n  const fetch = customFetch ?? globalThis.fetch\n  // byte length from HEAD request\n  byteLength ||= await byteLengthFromUrl(url, requestInit, fetch)\n\n  /**\n   * A promise for the whole buffer, if range requests are not supported.\n   * @type {Promise<ArrayBuffer>|undefined}\n   */\n  let buffer = undefined\n  const init = requestInit || {}\n\n  return {\n    byteLength,\n    async slice(start, end) {\n      if (buffer) {\n        return buffer.then(buffer => buffer.slice(start, end))\n      }\n\n      const headers = new Headers(init.headers)\n      const endStr = end === undefined ? '' : end - 1\n      headers.set('Range', `bytes=${start}-${endStr}`)\n\n      const res = await fetch(url, { ...init, headers })\n      if (!res.ok || !res.body) throw new Error(`fetch failed ${res.status}`)\n\n      if (res.status === 200) {\n        // Endpoint does not support range requests and returned the whole object\n        buffer = res.arrayBuffer()\n        return buffer.then(buffer => buffer.slice(start, end))\n      } else if (res.status === 206) {\n        // The endpoint supports range requests and sent us the requested range\n        return res.arrayBuffer()\n      } else {\n        throw new Error(`fetch received unexpected status code ${res.status}`)\n      }\n    },\n  }\n}\n\n/**\n * Returns a cached layer on top of an AsyncBuffer. For caching slices of a file\n * that are read multiple times, possibly over a network.\n *\n * @param {AsyncBuffer} file file-like object to cache\n * @param {{ minSize?: number }} [options]\n * @returns {AsyncBuffer} cached file-like object\n */\nexport function cachedAsyncBuffer({ byteLength, slice }, { minSize = defaultInitialFetchSize } = {}) {\n  if (byteLength < minSize) {\n    // Cache whole file if it's small\n    const buffer = slice(0, byteLength)\n    return {\n      byteLength,\n      async slice(start, end) {\n        return (await buffer).slice(start, end)\n      },\n    }\n  }\n  const cache = new Map()\n  return {\n    byteLength,\n    /**\n     * @param {number} start\n     * @param {number} [end]\n     * @returns {Awaitable<ArrayBuffer>}\n     */\n    slice(start, end) {\n      const key = cacheKey(start, end, byteLength)\n      const cached = cache.get(key)\n      if (cached) return cached\n      // cache miss, read from file\n      const promise = slice(start, end)\n      cache.set(key, promise)\n      return promise\n    },\n  }\n}\n\n\n/**\n * Returns canonical cache key for a byte range 'start,end'.\n * Normalize int-range and suffix-range requests to the same key.\n *\n * @import {AsyncBuffer, Awaitable, DecodedArray} from '../src/types.d.ts'\n * @param {number} start start byte of range\n * @param {number} [end] end byte of range, or undefined for suffix range\n * @param {number} [size] size of file, or undefined for suffix range\n * @returns {string}\n */\nfunction cacheKey(start, end, size) {\n  if (start < 0) {\n    if (end !== undefined) throw new Error(`invalid suffix range [${start}, ${end}]`)\n    if (size === undefined) return `${start},`\n    return `${size + start},${size}`\n  } else if (end !== undefined) {\n    if (start > end) throw new Error(`invalid empty range [${start}, ${end}]`)\n    return `${start},${end}`\n  } else if (size === undefined) {\n    return `${start},`\n  } else {\n    return `${start},${size}`\n  }\n}\n\n/**\n * Flatten a list of lists into a single list.\n *\n * @param {DecodedArray[]} [chunks]\n * @returns {DecodedArray}\n */\nexport function flatten(chunks) {\n  if (!chunks) return []\n  if (chunks.length === 1) return chunks[0]\n  /** @type {any[]} */\n  const output = []\n  for (const chunk of chunks) {\n    concat(output, chunk)\n  }\n  return output\n}\n","import { concat } from './utils.js'\n\n// Combine column chunks into a single byte range if less than 32mb\nconst columnChunkAggregation = 1 << 25 // 32mb\n\n/**\n * @import {AsyncBuffer, ByteRange, ColumnMetaData, GroupPlan, ParquetReadOptions, QueryPlan} from '../src/types.js'\n */\n/**\n * Plan which byte ranges to read to satisfy a read request.\n * Metadata must be non-null.\n *\n * @param {ParquetReadOptions} options\n * @returns {QueryPlan}\n */\nexport function parquetPlan({ metadata, rowStart = 0, rowEnd = Infinity, columns }) {\n  if (!metadata) throw new Error('parquetPlan requires metadata')\n  /** @type {GroupPlan[]} */\n  const groups = []\n  /** @type {ByteRange[]} */\n  const fetches = []\n\n  // find which row groups to read\n  let groupStart = 0 // first row index of the current group\n  for (const rowGroup of metadata.row_groups) {\n    const groupRows = Number(rowGroup.num_rows)\n    const groupEnd = groupStart + groupRows\n    // if row group overlaps with row range, add it to the plan\n    if (groupRows > 0 && groupEnd >= rowStart && groupStart < rowEnd) {\n      /** @type {ByteRange[]} */\n      const ranges = []\n      // loop through each column chunk\n      for (const { file_path, meta_data } of rowGroup.columns) {\n        if (file_path) throw new Error('parquet file_path not supported')\n        if (!meta_data) throw new Error('parquet column metadata is undefined')\n        // add included columns to the plan\n        if (!columns || columns.includes(meta_data.path_in_schema[0])) {\n          ranges.push(getColumnRange(meta_data))\n        }\n      }\n      const selectStart = Math.max(rowStart - groupStart, 0)\n      const selectEnd = Math.min(rowEnd - groupStart, groupRows)\n      groups.push({ ranges, rowGroup, groupStart, groupRows, selectStart, selectEnd })\n\n      // map group plan to ranges\n      const groupSize = ranges[ranges.length - 1]?.endByte - ranges[0]?.startByte\n      if (!columns && groupSize < columnChunkAggregation) {\n        // full row group\n        fetches.push({\n          startByte: ranges[0].startByte,\n          endByte: ranges[ranges.length - 1].endByte,\n        })\n      } else if (ranges.length) {\n        concat(fetches, ranges)\n      } else if (columns?.length) {\n        throw new Error(`parquet columns not found: ${columns.join(', ')}`)\n      }\n    }\n\n    groupStart = groupEnd\n  }\n  if (!isFinite(rowEnd)) rowEnd = groupStart\n\n  return { metadata, rowStart, rowEnd, columns, fetches, groups }\n}\n\n/**\n * @param {ColumnMetaData} columnMetadata\n * @returns {ByteRange}\n */\nexport function getColumnRange({ dictionary_page_offset, data_page_offset, total_compressed_size }) {\n  const columnOffset = dictionary_page_offset || data_page_offset\n  return {\n    startByte: Number(columnOffset),\n    endByte: Number(columnOffset + total_compressed_size),\n  }\n}\n\n/**\n * Prefetch byte ranges from an AsyncBuffer.\n *\n * @param {AsyncBuffer} file\n * @param {QueryPlan} plan\n * @returns {AsyncBuffer}\n */\nexport function prefetchAsyncBuffer(file, { fetches }) {\n  // fetch byte ranges from the file\n  const promises = fetches.map(({ startByte, endByte }) => file.slice(startByte, endByte))\n  return {\n    byteLength: file.byteLength,\n    slice(start, end = file.byteLength) {\n      // find matching slice\n      const index = fetches.findIndex(({ startByte, endByte }) => startByte <= start && end <= endByte)\n      if (index < 0) throw new Error(`no prefetch for range [${start}, ${end}]`)\n      if (fetches[index].startByte !== start || fetches[index].endByte !== end) {\n        // slice a subrange of the prefetch\n        const startOffset = start - fetches[index].startByte\n        const endOffset = end - fetches[index].startByte\n        if (promises[index] instanceof Promise) {\n          return promises[index].then(buffer => buffer.slice(startOffset, endOffset))\n        } else {\n          return promises[index].slice(startOffset, endOffset)\n        }\n      } else {\n        return promises[index]\n      }\n    },\n  }\n}\n","import { getMaxDefinitionLevel, isListLike, isMapLike } from './schema.js'\n\n/**\n * Reconstructs a complex nested structure from flat arrays of values and\n * definition and repetition levels, according to Dremel encoding.\n *\n * @param {any[]} output\n * @param {number[] | undefined} definitionLevels\n * @param {number[]} repetitionLevels\n * @param {DecodedArray} values\n * @param {SchemaTree[]} schemaPath\n * @returns {DecodedArray}\n */\nexport function assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath) {\n  const n = definitionLevels?.length || repetitionLevels.length\n  if (!n) return values\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  const repetitionPath = schemaPath.map(({ element }) => element.repetition_type)\n  let valueIndex = 0\n\n  // Track state of nested structures\n  const containerStack = [output]\n  let currentContainer = output\n  let currentDepth = 0 // schema depth\n  let currentDefLevel = 0 // list depth\n  let currentRepLevel = 0\n\n  if (repetitionLevels[0]) {\n    // continue previous row\n    while (currentDepth < repetitionPath.length - 2 && currentRepLevel < repetitionLevels[0]) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        // go into last list\n        currentContainer = currentContainer.at(-1)\n        containerStack.push(currentContainer)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n  }\n\n  for (let i = 0; i < n; i++) {\n    // assert(currentDefLevel === containerStack.length - 1)\n    const def = definitionLevels?.length ? definitionLevels[i] : maxDefinitionLevel\n    const rep = repetitionLevels[i]\n\n    // Pop up to start of rep level\n    while (currentDepth && (rep < currentRepLevel || repetitionPath[currentDepth] !== 'REPEATED')) {\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        containerStack.pop()\n        currentDefLevel--\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel--\n      currentDepth--\n    }\n    // @ts-expect-error won't be empty\n    currentContainer = containerStack.at(-1)\n\n    // Go deeper to end of definition level\n    while (\n      (currentDepth < repetitionPath.length - 2 || repetitionPath[currentDepth + 1] === 'REPEATED') &&\n      (currentDefLevel < def || repetitionPath[currentDepth + 1] === 'REQUIRED')\n    ) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        /** @type {any[]} */\n        const newList = []\n        currentContainer.push(newList)\n        currentContainer = newList\n        containerStack.push(newList)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n\n    // Add value or null based on definition level\n    if (def === maxDefinitionLevel) {\n      // assert(currentDepth === maxDefinitionLevel || currentDepth === repetitionPath.length - 2)\n      currentContainer.push(values[valueIndex++])\n    } else if (currentDepth === repetitionPath.length - 2) {\n      currentContainer.push(null)\n    } else {\n      currentContainer.push([])\n    }\n  }\n\n  // Handle edge cases for empty inputs or single-level data\n  if (!output.length) {\n    // return max definition level of nested lists\n    for (let i = 0; i < maxDefinitionLevel; i++) {\n      /** @type {any[]} */\n      const newList = []\n      currentContainer.push(newList)\n      currentContainer = newList\n    }\n  }\n\n  return output\n}\n\n/**\n * Assemble a nested structure from subcolumn data.\n * https://github.com/apache/parquet-format/blob/apache-parquet-format-2.10.0/LogicalTypes.md#nested-types\n *\n * @param {Map<string, DecodedArray>} subcolumnData\n * @param {SchemaTree} schema top-level schema element\n * @param {number} [depth] depth of nested structure\n */\nexport function assembleNested(subcolumnData, schema, depth = 0) {\n  const path = schema.path.join('.')\n  const optional = schema.element.repetition_type === 'OPTIONAL'\n  const nextDepth = optional ? depth + 1 : depth\n\n  if (isListLike(schema)) {\n    let sublist = schema.children[0]\n    let subDepth = nextDepth\n    if (sublist.children.length === 1) {\n      sublist = sublist.children[0]\n      subDepth++\n    }\n    assembleNested(subcolumnData, sublist, subDepth)\n\n    const subcolumn = sublist.path.join('.')\n    const values = subcolumnData.get(subcolumn)\n    if (!values) throw new Error('parquet list column missing values')\n    if (optional) flattenAtDepth(values, depth)\n    subcolumnData.set(path, values)\n    subcolumnData.delete(subcolumn)\n    return\n  }\n\n  if (isMapLike(schema)) {\n    const mapName = schema.children[0].element.name\n\n    // Assemble keys and values\n    assembleNested(subcolumnData, schema.children[0].children[0], nextDepth + 1)\n    assembleNested(subcolumnData, schema.children[0].children[1], nextDepth + 1)\n\n    const keys = subcolumnData.get(`${path}.${mapName}.key`)\n    const values = subcolumnData.get(`${path}.${mapName}.value`)\n\n    if (!keys) throw new Error('parquet map column missing keys')\n    if (!values) throw new Error('parquet map column missing values')\n    if (keys.length !== values.length) {\n      throw new Error('parquet map column key/value length mismatch')\n    }\n\n    const out = assembleMaps(keys, values, nextDepth)\n    if (optional) flattenAtDepth(out, depth)\n\n    subcolumnData.delete(`${path}.${mapName}.key`)\n    subcolumnData.delete(`${path}.${mapName}.value`)\n    subcolumnData.set(path, out)\n    return\n  }\n\n  // Struct-like column\n  if (schema.children.length) {\n    // construct a meta struct and then invert\n    const invertDepth = schema.element.repetition_type === 'REQUIRED' ? depth : depth + 1\n    /** @type {Record<string, any>} */\n    const struct = {}\n    for (const child of schema.children) {\n      assembleNested(subcolumnData, child, invertDepth)\n      const childData = subcolumnData.get(child.path.join('.'))\n      if (!childData) throw new Error('parquet struct missing child data')\n      struct[child.element.name] = childData\n    }\n    // remove children\n    for (const child of schema.children) {\n      subcolumnData.delete(child.path.join('.'))\n    }\n    // invert struct by depth\n    const inverted = invertStruct(struct, invertDepth)\n    if (optional) flattenAtDepth(inverted, depth)\n    subcolumnData.set(path, inverted)\n  }\n}\n\n/**\n * @import {DecodedArray, SchemaTree} from '../src/types.d.ts'\n * @param {DecodedArray} arr\n * @param {number} depth\n */\nfunction flattenAtDepth(arr, depth) {\n  for (let i = 0; i < arr.length; i++) {\n    if (depth) {\n      flattenAtDepth(arr[i], depth - 1)\n    } else {\n      arr[i] = arr[i][0]\n    }\n  }\n}\n\n/**\n * @param {DecodedArray} keys\n * @param {DecodedArray} values\n * @param {number} depth\n * @returns {any[]}\n */\nfunction assembleMaps(keys, values, depth) {\n  const out = []\n  for (let i = 0; i < keys.length; i++) {\n    if (depth) {\n      out.push(assembleMaps(keys[i], values[i], depth - 1)) // go deeper\n    } else {\n      if (keys[i]) {\n        /** @type {Record<string, any>} */\n        const obj = {}\n        for (let j = 0; j < keys[i].length; j++) {\n          const value = values[i][j]\n          obj[keys[i][j]] = value === undefined ? null : value\n        }\n        out.push(obj)\n      } else {\n        out.push(undefined)\n      }\n    }\n  }\n  return out\n}\n\n/**\n * Invert a struct-like object by depth.\n *\n * @param {Record<string, any[]>} struct\n * @param {number} depth\n * @returns {any[]}\n */\nfunction invertStruct(struct, depth) {\n  const keys = Object.keys(struct)\n  const length = struct[keys[0]]?.length\n  const out = []\n  for (let i = 0; i < length; i++) {\n    /** @type {Record<string, any>} */\n    const obj = {}\n    for (const key of keys) {\n      if (struct[key].length !== length) throw new Error('parquet struct parsing error')\n      obj[key] = struct[key][i]\n    }\n    if (depth) {\n      out.push(invertStruct(obj, depth - 1)) // deeper\n    } else {\n      out.push(obj)\n    }\n  }\n  return out\n}\n","import { readVarInt, readZigZagBigInt } from './thrift.js'\n\n/**\n * @import {DataReader} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} count number of values to read\n * @param {Int32Array | BigInt64Array} output\n */\nexport function deltaBinaryUnpack(reader, count, output) {\n  const int32 = output instanceof Int32Array\n  const blockSize = readVarInt(reader)\n  const miniblockPerBlock = readVarInt(reader)\n  readVarInt(reader) // assert(=== count)\n  let value = readZigZagBigInt(reader) // first value\n  let outputIndex = 0\n  output[outputIndex++] = int32 ? Number(value) : value\n\n  const valuesPerMiniblock = blockSize / miniblockPerBlock\n\n  while (outputIndex < count) {\n    // new block\n    const minDelta = readZigZagBigInt(reader)\n    const bitWidths = new Uint8Array(miniblockPerBlock)\n    for (let i = 0; i < miniblockPerBlock; i++) {\n      bitWidths[i] = reader.view.getUint8(reader.offset++)\n    }\n\n    for (let i = 0; i < miniblockPerBlock && outputIndex < count; i++) {\n      // new miniblock\n      const bitWidth = BigInt(bitWidths[i])\n      if (bitWidth) {\n        let bitpackPos = 0n\n        let miniblockCount = valuesPerMiniblock\n        const mask = (1n << bitWidth) - 1n\n        while (miniblockCount && outputIndex < count) {\n          let bits = BigInt(reader.view.getUint8(reader.offset)) >> bitpackPos & mask // TODO: don't re-read value every time\n          bitpackPos += bitWidth\n          while (bitpackPos >= 8) {\n            bitpackPos -= 8n\n            reader.offset++\n            if (bitpackPos) {\n              bits |= BigInt(reader.view.getUint8(reader.offset)) << bitWidth - bitpackPos & mask\n            }\n          }\n          const delta = minDelta + bits\n          value += delta\n          output[outputIndex++] = int32 ? Number(value) : value\n          miniblockCount--\n        }\n        if (miniblockCount) {\n          // consume leftover miniblock\n          reader.offset += Math.ceil((miniblockCount * Number(bitWidth) + Number(bitpackPos)) / 8)\n        }\n      } else {\n        for (let j = 0; j < valuesPerMiniblock && outputIndex < count; j++) {\n          value += minDelta\n          output[outputIndex++] = int32 ? Number(value) : value\n        }\n      }\n    }\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaLengthByteArray(reader, count, output) {\n  const lengths = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, lengths)\n  for (let i = 0; i < count; i++) {\n    output[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, lengths[i])\n    reader.offset += lengths[i]\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaByteArray(reader, count, output) {\n  const prefixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, prefixData)\n  const suffixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, suffixData)\n\n  for (let i = 0; i < count; i++) {\n    const suffix = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, suffixData[i])\n    if (prefixData[i]) {\n      // copy from previous value\n      output[i] = new Uint8Array(prefixData[i] + suffixData[i])\n      output[i].set(output[i - 1].subarray(0, prefixData[i]))\n      output[i].set(suffix, prefixData[i])\n    } else {\n      output[i] = suffix\n    }\n    reader.offset += suffixData[i]\n  }\n}\n","import { readVarInt } from './thrift.js'\n\n/**\n * Minimum bits needed to store value.\n *\n * @param {number} value\n * @returns {number}\n */\nexport function bitWidth(value) {\n  return 32 - Math.clz32(value)\n}\n\n/**\n * Read values from a run-length encoded/bit-packed hybrid encoding.\n *\n * If length is zero, then read int32 length at the start.\n *\n * @param {DataReader} reader\n * @param {number} width - bitwidth\n * @param {DecodedArray} output\n * @param {number} [length] - length of the encoded data\n */\nexport function readRleBitPackedHybrid(reader, width, output, length) {\n  if (length === undefined) {\n    length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n  }\n  const startOffset = reader.offset\n  let seen = 0\n  while (seen < output.length) {\n    const header = readVarInt(reader)\n    if (header & 1) {\n      // bit-packed\n      seen = readBitPacked(reader, header, width, output, seen)\n    } else {\n      // rle\n      const count = header >>> 1\n      readRle(reader, count, width, output, seen)\n      seen += count\n    }\n  }\n  reader.offset = startOffset + length // duckdb writes an empty block\n}\n\n/**\n * Run-length encoding: read value with bitWidth and repeat it count times.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n */\nfunction readRle(reader, count, bitWidth, output, seen) {\n  const width = bitWidth + 7 >> 3\n  let value = 0\n  for (let i = 0; i < width; i++) {\n    value |= reader.view.getUint8(reader.offset++) << (i << 3)\n  }\n  // assert(value < 1 << bitWidth)\n\n  // repeat value count times\n  for (let i = 0; i < count; i++) {\n    output[seen + i] = value\n  }\n}\n\n/**\n * Read a bit-packed run of the rle/bitpack hybrid.\n * Supports width > 8 (crossing bytes).\n *\n * @param {DataReader} reader\n * @param {number} header - bit-pack header\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n * @returns {number} total output values so far\n */\nfunction readBitPacked(reader, header, bitWidth, output, seen) {\n  let count = header >> 1 << 3 // values to read\n  const mask = (1 << bitWidth) - 1\n\n  let data = 0\n  if (reader.offset < reader.view.byteLength) {\n    data = reader.view.getUint8(reader.offset++)\n  } else if (mask) {\n    // sometimes out-of-bounds reads are masked out\n    throw new Error(`parquet bitpack offset ${reader.offset} out of range`)\n  }\n  let left = 8\n  let right = 0\n\n  // read values\n  while (count) {\n    // if we have crossed a byte boundary, shift the data\n    if (right > 8) {\n      right -= 8\n      left -= 8\n      data >>>= 8\n    } else if (left - right < bitWidth) {\n      // if we don't have bitWidth number of bits to read, read next byte\n      data |= reader.view.getUint8(reader.offset) << left\n      reader.offset++\n      left += 8\n    } else {\n      if (seen < output.length) {\n        // emit value\n        output[seen++] = data >> right & mask\n      }\n      count--\n      right += bitWidth\n    }\n  }\n\n  return seen\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {DecodedArray}\n */\nexport function byteStreamSplit(reader, count, type, typeLength) {\n  const width = byteWidth(type, typeLength)\n  const bytes = new Uint8Array(count * width)\n  for (let b = 0; b < width; b++) {\n    for (let i = 0; i < count; i++) {\n      bytes[i * width + b] = reader.view.getUint8(reader.offset++)\n    }\n  }\n  // interpret bytes as typed array\n  if (type === 'FLOAT') return new Float32Array(bytes.buffer)\n  else if (type === 'DOUBLE') return new Float64Array(bytes.buffer)\n  else if (type === 'INT32') return new Int32Array(bytes.buffer)\n  else if (type === 'INT64') return new BigInt64Array(bytes.buffer)\n  else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    // split into arrays of typeLength\n    const split = new Array(count)\n    for (let i = 0; i < count; i++) {\n      split[i] = bytes.subarray(i * width, (i + 1) * width)\n    }\n    return split\n  }\n  throw new Error(`parquet byte_stream_split unsupported type: ${type}`)\n}\n\n/**\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {number}\n */\nfunction byteWidth(type, typeLength) {\n  switch (type) {\n  case 'INT32':\n  case 'FLOAT':\n    return 4\n  case 'INT64':\n  case 'DOUBLE':\n    return 8\n  case 'FIXED_LEN_BYTE_ARRAY':\n    if (!typeLength) throw new Error('parquet byteWidth missing type_length')\n    return typeLength\n  default:\n    throw new Error(`parquet unsupported type: ${type}`)\n  }\n}\n","/**\n * Read `count` values of the given type from the reader.view.\n *\n * @param {DataReader} reader - buffer to read data from\n * @param {ParquetType} type - parquet type of the data\n * @param {number} count - number of values to read\n * @param {number | undefined} fixedLength - length of each fixed length byte array\n * @returns {DecodedArray} array of values\n */\nexport function readPlain(reader, type, count, fixedLength) {\n  if (count === 0) return []\n  if (type === 'BOOLEAN') {\n    return readPlainBoolean(reader, count)\n  } else if (type === 'INT32') {\n    return readPlainInt32(reader, count)\n  } else if (type === 'INT64') {\n    return readPlainInt64(reader, count)\n  } else if (type === 'INT96') {\n    return readPlainInt96(reader, count)\n  } else if (type === 'FLOAT') {\n    return readPlainFloat(reader, count)\n  } else if (type === 'DOUBLE') {\n    return readPlainDouble(reader, count)\n  } else if (type === 'BYTE_ARRAY') {\n    return readPlainByteArray(reader, count)\n  } else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    if (!fixedLength) throw new Error('parquet missing fixed length')\n    return readPlainByteArrayFixed(reader, count, fixedLength)\n  } else {\n    throw new Error(`parquet unhandled type: ${type}`)\n  }\n}\n\n/**\n * Read `count` boolean values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {boolean[]}\n */\nfunction readPlainBoolean(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const byteOffset = reader.offset + (i / 8 | 0)\n    const bitOffset = i % 8\n    const byte = reader.view.getUint8(byteOffset)\n    values[i] = (byte & 1 << bitOffset) !== 0\n  }\n  reader.offset += Math.ceil(count / 8)\n  return values\n}\n\n/**\n * Read `count` int32 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Int32Array}\n */\nfunction readPlainInt32(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Int32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Int32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` int64 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {BigInt64Array}\n */\nfunction readPlainInt64(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new BigInt64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new BigInt64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` int96 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {bigint[]}\n */\nfunction readPlainInt96(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const low = reader.view.getBigInt64(reader.offset + i * 12, true)\n    const high = reader.view.getInt32(reader.offset + i * 12 + 8, true)\n    values[i] = BigInt(high) << 64n | low\n  }\n  reader.offset += count * 12\n  return values\n}\n\n/**\n * Read `count` float values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float32Array}\n */\nfunction readPlainFloat(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Float32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Float32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` double values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float64Array}\n */\nfunction readPlainDouble(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new Float64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new Float64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` byte array values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArray(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, length)\n    reader.offset += length\n  }\n  return values\n}\n\n/**\n * Read a fixed length byte array.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} fixedLength\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArrayFixed(reader, count, fixedLength) {\n  // assert(reader.view.byteLength - reader.offset >= count * fixedLength)\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, fixedLength)\n    reader.offset += fixedLength\n  }\n  return values\n}\n\n/**\n * Create a new buffer with the offset and size.\n *\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ArrayBufferLike} buffer\n * @param {number} offset\n * @param {number} size\n * @returns {ArrayBuffer}\n */\nfunction align(buffer, offset, size) {\n  const aligned = new ArrayBuffer(size)\n  new Uint8Array(aligned).set(new Uint8Array(buffer, offset, size))\n  return aligned\n}\n","/**\n * The MIT License (MIT)\n * Copyright (c) 2016 Zhipeng Jia\n * https://github.com/zhipeng-jia/snappyjs\n */\n\nconst WORD_MASK = [0, 0xff, 0xffff, 0xffffff, 0xffffffff]\n\n/**\n * Copy bytes from one array to another\n *\n * @param {Uint8Array} fromArray source array\n * @param {number} fromPos source position\n * @param {Uint8Array} toArray destination array\n * @param {number} toPos destination position\n * @param {number} length number of bytes to copy\n */\nfunction copyBytes(fromArray, fromPos, toArray, toPos, length) {\n  for (let i = 0; i < length; i++) {\n    toArray[toPos + i] = fromArray[fromPos + i]\n  }\n}\n\n/**\n * Decompress snappy data.\n * Accepts an output buffer to avoid allocating a new buffer for each call.\n *\n * @param {Uint8Array} input compressed data\n * @param {Uint8Array} output output buffer\n */\nexport function snappyUncompress(input, output) {\n  const inputLength = input.byteLength\n  const outputLength = output.byteLength\n  let pos = 0\n  let outPos = 0\n\n  // skip preamble (contains uncompressed length as varint)\n  while (pos < inputLength) {\n    const c = input[pos]\n    pos++\n    if (c < 128) {\n      break\n    }\n  }\n  if (outputLength && pos >= inputLength) {\n    throw new Error('invalid snappy length header')\n  }\n\n  while (pos < inputLength) {\n    const c = input[pos]\n    let len = 0\n    pos++\n\n    if (pos >= inputLength) {\n      throw new Error('missing eof marker')\n    }\n\n    // There are two types of elements, literals and copies (back references)\n    if ((c & 0x3) === 0) {\n      // Literals are uncompressed data stored directly in the byte stream\n      let len = (c >>> 2) + 1\n      // Longer literal length is encoded in multiple bytes\n      if (len > 60) {\n        if (pos + 3 >= inputLength) {\n          throw new Error('snappy error literal pos + 3 >= inputLength')\n        }\n        const lengthSize = len - 60 // length bytes - 1\n        len = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        len = (len & WORD_MASK[lengthSize]) + 1\n        pos += lengthSize\n      }\n      if (pos + len > inputLength) {\n        throw new Error('snappy error literal exceeds input length')\n      }\n      copyBytes(input, pos, output, outPos, len)\n      pos += len\n      outPos += len\n    } else {\n      // Copy elements\n      let offset = 0 // offset back from current position to read\n      switch (c & 0x3) {\n      case 1:\n        // Copy with 1-byte offset\n        len = (c >>> 2 & 0x7) + 4\n        offset = input[pos] + (c >>> 5 << 8)\n        pos++\n        break\n      case 2:\n        // Copy with 2-byte offset\n        if (inputLength <= pos + 1) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos] + (input[pos + 1] << 8)\n        pos += 2\n        break\n      case 3:\n        // Copy with 4-byte offset\n        if (inputLength <= pos + 3) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        pos += 4\n        break\n      default:\n        break\n      }\n      if (offset === 0 || isNaN(offset)) {\n        throw new Error(`invalid offset ${offset} pos ${pos} inputLength ${inputLength}`)\n      }\n      if (offset > outPos) {\n        throw new Error('cannot copy from before start of buffer')\n      }\n      copyBytes(output, outPos - offset, output, outPos, len)\n      outPos += len\n    }\n  }\n\n  if (outPos !== outputLength) throw new Error('premature end of input')\n}\n","import { deltaBinaryUnpack, deltaByteArray, deltaLengthByteArray } from './delta.js'\nimport { bitWidth, byteStreamSplit, readRleBitPackedHybrid } from './encoding.js'\nimport { readPlain } from './plain.js'\nimport { getMaxDefinitionLevel, getMaxRepetitionLevel } from './schema.js'\nimport { snappyUncompress } from './snappy.js'\n\n/**\n * Read a data page from uncompressed reader.\n *\n * @param {Uint8Array} bytes raw page data (should already be decompressed)\n * @param {DataPageHeader} daph data page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPage(bytes, daph, { type, element, schemaPath }) {\n  const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength)\n  const reader = { view, offset: 0 }\n  /** @type {DecodedArray} */\n  let dataPage\n\n  // repetition and definition levels\n  const repetitionLevels = readRepetitionLevels(reader, daph, schemaPath)\n  // assert(!repetitionLevels.length || repetitionLevels.length === daph.num_values)\n  const { definitionLevels, numNulls } = readDefinitionLevels(reader, daph, schemaPath)\n  // assert(!definitionLevels.length || definitionLevels.length === daph.num_values)\n\n  // read values based on encoding\n  const nValues = daph.num_values - numNulls\n  if (daph.encoding === 'PLAIN') {\n    dataPage = readPlain(reader, type, nValues, element.type_length)\n  } else if (\n    daph.encoding === 'PLAIN_DICTIONARY' ||\n    daph.encoding === 'RLE_DICTIONARY' ||\n    daph.encoding === 'RLE'\n  ) {\n    const bitWidth = type === 'BOOLEAN' ? 1 : view.getUint8(reader.offset++)\n    if (bitWidth) {\n      dataPage = new Array(nValues)\n      if (type === 'BOOLEAN') {\n        readRleBitPackedHybrid(reader, bitWidth, dataPage)\n        dataPage = dataPage.map(x => !!x) // convert to boolean\n      } else {\n        // assert(daph.encoding.endsWith('_DICTIONARY'))\n        readRleBitPackedHybrid(reader, bitWidth, dataPage, view.byteLength - reader.offset)\n      }\n    } else {\n      dataPage = new Uint8Array(nValues) // nValue zeroes\n    }\n  } else if (daph.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(reader, nValues, type, element.type_length)\n  } else if (daph.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(reader, nValues, dataPage)\n  } else if (daph.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(reader, nValues, dataPage)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @import {ColumnDecoder, CompressionCodec, Compressors, DataPage, DataPageHeader, DataPageHeaderV2, DataReader, DecodedArray, PageHeader, SchemaTree} from '../src/types.d.ts'\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels and number of bytes read\n */\nfunction readRepetitionLevels(reader, daph, schemaPath) {\n  if (schemaPath.length > 1) {\n    const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n    if (maxRepetitionLevel) {\n      const values = new Array(daph.num_values)\n      readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values)\n      return values\n    }\n  }\n  return []\n}\n\n/**\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {{ definitionLevels: number[], numNulls: number }} definition levels\n */\nfunction readDefinitionLevels(reader, daph, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (!maxDefinitionLevel) return { definitionLevels: [], numNulls: 0 }\n\n  const definitionLevels = new Array(daph.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), definitionLevels)\n\n  // count nulls\n  let numNulls = daph.num_values\n  for (const def of definitionLevels) {\n    if (def === maxDefinitionLevel) numNulls--\n  }\n  if (numNulls === 0) definitionLevels.length = 0\n\n  return { definitionLevels, numNulls }\n}\n\n/**\n * @param {Uint8Array} compressedBytes\n * @param {number} uncompressed_page_size\n * @param {CompressionCodec} codec\n * @param {Compressors | undefined} compressors\n * @returns {Uint8Array}\n */\nexport function decompressPage(compressedBytes, uncompressed_page_size, codec, compressors) {\n  /** @type {Uint8Array} */\n  let page\n  const customDecompressor = compressors?.[codec]\n  if (codec === 'UNCOMPRESSED') {\n    page = compressedBytes\n  } else if (customDecompressor) {\n    page = customDecompressor(compressedBytes, uncompressed_page_size)\n  } else if (codec === 'SNAPPY') {\n    page = new Uint8Array(uncompressed_page_size)\n    snappyUncompress(compressedBytes, page)\n  } else {\n    throw new Error(`parquet unsupported compression codec: ${codec}`)\n  }\n  if (page?.length !== uncompressed_page_size) {\n    throw new Error(`parquet decompressed page length ${page?.length} does not match header ${uncompressed_page_size}`)\n  }\n  return page\n}\n\n\n/**\n * Read a data page from the given Uint8Array.\n *\n * @param {Uint8Array} compressedBytes raw page data\n * @param {PageHeader} ph page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPageV2(compressedBytes, ph, columnDecoder) {\n  const view = new DataView(compressedBytes.buffer, compressedBytes.byteOffset, compressedBytes.byteLength)\n  const reader = { view, offset: 0 }\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  const daph2 = ph.data_page_header_v2\n  if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n  // repetition levels\n  const repetitionLevels = readRepetitionLevelsV2(reader, daph2, schemaPath)\n  reader.offset = daph2.repetition_levels_byte_length // readVarInt() => len for boolean v2?\n\n  // definition levels\n  const definitionLevels = readDefinitionLevelsV2(reader, daph2, schemaPath)\n  // assert(reader.offset === daph2.repetition_levels_byte_length + daph2.definition_levels_byte_length)\n\n  const uncompressedPageSize = ph.uncompressed_page_size - daph2.definition_levels_byte_length - daph2.repetition_levels_byte_length\n\n  let page = compressedBytes.subarray(reader.offset)\n  if (daph2.is_compressed !== false) {\n    page = decompressPage(page, uncompressedPageSize, codec, compressors)\n  }\n  const pageView = new DataView(page.buffer, page.byteOffset, page.byteLength)\n  const pageReader = { view: pageView, offset: 0 }\n\n  // read values based on encoding\n  /** @type {DecodedArray} */\n  let dataPage\n  const nValues = daph2.num_values - daph2.num_nulls\n  if (daph2.encoding === 'PLAIN') {\n    dataPage = readPlain(pageReader, type, nValues, element.type_length)\n  } else if (daph2.encoding === 'RLE') {\n    // assert(type === 'BOOLEAN')\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, 1, dataPage)\n    dataPage = dataPage.map(x => !!x)\n  } else if (\n    daph2.encoding === 'PLAIN_DICTIONARY' ||\n    daph2.encoding === 'RLE_DICTIONARY'\n  ) {\n    const bitWidth = pageView.getUint8(pageReader.offset++)\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, bitWidth, dataPage, uncompressedPageSize - 1)\n  } else if (daph2.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(reader, nValues, type, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph2.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels\n */\nfunction readRepetitionLevelsV2(reader, daph2, schemaPath) {\n  const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n  if (!maxRepetitionLevel) return []\n\n  const values = new Array(daph2.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values, daph2.repetition_levels_byte_length)\n  return values\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {number[] | undefined} definition levels\n */\nfunction readDefinitionLevelsV2(reader, daph2, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (maxDefinitionLevel) {\n    // V2 we know the length\n    const values = new Array(daph2.num_values)\n    readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), values, daph2.definition_levels_byte_length)\n    return values\n  }\n}\n","import { assembleLists } from './assemble.js'\nimport { Encoding, PageType } from './constants.js'\nimport { convert, convertWithDictionary } from './convert.js'\nimport { decompressPage, readDataPage, readDataPageV2 } from './datapage.js'\nimport { readPlain } from './plain.js'\nimport { isFlatColumn } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\n\n/**\n * Parse column data from a buffer.\n *\n * @param {DataReader} reader\n * @param {RowGroupSelect} rowGroupSelect row group selection\n * @param {ColumnDecoder} columnDecoder column decoder params\n * @param {(chunk: ColumnData) => void} [onPage] callback for each page\n * @returns {DecodedArray[]}\n */\nexport function readColumn(reader, { groupStart, selectStart, selectEnd }, columnDecoder, onPage) {\n  const { columnName } = columnDecoder\n  /** @type {DecodedArray[]} */\n  const chunks = []\n  /** @type {DecodedArray | undefined} */\n  let dictionary = undefined\n  /** @type {DecodedArray | undefined} */\n  let lastChunk = undefined\n  let rowCount = 0\n\n  const emitLastChunk = onPage && (() => {\n    lastChunk && onPage({\n      columnName,\n      columnData: lastChunk,\n      rowStart: groupStart + rowCount - lastChunk.length,\n      rowEnd: groupStart + rowCount,\n    })\n  })\n\n  while (rowCount < selectEnd) {\n    if (reader.offset >= reader.view.byteLength - 1) break // end of reader\n\n    // read page header\n    const header = parquetHeader(reader)\n    if (header.type === 'DICTIONARY_PAGE') {\n      // assert(!dictionary)\n      dictionary = readPage(reader, header, columnDecoder, dictionary, undefined, 0)\n      dictionary = convert(dictionary, columnDecoder)\n    } else {\n      const lastChunkLength = lastChunk?.length || 0\n      const values = readPage(reader, header, columnDecoder, dictionary, lastChunk, selectStart - rowCount)\n      if (lastChunk === values) {\n        // continued from previous page\n        rowCount += values.length - lastChunkLength\n      } else {\n        emitLastChunk?.()\n        chunks.push(values)\n        rowCount += values.length\n        lastChunk = values\n      }\n    }\n  }\n  emitLastChunk?.()\n  // assert(rowCount >= selectEnd)\n  if (rowCount > selectEnd && lastChunk) {\n    // truncate last chunk to row limit\n    chunks[chunks.length - 1] = lastChunk.slice(0, selectEnd - (rowCount - lastChunk.length))\n  }\n  return chunks\n}\n\n/**\n * Read a page (data or dictionary) from a buffer.\n *\n * @param {DataReader} reader\n * @param {PageHeader} header\n * @param {ColumnDecoder} columnDecoder\n * @param {DecodedArray | undefined} dictionary\n * @param {DecodedArray | undefined} previousChunk\n * @param {number} pageStart skip this many rows in the page\n * @returns {DecodedArray}\n */\nexport function readPage(reader, header, columnDecoder, dictionary, previousChunk, pageStart) {\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  // read compressed_page_size bytes\n  const compressedBytes = new Uint8Array(\n    reader.view.buffer, reader.view.byteOffset + reader.offset, header.compressed_page_size\n  )\n  reader.offset += header.compressed_page_size\n\n  // parse page data by type\n  if (header.type === 'DATA_PAGE') {\n    const daph = header.data_page_header\n    if (!daph) throw new Error('parquet data page header is undefined')\n\n    // skip unnecessary non-nested pages\n    if (pageStart > daph.num_values && isFlatColumn(schemaPath)) {\n      return new Array(daph.num_values) // TODO: don't allocate array\n    }\n\n    const page = decompressPage(compressedBytes, Number(header.uncompressed_page_size), codec, compressors)\n    const { definitionLevels, repetitionLevels, dataPage } = readDataPage(page, daph, columnDecoder)\n    // assert(!daph.statistics?.null_count || daph.statistics.null_count === BigInt(daph.num_values - dataPage.length))\n\n    // convert types, dereference dictionary, and assemble lists\n    let values = convertWithDictionary(dataPage, dictionary, daph.encoding, columnDecoder)\n    if (repetitionLevels.length || definitionLevels?.length) {\n      const output = Array.isArray(previousChunk) ? previousChunk : []\n      return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n    } else {\n      // wrap nested flat data by depth\n      for (let i = 2; i < schemaPath.length; i++) {\n        if (schemaPath[i].element.repetition_type !== 'REQUIRED') {\n          values = Array.from(values, e => [e])\n        }\n      }\n      return values\n    }\n  } else if (header.type === 'DATA_PAGE_V2') {\n    const daph2 = header.data_page_header_v2\n    if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n    // skip unnecessary pages\n    if (pageStart > daph2.num_rows) {\n      return new Array(daph2.num_values) // TODO: don't allocate array\n    }\n\n    const { definitionLevels, repetitionLevels, dataPage } =\n      readDataPageV2(compressedBytes, header, columnDecoder)\n\n    // convert types, dereference dictionary, and assemble lists\n    const values = convertWithDictionary(dataPage, dictionary, daph2.encoding, columnDecoder)\n    const output = Array.isArray(previousChunk) ? previousChunk : []\n    return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n  } else if (header.type === 'DICTIONARY_PAGE') {\n    const diph = header.dictionary_page_header\n    if (!diph) throw new Error('parquet dictionary page header is undefined')\n\n    const page = decompressPage(\n      compressedBytes, Number(header.uncompressed_page_size), codec, compressors\n    )\n\n    const reader = { view: new DataView(page.buffer, page.byteOffset, page.byteLength), offset: 0 }\n    return readPlain(reader, type, diph.num_values, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported page type: ${header.type}`)\n  }\n}\n\n/**\n * Read parquet header from a buffer.\n *\n * @import {ColumnData, ColumnDecoder, DataReader, DecodedArray, PageHeader, RowGroupSelect} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @returns {PageHeader}\n */\nfunction parquetHeader(reader) {\n  const header = deserializeTCompactProtocol(reader)\n\n  // Parse parquet header from thrift data\n  const type = PageType[header.field_1]\n  const uncompressed_page_size = header.field_2\n  const compressed_page_size = header.field_3\n  const crc = header.field_4\n  const data_page_header = header.field_5 && {\n    num_values: header.field_5.field_1,\n    encoding: Encoding[header.field_5.field_2],\n    definition_level_encoding: Encoding[header.field_5.field_3],\n    repetition_level_encoding: Encoding[header.field_5.field_4],\n    statistics: header.field_5.field_5 && {\n      max: header.field_5.field_5.field_1,\n      min: header.field_5.field_5.field_2,\n      null_count: header.field_5.field_5.field_3,\n      distinct_count: header.field_5.field_5.field_4,\n      max_value: header.field_5.field_5.field_5,\n      min_value: header.field_5.field_5.field_6,\n    },\n  }\n  const index_page_header = header.field_6\n  const dictionary_page_header = header.field_7 && {\n    num_values: header.field_7.field_1,\n    encoding: Encoding[header.field_7.field_2],\n    is_sorted: header.field_7.field_3,\n  }\n  const data_page_header_v2 = header.field_8 && {\n    num_values: header.field_8.field_1,\n    num_nulls: header.field_8.field_2,\n    num_rows: header.field_8.field_3,\n    encoding: Encoding[header.field_8.field_4],\n    definition_levels_byte_length: header.field_8.field_5,\n    repetition_levels_byte_length: header.field_8.field_6,\n    is_compressed: header.field_8.field_7 === undefined ? true : header.field_8.field_7, // default true\n    statistics: header.field_8.field_8,\n  }\n\n  return {\n    type,\n    uncompressed_page_size,\n    compressed_page_size,\n    crc,\n    data_page_header,\n    index_page_header,\n    dictionary_page_header,\n    data_page_header_v2,\n  }\n}\n","import { assembleNested } from './assemble.js'\nimport { readColumn } from './column.js'\nimport { DEFAULT_PARSERS } from './convert.js'\nimport { getColumnRange } from './plan.js'\nimport { getSchemaPath } from './schema.js'\nimport { flatten } from './utils.js'\n\n/**\n * @import {AsyncColumn, AsyncRowGroup, DecodedArray, GroupPlan, ParquetParsers, ParquetReadOptions, QueryPlan, RowGroup, SchemaTree} from './types.js'\n */\n/**\n * Read a row group from a file-like object.\n *\n * @param {ParquetReadOptions} options\n * @param {QueryPlan} plan\n * @param {GroupPlan} groupPlan\n * @returns {AsyncRowGroup} resolves to column data\n */\nexport function readRowGroup(options, { metadata, columns }, groupPlan) {\n  const { file, compressors, utf8 } = options\n\n  /** @type {AsyncColumn[]} */\n  const asyncColumns = []\n  /** @type {ParquetParsers} */\n  const parsers = { ...DEFAULT_PARSERS, ...options.parsers }\n\n  // read column data\n  for (const { file_path, meta_data } of groupPlan.rowGroup.columns) {\n    if (file_path) throw new Error('parquet file_path not supported')\n    if (!meta_data) throw new Error('parquet column metadata is undefined')\n\n    // skip columns that are not requested\n    const columnName = meta_data.path_in_schema[0]\n    if (columns && !columns.includes(columnName)) continue\n\n    const { startByte, endByte } = getColumnRange(meta_data)\n    const columnBytes = endByte - startByte\n\n    // skip columns larger than 1gb\n    // TODO: stream process the data, returning only the requested rows\n    if (columnBytes > 1 << 30) {\n      console.warn(`parquet skipping huge column \"${meta_data.path_in_schema}\" ${columnBytes} bytes`)\n      // TODO: set column to new Error('parquet column too large')\n      continue\n    }\n\n    // wrap awaitable to ensure it's a promise\n    /** @type {Promise<ArrayBuffer>} */\n    const buffer = Promise.resolve(file.slice(startByte, endByte))\n\n    // read column data async\n    asyncColumns.push({\n      pathInSchema: meta_data.path_in_schema,\n      data: buffer.then(arrayBuffer => {\n        const schemaPath = getSchemaPath(metadata.schema, meta_data.path_in_schema)\n        const reader = { view: new DataView(arrayBuffer), offset: 0 }\n        const subcolumn = meta_data.path_in_schema.join('.')\n        const columnDecoder = {\n          columnName: subcolumn,\n          type: meta_data.type,\n          element: schemaPath[schemaPath.length - 1].element,\n          schemaPath,\n          codec: meta_data.codec,\n          parsers,\n          compressors,\n          utf8,\n        }\n        return readColumn(reader, groupPlan, columnDecoder, options.onPage)\n      }),\n    })\n  }\n\n  return { groupStart: groupPlan.groupStart, groupRows: groupPlan.groupRows, asyncColumns }\n}\n\n/**\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'object' | 'array'} [rowFormat]\n * @returns {Promise<Record<string, any>[]>} resolves to row data\n */\nexport async function asyncGroupToRows({ asyncColumns }, selectStart, selectEnd, columns, rowFormat) {\n  const groupData = new Array(selectEnd)\n\n  // columnData[i] for asyncColumns[i]\n  // TODO: do it without flatten\n  const columnDatas = await Promise.all(asyncColumns.map(({ data }) => data.then(flatten)))\n\n  // careful mapping of column order for rowFormat: array\n  const includedColumnNames = asyncColumns\n    .map(child => child.pathInSchema[0])\n    .filter(name => !columns || columns.includes(name))\n  const columnOrder = columns ?? includedColumnNames\n  const columnIndexes = columnOrder.map(name => asyncColumns.findIndex(column => column.pathInSchema[0] === name))\n\n  // transpose columns into rows\n  for (let row = selectStart; row < selectEnd; row++) {\n    if (rowFormat === 'object') {\n      // return each row as an object\n      /** @type {Record<string, any>} */\n      const rowData = {}\n      for (let i = 0; i < asyncColumns.length; i++) {\n        rowData[asyncColumns[i].pathInSchema[0]] = columnDatas[i][row]\n      }\n      groupData[row] = rowData\n    } else {\n      // return each row as an array\n      const rowData = new Array(asyncColumns.length)\n      for (let i = 0; i < columnOrder.length; i++) {\n        if (columnIndexes[i] >= 0) {\n          rowData[i] = columnDatas[columnIndexes[i]][row]\n        }\n      }\n      groupData[row] = rowData\n    }\n  }\n  return groupData\n}\n\n/**\n * Assemble physical columns into top-level columns asynchronously.\n *\n * @param {AsyncRowGroup} asyncRowGroup\n * @param {SchemaTree} schemaTree\n * @returns {AsyncRowGroup}\n */\nexport function assembleAsync(asyncRowGroup, schemaTree) {\n  const { asyncColumns } = asyncRowGroup\n  /** @type {AsyncColumn[]} */\n  const assembled = []\n  for (const child of schemaTree.children) {\n    if (child.children.length) {\n      const childColumns = asyncColumns.filter(column => column.pathInSchema[0] === child.element.name)\n      if (!childColumns.length) continue\n\n      // wait for all child columns to be read\n      /** @type {Map<string, DecodedArray>} */\n      const flatData = new Map()\n      const data = Promise.all(childColumns.map(column => {\n        return column.data.then(columnData => {\n          flatData.set(column.pathInSchema.join('.'), flatten(columnData))\n        })\n      })).then(() => {\n        // assemble the column\n        assembleNested(flatData, child)\n        const flatColumn = flatData.get(child.path.join('.'))\n        if (!flatColumn) throw new Error('parquet column data not assembled')\n        return [flatColumn]\n      })\n\n      assembled.push({ pathInSchema: child.path, data })\n    } else {\n      // leaf node, return the column\n      const asyncColumn = asyncColumns.find(column => column.pathInSchema[0] === child.element.name)\n      if (asyncColumn) {\n        assembled.push(asyncColumn)\n      }\n    }\n  }\n  return { ...asyncRowGroup, asyncColumns: assembled }\n}\n","import { parquetMetadataAsync, parquetSchema } from './metadata.js'\nimport { parquetPlan, prefetchAsyncBuffer } from './plan.js'\nimport { assembleAsync, asyncGroupToRows, readRowGroup } from './rowgroup.js'\nimport { concat, flatten } from './utils.js'\n\n/**\n * @import {AsyncBuffer, AsyncRowGroup, DecodedArray, FileMetaData, ParquetReadOptions} from '../src/types.js'\n */\n/**\n * Read parquet data rows from a file-like object.\n * Reads the minimal number of row groups and columns to satisfy the request.\n *\n * Returns a void promise when complete.\n * Errors are thrown on the returned promise.\n * Data is returned in callbacks onComplete, onChunk, onPage, NOT the return promise.\n * See parquetReadObjects for a more convenient API.\n *\n * @param {ParquetReadOptions} options read options\n * @returns {Promise<void>} resolves when all requested rows and columns are parsed, all errors are thrown here\n */\nexport async function parquetRead(options) {\n  // load metadata if not provided\n  options.metadata ??= await parquetMetadataAsync(options.file)\n\n  // read row groups\n  const asyncGroups = await parquetReadAsync(options)\n\n  const { rowStart = 0, rowEnd, columns, onChunk, onComplete, rowFormat } = options\n\n  // skip assembly if no onComplete or onChunk, but wait for reading to finish\n  if (!onComplete && !onChunk) {\n    for (const { asyncColumns } of asyncGroups) {\n      for (const { data } of asyncColumns) await data\n    }\n    return\n  }\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  // onChunk emit all chunks (don't await)\n  if (onChunk) {\n    for (const asyncGroup of assembled) {\n      for (const asyncColumn of asyncGroup.asyncColumns) {\n        asyncColumn.data.then(columnDatas => {\n          let rowStart = asyncGroup.groupStart\n          for (const columnData of columnDatas) {\n            onChunk({\n              columnName: asyncColumn.pathInSchema[0],\n              columnData,\n              rowStart,\n              rowEnd: rowStart + columnData.length,\n            })\n            rowStart += columnData.length\n          }\n        })\n      }\n    }\n  }\n\n  // onComplete transpose column chunks to rows\n  if (onComplete) {\n    /** @type {any[][]} */\n    const rows = []\n    for (const asyncGroup of assembled) {\n      // filter to rows in range\n      const selectStart = Math.max(rowStart - asyncGroup.groupStart, 0)\n      const selectEnd = Math.min((rowEnd ?? Infinity) - asyncGroup.groupStart, asyncGroup.groupRows)\n      // transpose column chunks to rows in output\n      const groupData = await asyncGroupToRows(asyncGroup, selectStart, selectEnd, columns, rowFormat)\n      concat(rows, groupData.slice(selectStart, selectEnd))\n    }\n    onComplete(rows)\n  } else {\n    // wait for all async groups to finish (complete takes care of this)\n    for (const { asyncColumns } of assembled) {\n      for (const { data } of asyncColumns) await data\n    }\n  }\n}\n\n/**\n * @param {ParquetReadOptions} options read options\n * @returns {AsyncRowGroup[]}\n */\nexport function parquetReadAsync(options) {\n  if (!options.metadata) throw new Error('parquet requires metadata')\n  // TODO: validate options (start, end, columns, etc)\n\n  // prefetch byte ranges\n  const plan = parquetPlan(options)\n  options.file = prefetchAsyncBuffer(options.file, plan)\n\n  // read row groups\n  return plan.groups.map(groupPlan => readRowGroup(options, plan, groupPlan))\n}\n\n/**\n * Reads a single column from a parquet file.\n *\n * @param {ParquetReadOptions} options\n * @returns {Promise<DecodedArray>}\n */\nexport async function parquetReadColumn(options) {\n  if (options.columns?.length !== 1) {\n    throw new Error('parquetReadColumn expected columns: [columnName]')\n  }\n  options.metadata ??= await parquetMetadataAsync(options.file)\n  const asyncGroups = parquetReadAsync(options)\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  /** @type {DecodedArray[]} */\n  const columnData = []\n  for (const rg of assembled) {\n    columnData.push(flatten(await rg.asyncColumns[0].data))\n  }\n  return flatten(columnData)\n}\n","import { parquetReadObjects } from './index.js'\nimport { parquetMetadataAsync, parquetSchema } from './metadata.js'\nimport { parquetReadColumn } from './read.js'\nimport { equals } from './utils.js'\n\n/**\n * Wraps parquetRead with filter and orderBy support.\n * This is a parquet-aware query engine that can read a subset of rows and columns.\n * Accepts optional filter object to filter the results and orderBy column name to sort the results.\n * Note that using orderBy may SIGNIFICANTLY increase the query time.\n *\n * @param {ParquetReadOptions & { filter?: ParquetQueryFilter, orderBy?: string }} options\n * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed\n */\nexport async function parquetQuery(options) {\n  if (!options.file || !(options.file.byteLength >= 0)) {\n    throw new Error('parquet expected AsyncBuffer')\n  }\n  options.metadata ??= await parquetMetadataAsync(options.file)\n\n  const { metadata, rowStart = 0, columns, orderBy, filter } = options\n  if (rowStart < 0) throw new Error('parquet rowStart must be positive')\n  const rowEnd = options.rowEnd ?? Number(metadata.num_rows)\n\n  // Collect columns needed for the query\n  const filterColumns = columnsNeededForFilter(filter)\n  const allColumns = parquetSchema(options.metadata).children.map(c => c.element.name)\n  // Check if all filter columns exist\n  const missingColumns = filterColumns.filter(column => !allColumns.includes(column))\n  if (missingColumns.length) {\n    throw new Error(`parquet filter columns not found: ${missingColumns.join(', ')}`)\n  }\n  if (orderBy && !allColumns.includes(orderBy)) {\n    throw new Error(`parquet orderBy column not found: ${orderBy}`)\n  }\n  const relevantColumns = columns ? allColumns.filter(column =>\n    columns.includes(column) || filterColumns.includes(column) || column === orderBy\n  ) : undefined\n  // Is the output a subset of the relevant columns?\n  const requiresProjection = columns && relevantColumns ? columns.length < relevantColumns.length : false\n\n  if (filter && !orderBy && rowEnd < metadata.num_rows) {\n    // iterate through row groups and filter until we have enough rows\n    const filteredRows = new Array()\n    let groupStart = 0\n    for (const group of metadata.row_groups) {\n      const groupEnd = groupStart + Number(group.num_rows)\n      // TODO: if expected > group size, start fetching next groups\n      const groupData = await parquetReadObjects({\n        ...options,\n        rowStart: groupStart,\n        rowEnd: groupEnd,\n        columns: relevantColumns,\n      })\n      for (const row of groupData) {\n        if (matchQuery(row, filter)) {\n          if (requiresProjection && relevantColumns) {\n            for (const column of relevantColumns) {\n              if (columns && !columns.includes(column)) {\n                delete row[column] // remove columns not in the projection\n              }\n            }\n          }\n          filteredRows.push(row)\n        }\n      }\n      if (filteredRows.length >= rowEnd) break\n      groupStart = groupEnd\n    }\n    return filteredRows.slice(rowStart, rowEnd)\n  } else if (filter) {\n    // read all rows, sort, and filter\n    const results = await parquetReadObjects({\n      ...options,\n      rowStart: undefined,\n      rowEnd: undefined,\n      columns: relevantColumns,\n    })\n    if (orderBy) results.sort((a, b) => compare(a[orderBy], b[orderBy]))\n    const filteredRows = new Array()\n    for (const row of results) {\n      if (matchQuery(row, filter)) {\n        if (requiresProjection && relevantColumns) {\n          for (const column of relevantColumns) {\n            if (columns && !columns.includes(column)) {\n              delete row[column] // remove columns not in the projection\n            }\n          }\n        }\n        filteredRows.push(row)\n      }\n    }\n    return filteredRows.slice(rowStart, rowEnd)\n  } else if (typeof orderBy === 'string') {\n    // sorted but unfiltered: fetch orderBy column first\n    const orderColumn = await parquetReadColumn({ ...options, rowStart: undefined, rowEnd: undefined, columns: [orderBy] })\n\n    // compute row groups to fetch\n    const sortedIndices = Array.from(orderColumn, (_, index) => index)\n      .sort((a, b) => compare(orderColumn[a], orderColumn[b]))\n      .slice(rowStart, rowEnd)\n\n    const sparseData = await parquetReadRows({ ...options, rows: sortedIndices })\n    const data = sortedIndices.map(index => sparseData[index])\n    return data\n  } else {\n    return await parquetReadObjects(options)\n  }\n}\n\n/**\n * Reads a list rows from a parquet file, reading only the row groups that contain the rows.\n * Returns a sparse array of rows.\n * @import {ParquetQueryFilter, ParquetReadOptions} from '../src/types.d.ts'\n * @param {ParquetReadOptions & { rows: number[] }} options\n * @returns {Promise<Record<string, any>[]>}\n */\nasync function parquetReadRows(options) {\n  const { file, rows } = options\n  options.metadata ||= await parquetMetadataAsync(file)\n  const { row_groups: rowGroups } = options.metadata\n  // Compute row groups to fetch\n  const groupIncluded = Array(rowGroups.length).fill(false)\n  let groupStart = 0\n  const groupEnds = rowGroups.map(group => groupStart += Number(group.num_rows))\n  for (const index of rows) {\n    const groupIndex = groupEnds.findIndex(end => index < end)\n    groupIncluded[groupIndex] = true\n  }\n\n  // Compute row ranges to fetch\n  const rowRanges = []\n  let rangeStart\n  groupStart = 0\n  for (let i = 0; i < groupIncluded.length; i++) {\n    const groupEnd = groupStart + Number(rowGroups[i].num_rows)\n    if (groupIncluded[i]) {\n      if (rangeStart === undefined) {\n        rangeStart = groupStart\n      }\n    } else {\n      if (rangeStart !== undefined) {\n        rowRanges.push([rangeStart, groupEnd])\n        rangeStart = undefined\n      }\n    }\n    groupStart = groupEnd\n  }\n  if (rangeStart !== undefined) {\n    rowRanges.push([rangeStart, groupStart])\n  }\n\n  // Fetch by row group and map to rows\n  const sparseData = new Array(Number(options.metadata.num_rows))\n  for (const [rangeStart, rangeEnd] of rowRanges) {\n    // TODO: fetch in parallel\n    const groupData = await parquetReadObjects({ ...options, rowStart: rangeStart, rowEnd: rangeEnd })\n    for (let i = rangeStart; i < rangeEnd; i++) {\n      sparseData[i] = groupData[i - rangeStart]\n      sparseData[i].__index__ = i\n    }\n  }\n  return sparseData\n}\n\n/**\n * @param {any} a\n * @param {any} b\n * @returns {number}\n */\nfunction compare(a, b) {\n  if (a < b) return -1\n  if (a > b) return 1\n  return 0 // TODO: null handling\n}\n\n/**\n * Match a record against a query filter\n *\n * @param {any} record\n * @param {ParquetQueryFilter} query\n * @returns {boolean}\n * @example matchQuery({ id: 1 }, { id: {$gte: 1} }) // true\n */\nexport function matchQuery(record, query = {}) {\n  if ('$and' in query && Array.isArray(query.$and)) {\n    return query.$and.every(subQuery => matchQuery(record, subQuery))\n  }\n  if ('$or' in query && Array.isArray(query.$or)) {\n    return query.$or.some(subQuery => matchQuery(record, subQuery))\n  }\n  if ('$nor' in query && Array.isArray(query.$nor)) {\n    return !query.$nor.some(subQuery => matchQuery(record, subQuery))\n  }\n\n  return Object.entries(query).every(([field, condition]) => {\n    const value = record[field]\n\n    // implicit $eq for non-object conditions\n    if (typeof condition !== 'object' || condition === null || Array.isArray(condition)) {\n      return equals(value, condition)\n    }\n\n    return Object.entries(condition || {}).every(([operator, target]) => {\n      switch (operator) {\n      case '$gt':\n        return value > target\n      case '$gte':\n        return value >= target\n      case '$lt':\n        return value < target\n      case '$lte':\n        return value <= target\n      case '$eq':\n        return equals(value, target)\n      case '$ne':\n        return !equals(value, target)\n      case '$in':\n        return Array.isArray(target) && target.includes(value)\n      case '$nin':\n        return Array.isArray(target) && !target.includes(value)\n      case '$not':\n        return !matchQuery({ [field]: value }, { [field]: target })\n      default:\n        return true\n      }\n    })\n  })\n}\n\n/**\n * Returns an array of column names that are needed to evaluate the mongo filter.\n *\n * @param {ParquetQueryFilter} [filter]\n * @returns {string[]}\n */\nfunction columnsNeededForFilter(filter) {\n  if (!filter) return []\n  /** @type {string[]} */\n  const columns = []\n  if ('$and' in filter && Array.isArray(filter.$and)) {\n    columns.push(...filter.$and.flatMap(columnsNeededForFilter))\n  } else if ('$or' in filter && Array.isArray(filter.$or)) {\n    columns.push(...filter.$or.flatMap(columnsNeededForFilter))\n  } else if ('$nor' in filter && Array.isArray(filter.$nor)) {\n    columns.push(...filter.$nor.flatMap(columnsNeededForFilter))\n  } else {\n    // Column filters\n    columns.push(...Object.keys(filter))\n  }\n  return columns\n}\n","import { parquetRead } from './read.js'\n\nexport { parquetMetadata, parquetMetadataAsync, parquetSchema } from './metadata.js'\nexport { parquetRead }\nexport { parquetQuery } from './query.js'\nexport { snappyUncompress } from './snappy.js'\nexport { asyncBufferFromUrl, byteLengthFromUrl, cachedAsyncBuffer, flatten, toJson } from './utils.js'\n\n/**\n * This is a helper function to read parquet row data as a promise.\n * It is a wrapper around the more configurable parquetRead function.\n *\n * @param {Omit<ParquetReadOptions, 'onComplete'>} options\n * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed\n*/\nexport function parquetReadObjects(options) {\n  return new Promise((onComplete, reject) => {\n    parquetRead({\n      rowFormat: 'object',\n      ...options,\n      onComplete,\n    }).catch(reject)\n  })\n}\n\n/**\n * Explicitly export types for use in downstream typescript projects through\n * `import { ParquetReadOptions } from 'hyparquet'` for example.\n *\n * @template {any} T\n * @typedef {import('../src/types.d.ts').Awaitable<T>} Awaitable<T>\n */\n/**\n * @typedef {import('../src/types.d.ts').AsyncBuffer} AsyncBuffer\n * @typedef {import('../src/types.d.ts').DataReader} DataReader\n * @typedef {import('../src/types.d.ts').FileMetaData} FileMetaData\n * @typedef {import('../src/types.d.ts').SchemaTree} SchemaTree\n * @typedef {import('../src/types.d.ts').SchemaElement} SchemaElement\n * @typedef {import('../src/types.d.ts').ParquetType} ParquetType\n * @typedef {import('../src/types.d.ts').FieldRepetitionType} FieldRepetitionType\n * @typedef {import('../src/types.d.ts').ConvertedType} ConvertedType\n * @typedef {import('../src/types.d.ts').TimeUnit} TimeUnit\n * @typedef {import('../src/types.d.ts').LogicalType} LogicalType\n * @typedef {import('../src/types.d.ts').LogicalTypeType} LogicalTypeType\n * @typedef {import('../src/types.d.ts').RowGroup} RowGroup\n * @typedef {import('../src/types.d.ts').ColumnChunk} ColumnChunk\n * @typedef {import('../src/types.d.ts').ColumnMetaData} ColumnMetaData\n * @typedef {import('../src/types.d.ts').Encoding} Encoding\n * @typedef {import('../src/types.d.ts').CompressionCodec} CompressionCodec\n * @typedef {import('../src/types.d.ts').Compressors} Compressors\n * @typedef {import('../src/types.d.ts').KeyValue} KeyValue\n * @typedef {import('../src/types.d.ts').Statistics} Statistics\n * @typedef {import('../src/types.d.ts').PageType} PageType\n * @typedef {import('../src/types.d.ts').PageHeader} PageHeader\n * @typedef {import('../src/types.d.ts').DataPageHeader} DataPageHeader\n * @typedef {import('../src/types.d.ts').DictionaryPageHeader} DictionaryPageHeader\n * @typedef {import('../src/types.d.ts').DecodedArray} DecodedArray\n * @typedef {import('../src/types.d.ts').OffsetIndex} OffsetIndex\n * @typedef {import('../src/types.d.ts').ColumnIndex} ColumnIndex\n * @typedef {import('../src/types.d.ts').BoundaryOrder} BoundaryOrder\n * @typedef {import('../src/types.d.ts').ColumnData} ColumnData\n * @typedef {import('../src/types.d.ts').ParquetReadOptions} ParquetReadOptions\n * @typedef {import('../src/types.d.ts').MetadataOptions} MetadataOptions\n * @typedef {import('../src/types.d.ts').ParquetParsers} ParquetParsers\n */\n","\nconst geometryTypePoint = 1\nconst geometryTypeLineString = 2\nconst geometryTypePolygon = 3\nconst geometryTypeMultiPoint = 4\nconst geometryTypeMultiLineString = 5\nconst geometryTypeMultiPolygon = 6\nconst geometryTypeGeometryCollection = 7\nconst geometryTypeCircularString = 8\nconst geometryTypeCompoundCurve = 9\nconst geometryTypeCurvePolygon = 10\nconst geometryTypeMultiCurve = 11\nconst geometryTypeMultiSurface = 12\nconst geometryTypeCurve = 13\nconst geometryTypeSurface = 14\nconst geometryTypePolyhedralSurface = 15\nconst geometryTypeTIN = 16\nconst geometryTypeTriangle = 17\nconst geometryTypeCircle = 18\nconst geometryTypeGeodesicString = 19\nconst geometryTypeEllipticalCurve = 20\nconst geometryTypeNurbsCurve = 21\nconst geometryTypeClothoid = 22\nconst geometryTypeSpiralCurve = 23\nconst geometryTypeCompoundSurface = 24\n\n/**\n * WKB (Well Known Binary) decoder for geometry objects.\n *\n * @import { Geometry } from './geojson.js'\n * @param {Uint8Array} wkb\n * @returns {Geometry} GeoJSON geometry object\n */\nexport function decodeWKB(wkb) {\n  const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength)\n  let offset = 0\n\n  // Byte order: 0 = big-endian, 1 = little-endian\n  const byteOrder = wkb[offset]; offset += 1\n  const isLittleEndian = byteOrder === 1\n\n  // Read geometry type\n  const geometryType = dv.getUint32(offset, isLittleEndian)\n  offset += 4\n\n  // WKB geometry types (OGC):\n  if (geometryType === geometryTypePoint) {\n    // Point\n    const x = dv.getFloat64(offset, isLittleEndian); offset += 8\n    const y = dv.getFloat64(offset, isLittleEndian); offset += 8\n    return { type: 'Point', coordinates: [x, y] }\n  } else if (geometryType === geometryTypeLineString) {\n    // LineString\n    const numPoints = dv.getUint32(offset, isLittleEndian); offset += 4\n    const coords = []\n    for (let i = 0; i < numPoints; i++) {\n      const x = dv.getFloat64(offset, isLittleEndian); offset += 8\n      const y = dv.getFloat64(offset, isLittleEndian); offset += 8\n      coords.push([x, y])\n    }\n    return { type: 'LineString', coordinates: coords }\n  } else if (geometryType === geometryTypePolygon) {\n    // Polygon\n    const numRings = dv.getUint32(offset, isLittleEndian); offset += 4\n    const coords = []\n    for (let r = 0; r < numRings; r++) {\n      const numPoints = dv.getUint32(offset, isLittleEndian); offset += 4\n      const ring = []\n      for (let p = 0; p < numPoints; p++) {\n        const x = dv.getFloat64(offset, isLittleEndian); offset += 8\n        const y = dv.getFloat64(offset, isLittleEndian); offset += 8\n        ring.push([x, y])\n      }\n      coords.push(ring)\n    }\n    return { type: 'Polygon', coordinates: coords }\n  } else if (geometryType === geometryTypeMultiPolygon) {\n    // MultiPolygon\n    const numPolygons = dv.getUint32(offset, isLittleEndian); offset += 4\n    const polygons = []\n    for (let i = 0; i < numPolygons; i++) {\n      // Each polygon has its own byte order & geometry type\n      const polyIsLittleEndian = wkb[offset] === 1; offset += 1\n      const polyType = dv.getUint32(offset, polyIsLittleEndian); offset += 4\n      if (polyType !== geometryTypePolygon) {\n        throw new Error(`Expected Polygon in MultiPolygon, got ${polyType}`)\n      }\n      const numRings = dv.getUint32(offset, polyIsLittleEndian); offset += 4\n\n      const pgCoords = []\n      for (let r = 0; r < numRings; r++) {\n        const numPoints = dv.getUint32(offset, polyIsLittleEndian); offset += 4\n        const ring = []\n        for (let p = 0; p < numPoints; p++) {\n          const x = dv.getFloat64(offset, polyIsLittleEndian); offset += 8\n          const y = dv.getFloat64(offset, polyIsLittleEndian); offset += 8\n          ring.push([x, y])\n        }\n        pgCoords.push(ring)\n      }\n      polygons.push(pgCoords)\n    }\n    return { type: 'MultiPolygon', coordinates: polygons }\n  } else if (geometryType === geometryTypeMultiPoint) {\n    // MultiPoint\n    const numPoints = dv.getUint32(offset, isLittleEndian); offset += 4\n    const points = []\n    for (let i = 0; i < numPoints; i++) {\n      // Each point has its own byte order & geometry type\n      const pointIsLittleEndian = wkb[offset] === 1; offset += 1\n      const pointType = dv.getUint32(offset, pointIsLittleEndian); offset += 4\n      if (pointType !== geometryTypePoint) {\n        throw new Error(`Expected Point in MultiPoint, got ${pointType}`)\n      }\n      const x = dv.getFloat64(offset, pointIsLittleEndian); offset += 8\n      const y = dv.getFloat64(offset, pointIsLittleEndian); offset += 8\n      points.push([x, y])\n    }\n    return { type: 'MultiPoint', coordinates: points }\n  } else if (geometryType === geometryTypeMultiLineString) {\n    // MultiLineString\n    const numLineStrings = dv.getUint32(offset, isLittleEndian); offset += 4\n    const lineStrings = []\n    for (let i = 0; i < numLineStrings; i++) {\n      // Each line has its own byte order & geometry type\n      const lineIsLittleEndian = wkb[offset] === 1; offset += 1\n      const lineType = dv.getUint32(offset, lineIsLittleEndian); offset += 4\n      if (lineType !== geometryTypeLineString) {\n        throw new Error(`Expected LineString in MultiLineString, got ${lineType}`)\n      }\n      const numPoints = dv.getUint32(offset, isLittleEndian); offset += 4\n      const coords = []\n      for (let p = 0; p < numPoints; p++) {\n        const x = dv.getFloat64(offset, lineIsLittleEndian); offset += 8\n        const y = dv.getFloat64(offset, lineIsLittleEndian); offset += 8\n        coords.push([x, y])\n      }\n      lineStrings.push(coords)\n    }\n    return { type: 'MultiLineString', coordinates: lineStrings }\n  } else {\n    throw new Error(`Unsupported geometry type: ${geometryType}`)\n  }\n}\n","import { asyncBufferFromUrl } from 'hyparquet'\nimport { toGeoJson } from '../src/index.js'\n\nasync function initMap() {\n  // @ts-expect-error MapsLibrary\n  const { Map } = await google.maps.importLibrary('maps')\n  const div = /** @type {HTMLElement} */document.getElementById('map')\n  // Create a new map\n  const map = new Map(div, {\n    center: { lat: 39, lng: -98 },\n    zoom: 4,\n  })\n\n  // URL or path to your GeoParquet file\n  const parquetUrl = 'https://hyparam.github.io/geoparquet/demo/polys.parquet'\n\n  try {\n    // Read the GeoParquet file and convert to GeoJSON\n    const file = await asyncBufferFromUrl({ url: parquetUrl, byteLength: 29838 })\n    console.log('GeoParquet file:', file)\n    const geojson = await toGeoJson({ file })\n\n    console.log('GeoJSON:', geojson)\n\n    // Add the GeoJSON data to the map\n    map.data.addGeoJson(geojson)\n  } catch (error) {\n    console.error('Error loading or parsing GeoParquet file:', error)\n  }\n}\ninitMap()\n","import { parquetMetadataAsync, parquetQuery } from 'hyparquet'\nimport { decodeWKB } from './wkb.js'\n\n/**\n * Convert a GeoParquet file to GeoJSON.\n * Input is an AsyncBuffer representing a GeoParquet file.\n * An AsyncBuffer is a buffer-like object that can be read asynchronously.\n *\n * @import { AsyncBuffer, Compressors } from 'hyparquet'\n * @import { Feature, GeoJSON } from './geojson.js'\n * @param {Object} options\n * @param {AsyncBuffer} options.file\n * @param {Compressors} [options.compressors]\n * @returns {Promise<GeoJSON>}\n */\nexport async function toGeoJson({ file, compressors }) {\n  const metadata = await parquetMetadataAsync(file)\n  const geoMetadata = metadata.key_value_metadata?.find(kv => kv.key === 'geo')\n  if (!geoMetadata) {\n    throw new Error('Invalid GeoParquet file: missing \"geo\" metadata')\n  }\n\n  // Geoparquet metadata\n  const geoSchema = JSON.parse(geoMetadata.value || '{}')\n\n  // Read all parquet data\n  const data = await parquetQuery({ file, utf8: false, compressors })\n\n  /** @type {Feature[]} */\n  const features = []\n  const primaryColumn = geoSchema.primary_column || 'geometry'\n  for (const row of data) {\n    const wkb = row[primaryColumn]\n    if (!wkb) {\n      // No geometry\n      continue\n    }\n\n    const geometry = decodeWKB(wkb)\n\n    // Extract properties (all fields except geometry)\n    /** @type {Record<string, any>} */\n    const properties = {}\n    for (const key of Object.keys(row)) {\n      let value = row[key]\n      if (key !== primaryColumn && value !== null) {\n        try {\n          // Try to parse JSON\n          value = JSON.parse(value)\n        } catch (error) {\n          // Ignore\n        }\n        properties[key] = value\n      }\n    }\n\n    /** @type {Feature} */\n    const feature = {\n      type: 'Feature',\n      geometry,\n      properties,\n    }\n\n    features.push(feature)\n  }\n\n  return {\n    type: 'FeatureCollection',\n    features,\n  }\n}\n"],"names":["ParquetType","Encoding","FieldRepetitionType","ConvertedType","CompressionCodec","PageType","DEFAULT_PARSERS","timestampFromMilliseconds","millis","Date","Number","timestampFromMicroseconds","micros","timestampFromNanoseconds","nanos","dateFromDays","days","convertWithDictionary","data","dictionary","encoding","columnDecoder","endsWith","output","Uint8Array","constructor","length","i","convert","element","parsers","utf8","type","converted_type","ctype","logical_type","ltype","factor","scale","arr","Array","parseDecimal","parseInt96Nanos","decoder","TextDecoder","map","v","JSON","parse","decode","Error","bitWidth","isSigned","BigInt64Array","BigUint64Array","buffer","byteOffset","BigInt","Int32Array","Uint32Array","from","parseFloat16","unit","parser","bytes","value","byte","bits","int16","sign","exp","frac","NaN","Infinity","schemaTree","schema","rootIndex","path","children","count","num_children","childElement","child","name","push","getSchemaPath","tree","part","find","getMaxRepetitionLevel","schemaPath","maxLevel","repetition_type","getMaxDefinitionLevel","slice","CompactType","deserializeTCompactProtocol","reader","lastFid","offset","view","byteLength","fid","newLastFid","readFieldBegin","readElement","getInt8","zigzag","readVarInt","readZigZag","readZigZagBigInt","getFloat64","stringLength","strBytes","elemType","listSize","sizeType","getUint8","size","getCompactType","readCollectionBegin","boolType","values","structValues","structLastFid","structFieldType","structFid","result","shift","readVarBigInt","delta","async","parquetMetadataAsync","asyncBuffer","initialFetchSize","footerOffset","Math","max","footerBuffer","footerView","DataView","getUint32","metadataLength","metadataOffset","metadataBuffer","combinedBuffer","ArrayBuffer","combinedView","set","parquetMetadata","arrayBuffer","metadataLengthOffset","metadata","version","field_1","field_2","field","type_length","field_3","field_4","field_5","field_6","field_7","precision","field_8","field_id","field_9","logicalType","field_10","columnSchema","filter","e","num_rows","row_groups","rowGroup","columns","column","columnIndex","file_path","file_offset","meta_data","encodings","path_in_schema","codec","num_values","total_uncompressed_size","total_compressed_size","key_value_metadata","data_page_offset","index_page_offset","dictionary_page_offset","field_11","statistics","convertStats","field_12","encoding_stats","field_13","encodingStat","page_type","bloom_filter_offset","field_14","bloom_filter_length","field_15","size_statistics","field_16","unencoded_byte_array_data_bytes","repetition_level_histogram","definition_level_histogram","offset_index_offset","offset_index_length","column_index_offset","column_index_length","crypto_metadata","encrypted_column_metadata","total_byte_size","sorting_columns","sortingColumn","column_idx","descending","nulls_first","ordinal","keyValue","key","created_by","metadata_length","parquetSchema","isAdjustedToUTC","timeUnit","stats","convertMetadata","min","null_count","distinct_count","max_value","min_value","is_max_value_exact","is_min_value_exact","undefined","getFloat32","getInt32","getBigInt64","concat","aaa","bbb","equals","a","b","isArray","every","Object","keys","k","asyncBufferFromUrl","url","requestInit","fetch","customFetch","globalThis","method","then","res","ok","status","headers","get","parseInt","byteLengthFromUrl","init","start","end","Headers","endStr","body","flatten","chunks","chunk","getColumnRange","columnOffset","startByte","endByte","assembleLists","definitionLevels","repetitionLevels","n","maxDefinitionLevel","repetitionPath","valueIndex","containerStack","currentContainer","currentDepth","currentDefLevel","currentRepLevel","at","def","rep","pop","newList","assembleNested","subcolumnData","depth","join","optional","nextDepth","firstChild","isListLike","sublist","subDepth","subcolumn","flattenAtDepth","delete","keyChild","valueChild","isMapLike","mapName","out","assembleMaps","invertDepth","struct","childData","inverted","invertStruct","obj","j","deltaBinaryUnpack","int32","blockSize","miniblockPerBlock","outputIndex","valuesPerMiniblock","minDelta","bitWidths","bitpackPos","miniblockCount","mask","ceil","deltaLengthByteArray","lengths","clz32","readRleBitPackedHybrid","width","startOffset","seen","header","readBitPacked","readRle","left","right","byteStreamSplit","typeLength","byteWidth","Float32Array","Float64Array","split","subarray","readPlain","fixedLength","bitOffset","readPlainBoolean","align","readPlainInt32","readPlainInt64","low","high","readPlainInt96","readPlainFloat","readPlainDouble","readPlainByteArray","readPlainByteArrayFixed","aligned","WORD_MASK","copyBytes","fromArray","fromPos","toArray","toPos","readDataPage","daph","dataPage","maxRepetitionLevel","readRepetitionLevels","numNulls","readDefinitionLevels","nValues","x","decompressPage","compressedBytes","uncompressed_page_size","compressors","page","customDecompressor","input","inputLength","outputLength","pos","outPos","c","len","isNaN","lengthSize","snappyUncompress","readDataPageV2","ph","daph2","data_page_header_v2","repetition_levels_byte_length","readRepetitionLevelsV2","definition_levels_byte_length","readDefinitionLevelsV2","uncompressedPageSize","is_compressed","pageView","pageReader","num_nulls","prefixData","suffixData","suffix","deltaByteArray","readColumn","groupStart","selectStart","selectEnd","onPage","columnName","lastChunk","rowCount","emitLastChunk","columnData","rowStart","rowEnd","parquetHeader","readPage","lastChunkLength","previousChunk","pageStart","compressed_page_size","data_page_header","isFlatColumn","diph","dictionary_page_header","crc","definition_level_encoding","repetition_level_encoding","index_page_header","is_sorted","asyncGroupToRows","asyncColumns","rowFormat","groupData","columnDatas","Promise","all","includedColumnNames","pathInSchema","includes","columnOrder","columnIndexes","findIndex","row","rowData","assembleAsync","asyncRowGroup","assembled","childColumns","flatData","Map","flatColumn","asyncColumn","parquetReadAsync","options","plan","groups","fetches","groupRows","groupEnd","ranges","groupSize","isFinite","parquetPlan","file","promises","index","endOffset","prefetchAsyncBuffer","groupPlan","columnBytes","console","warn","resolve","readRowGroup","parquetQuery","orderBy","filterColumns","columnsNeededForFilter","allColumns","missingColumns","relevantColumns","requiresProjection","filteredRows","group","parquetReadObjects","matchQuery","results","sort","compare","orderColumn","asyncGroups","arg","rg","parquetReadColumn","sortedIndices","_","sparseData","rows","rowGroups","groupIncluded","fill","groupEnds","rowRanges","rangeStart","rangeEnd","__index__","parquetReadRows","record","query","$and","subQuery","$or","some","$nor","entries","condition","operator","target","flatMap","onComplete","reject","onChunk","asyncGroup","parquetRead","catch","decodeWKB","wkb","dv","byteOrder","isLittleEndian","geometryType","y","coordinates","numPoints","coords","numRings","r","ring","p","numPolygons","polygons","polyIsLittleEndian","polyType","pgCoords","points","pointIsLittleEndian","pointType","numLineStrings","lineStrings","lineIsLittleEndian","lineType","google","maps","importLibrary","document","getElementById","center","lat","lng","zoom","log","geojson","geoMetadata","kv","geoSchema","features","primaryColumn","primary_column","geometry","properties","error","feature","toGeoJson","addGeoJson","initMap"],"mappings":"AACO,MAAMA,EAAc,CACzB,UACA,QACA,QACA,QACA,QACA,SACA,aACA,wBAIWC,EAAW,CACtB,QACA,gBACA,mBACA,MACA,aACA,sBACA,0BACA,mBACA,iBACA,qBAIWC,EAAsB,CACjC,WACA,WACA,YAIWC,EAAgB,CAC3B,OACA,MACA,gBACA,OACA,OACA,UACA,OACA,cACA,cACA,mBACA,mBACA,SACA,UACA,UACA,UACA,QACA,SACA,SACA,SACA,OACA,OACA,YAIWC,EAAmB,CAC9B,eACA,SACA,OACA,MACA,SACA,MACA,OACA,WAIWC,EAAW,CACtB,YACA,aACA,kBACA,gBCpEWC,EAAkB,CAC7BC,0BAA0BC,GACjB,IAAIC,KAAKC,OAAOF,IAEzBG,0BAA0BC,GACjB,IAAIH,KAAKC,OAAOE,EAAS,QAElCC,yBAAyBC,GAChB,IAAIL,KAAKC,OAAOI,EAAQ,WAEjCC,aAAaC,GAEJ,IAAIP,KADS,MACJO,IAab,SAASC,EAAsBC,EAAMC,EAAYC,EAAUC,GAChE,GAAIF,GAAcC,EAASE,SAAS,eAAgB,CAClD,IAAIC,EAASL,EACTA,aAAgBM,cAAgBL,aAAsBK,cAExDD,EAAS,IAAIJ,EAAWM,YAAYP,EAAKQ,SAE3C,IAAK,IAAIC,EAAI,EAAGA,EAAIT,EAAKQ,OAAQC,IAC/BJ,EAAOI,GAAKR,EAAWD,EAAKS,IAE9B,OAAOJ,CACT,CACE,OAAOK,EAAQV,EAAMG,EAEzB,CASO,SAASO,EAAQV,EAAMG,GAC5B,MAAMQ,QAAEA,EAAOC,QAAEA,EAAOC,KAAEA,GAAO,GAASV,GACpCW,KAAEA,EAAMC,eAAgBC,EAAOC,aAAcC,GAAUP,EAC7D,GAAc,YAAVK,EAAqB,CACvB,MACMG,EAAS,MADDR,EAAQS,OAAS,GAEzBC,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC1BT,EAAK,aAAcM,WACrBe,EAAIZ,GAAKc,EAAavB,EAAKS,IAAMU,EAEjCE,EAAIZ,GAAKjB,OAAOQ,EAAKS,IAAMU,EAG/B,OAAOE,CACT,CACA,IAAKL,GAAkB,UAATF,EAAkB,CAC9B,MAAMO,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKG,EAAQjB,yBAAyB6B,EAAgBxB,EAAKS,KAEjE,OAAOY,CACT,CACA,GAAc,SAAVL,EAAkB,CACpB,MAAMK,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKG,EAAQf,aAAaG,EAAKS,IAErC,OAAOY,CACT,CACA,GAAc,qBAAVL,EAA8B,CAChC,MAAMK,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKG,EAAQvB,0BAA0BW,EAAKS,IAElD,OAAOY,CACT,CACA,GAAc,qBAAVL,EAA8B,CAChC,MAAMK,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKG,EAAQnB,0BAA0BO,EAAKS,IAElD,OAAOY,CACT,CACA,GAAc,SAAVL,EAAkB,CACpB,MAAMS,EAAU,IAAIC,YACpB,OAAO1B,EAAK2B,IAAIC,GAAKC,KAAKC,MAAML,EAAQM,OAAOH,IACjD,CACA,GAAc,SAAVZ,EACF,MAAM,IAAIgB,MAAM,8BAElB,GAAc,aAAVhB,EACF,MAAM,IAAIgB,MAAM,kCAElB,GAAc,SAAVhB,GAAoC,WAAhBE,GAAOJ,MAAqBD,GAAiB,eAATC,EAAuB,CACjF,MAAMW,EAAU,IAAIC,YACdL,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKT,EAAKS,IAAMgB,EAAQM,OAAO/B,EAAKS,IAE1C,OAAOY,CACT,CACA,GAAc,YAAVL,GAAuC,YAAhBE,GAAOJ,MAAyC,KAAnBI,EAAMe,WAAoBf,EAAMgB,SAAU,CAChG,GAAIlC,aAAgBmC,cAClB,OAAO,IAAIC,eAAepC,EAAKqC,OAAQrC,EAAKsC,WAAYtC,EAAKQ,QAE/D,MAAMa,EAAM,IAAIe,eAAepC,EAAKQ,QACpC,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAAKY,EAAIZ,GAAK8B,OAAOvC,EAAKS,IAC1D,OAAOY,CACT,CACA,GAAc,YAAVL,GAAuC,YAAhBE,GAAOJ,MAAyC,KAAnBI,EAAMe,WAAoBf,EAAMgB,SAAU,CAChG,GAAIlC,aAAgBwC,WAClB,OAAO,IAAIC,YAAYzC,EAAKqC,OAAQrC,EAAKsC,WAAYtC,EAAKQ,QAE5D,MAAMa,EAAM,IAAIoB,YAAYzC,EAAKQ,QACjC,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAAKY,EAAIZ,GAAKT,EAAKS,GACnD,OAAOY,CACT,CACA,GAAoB,YAAhBH,GAAOJ,KACT,OAAOQ,MAAMoB,KAAK1C,GAAM2B,IAAIgB,GAE9B,GAAoB,cAAhBzB,GAAOJ,KAAsB,CAC/B,MAAM8B,KAAEA,GAAS1B,EAEjB,IAAI2B,EAASjC,EAAQvB,0BACR,WAATuD,IAAmBC,EAASjC,EAAQnB,2BAC3B,UAATmD,IAAkBC,EAASjC,EAAQjB,0BACvC,MAAM0B,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKoC,EAAO7C,EAAKS,IAEvB,OAAOY,CACT,CACA,OAAOrB,CACT,CAMO,SAASuB,EAAauB,GAC3B,IAAIC,EAAQ,EACZ,IAAK,MAAMC,KAAQF,EACjBC,EAAgB,IAARA,EAAcC,EAIxB,MAAMC,EAAsB,EAAfH,EAAMtC,OAKnB,OAJIuC,GAAS,IAAME,EAAO,KACxBF,GAAS,GAAKE,GAGTF,CACT,CAOA,SAASvB,EAAgBuB,GAGvB,OAAc,kBAFAA,GAAS,KAAO,WACT,oBAARA,EAEf,CAMO,SAASJ,EAAaG,GAC3B,IAAKA,EAAO,OACZ,MAAMI,EAAQJ,EAAM,IAAM,EAAIA,EAAM,GAC9BK,EAAOD,GAAS,MAAU,EAC1BE,EAAMF,GAAS,GAAK,GACpBG,EAAe,KAARH,EACb,OAAY,IAARE,EAAkBD,EAAO,IAAK,IAAOE,EAAO,MACpC,KAARD,EAAqBC,EAAOC,IAAMH,GAAOI,KACtCJ,EAAO,IAAMC,EAAM,KAAO,EAAIC,EAAO,KAC9C,CC1LA,SAASG,EAAWC,EAAQC,EAAWC,GACrC,MAAMhD,EAAU8C,EAAOC,GACjBE,EAAW,GACjB,IAAIC,EAAQ,EAGZ,GAAIlD,EAAQmD,aACV,KAAOF,EAASpD,OAASG,EAAQmD,cAAc,CAC7C,MAAMC,EAAeN,EAAOC,EAAYG,GAClCG,EAAQR,EAAWC,EAAQC,EAAYG,EAAO,IAAIF,EAAMI,EAAaE,OAC3EJ,GAASG,EAAMH,MACfD,EAASM,KAAKF,EAChB,CAGF,MAAO,CAAEH,QAAOlD,UAASiD,WAAUD,OACrC,CASO,SAASQ,EAAcV,EAAQQ,GACpC,IAAIG,EAAOZ,EAAWC,EAAQ,EAAG,IACjC,MAAME,EAAO,CAACS,GACd,IAAK,MAAMC,KAAQJ,EAAM,CACvB,MAAMD,EAAQI,EAAKR,SAASU,KAAKN,GAASA,EAAMrD,QAAQsD,OAASI,GACjE,IAAKL,EAAO,MAAM,IAAIhC,MAAM,qCAAqCiC,KACjEN,EAAKO,KAAKF,GACVI,EAAOJ,CACT,CACA,OAAOL,CACT,CAQO,SAASY,EAAsBC,GACpC,IAAIC,EAAW,EACf,IAAK,MAAM9D,QAAEA,KAAa6D,EACQ,aAA5B7D,EAAQ+D,iBACVD,IAGJ,OAAOA,CACT,CAQO,SAASE,EAAsBH,GACpC,IAAIC,EAAW,EACf,IAAK,MAAM9D,QAAEA,KAAa6D,EAAWI,MAAM,GACT,aAA5BjE,EAAQ+D,iBACVD,IAGJ,OAAOA,CACT,CC3EO,MAAMI,EACL,EADKA,EAEL,EAFKA,EAGJ,EAHIA,EAIL,EAJKA,EAKN,EALMA,EAMN,EANMA,EAON,EAPMA,EAQH,EARGA,EASH,EATGA,EAUL,EAVKA,EAaH,GAUH,SAASC,EAA4BC,GAC1C,IAAIC,EAAU,EAEd,MAAMjC,EAAQ,CAAA,EAEd,KAAOgC,EAAOE,OAASF,EAAOG,KAAKC,YAAY,CAE7C,MAAOrE,EAAMsE,EAAKC,GAAcC,EAAeP,EAAQC,GAGvD,GAFAA,EAAUK,EAENvE,IAAS+D,EACX,MAIF9B,EAAM,SAASqC,KAASG,EAAYR,EAAQjE,EAC9C,CAEA,OAAOiC,CACT,CAUA,SAASwC,EAAYR,EAAQjE,GAC3B,OAAQA,GACR,KAAK+D,EACH,OAAO,EACT,KAAKA,EACH,OAAO,EACT,KAAKA,EAEH,OAAOE,EAAOG,KAAKM,QAAQT,EAAOE,UACpC,KAAKJ,EACL,KAAKA,EACH,OA0FG,SAAoBE,GACzB,MAAMU,EAASC,EAAWX,GAE1B,OAAOU,IAAW,IAAe,EAATA,EAC1B,CA9FWE,CAAWZ,GACpB,KAAKF,EACH,OAAOe,EAAiBb,GAC1B,KAAKF,EAAoB,CACvB,MAAM9B,EAAQgC,EAAOG,KAAKW,WAAWd,EAAOE,QAAQ,GAEpD,OADAF,EAAOE,QAAU,EACVlC,CACT,CACA,KAAK8B,EAAoB,CACvB,MAAMiB,EAAeJ,EAAWX,GAC1BgB,EAAW,IAAIzF,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQa,GAE5F,OADAf,EAAOE,QAAUa,EACVC,CACT,CACA,KAAKlB,EAAkB,CACrB,MAAOmB,EAAUC,GAsIrB,SAA6BlB,GAC3B,MAAMmB,EAAWnB,EAAOG,KAAKiB,SAASpB,EAAOE,UACvCmB,EAAOF,GAAY,EACnBpF,EAAOuF,EAAeH,GAC5B,GAAa,KAATE,EAAa,CAEf,MAAO,CAACtF,EADQ4E,EAAWX,GAE7B,CACA,MAAO,CAACjE,EAAMsF,EAChB,CA/IiCE,CAAoBvB,GAC3CwB,EAAWP,IAAanB,GAAoBmB,IAAanB,EACzD2B,EAAS,IAAIlF,MAAM2E,GACzB,IAAK,IAAIxF,EAAI,EAAGA,EAAIwF,EAAUxF,IAC5B+F,EAAO/F,GAAK8F,EAAqD,IAA1ChB,EAAYR,EAAQF,GAA0BU,EAAYR,EAAQiB,GAE3F,OAAOQ,CACT,CACA,KAAK3B,EAAoB,CAEvB,MAAM4B,EAAe,CAAA,EACrB,IAAIC,EAAgB,EACpB,OAAa,CACX,IAAIC,EAAiBC,EAErB,IADCD,EAAiBC,EAAWF,GAAiBpB,EAAeP,EAAQ2B,GACjEC,IAAoB9B,EACtB,MAEF4B,EAAa,SAASG,KAAerB,EAAYR,EAAQ4B,EAC3D,CACA,OAAOF,CACT,CAEA,QACE,MAAM,IAAIzE,MAAM,0BAA0BlB,KAE9C,CAUO,SAAS4E,EAAWX,GACzB,IAAI8B,EAAS,EACTC,EAAQ,EACZ,OAAa,CACX,MAAM9D,EAAO+B,EAAOG,KAAKiB,SAASpB,EAAOE,UAEzC,GADA4B,IAAkB,IAAP7D,IAAgB8D,IACd,IAAP9D,GACJ,OAAO6D,EAETC,GAAS,CACX,CACF,CAyCO,SAASlB,EAAiBb,GAC/B,MAAMU,EAlCR,SAAuBV,GACrB,IAAI8B,EAAS,GACTC,EAAQ,GACZ,OAAa,CACX,MAAM9D,EAAO+B,EAAOG,KAAKiB,SAASpB,EAAOE,UAEzC,GADA4B,GAAUtE,OAAc,IAAPS,IAAgB8D,IACpB,IAAP9D,GACJ,OAAO6D,EAETC,GAAS,EACX,CACF,CAuBiBC,CAAchC,GAE7B,OAAOU,GAAU,KAAgB,GAATA,EAC1B,CAQA,SAASY,EAAerD,GACtB,OAAc,GAAPA,CACT,CASA,SAASsC,EAAeP,EAAQC,GAC9B,MAAMlE,EAAOiE,EAAOG,KAAKiB,SAASpB,EAAOE,UACzC,IAAY,GAAPnE,KAAiB+D,EAEpB,MAAO,CAAC,EAAG,EAAGG,GAEhB,MAAMgC,EAAQlG,GAAQ,EACtB,IAAIsE,EACJ,IAAI4B,EAIF,MAAM,IAAIhF,MAAM,oCAElB,OAJEoD,EAAMJ,EAAUgC,EAIX,CAACX,EAAevF,GAAOsE,EAAKA,EACrC,CC9KO6B,eAAeC,EAAqBC,GAAavG,QAAEA,EAAOwG,iBAAEA,EA1B5B,QA0B2E,IAChH,KAAKD,GAAiBA,EAAYhC,YAAc,GAAI,MAAM,IAAInD,MAAM,gCAGpE,MAAMqF,EAAeC,KAAKC,IAAI,EAAGJ,EAAYhC,WAAaiC,GACpDI,QAAqBL,EAAYvC,MAAMyC,EAAcF,EAAYhC,YAGjEsC,EAAa,IAAIC,SAASF,GAChC,GAAgE,YAA5DC,EAAWE,UAAUH,EAAarC,WAAa,GAAG,GACpD,MAAM,IAAInD,MAAM,yCAKlB,MAAM4F,EAAiBH,EAAWE,UAAUH,EAAarC,WAAa,GAAG,GACzE,GAAIyC,EAAiBT,EAAYhC,WAAa,EAC5C,MAAM,IAAInD,MAAM,2BAA2B4F,8BAA2CT,EAAYhC,WAAa,KAIjH,GAAIyC,EAAiB,EAAIR,EAAkB,CAEzC,MAAMS,EAAiBV,EAAYhC,WAAayC,EAAiB,EAC3DE,QAAuBX,EAAYvC,MAAMiD,EAAgBR,GAEzDU,EAAiB,IAAIC,YAAYJ,EAAiB,GAClDK,EAAe,IAAI3H,WAAWyH,GAGpC,OAFAE,EAAaC,IAAI,IAAI5H,WAAWwH,IAChCG,EAAaC,IAAI,IAAI5H,WAAWkH,GAAeH,EAAeQ,GACvDM,EAAgBJ,EAAgB,CAAEnH,WAC3C,CAEE,OAAOuH,EAAgBX,EAAc,CAAE5G,WAE3C,CASO,SAASuH,EAAgBC,GAAaxH,QAAEA,GAAY,CAAA,GACzD,KAAMwH,aAAuBJ,aAAc,MAAM,IAAIhG,MAAM,gCAC3D,MAAMkD,EAAO,IAAIwC,SAASU,GAM1B,GAHAxH,EAAU,IAAKxB,KAAoBwB,GAG/BsE,EAAKC,WAAa,EACpB,MAAM,IAAInD,MAAM,6BAElB,GAAkD,YAA9CkD,EAAKyC,UAAUzC,EAAKC,WAAa,GAAG,GACtC,MAAM,IAAInD,MAAM,yCAKlB,MAAMqG,EAAuBnD,EAAKC,WAAa,EACzCyC,EAAiB1C,EAAKyC,UAAUU,GAAsB,GAC5D,GAAIT,EAAiB1C,EAAKC,WAAa,EAErC,MAAM,IAAInD,MAAM,2BAA2B4F,8BAA2C1C,EAAKC,WAAa,KAG1G,MAEMmD,EAAWxD,EADF,CAAEI,OAAMD,OADAoD,EAAuBT,IAGxCnG,EAAU,IAAIC,YACpB,SAASK,EAAiCgB,GACxC,OAAOA,GAAStB,EAAQM,OAAOgB,EACjC,CAGA,MAAMwF,EAAUD,EAASE,QAEnB/E,EAAS6E,EAASG,QAAQ9G,IAAwB+G,IAAK,CAC3D5H,KAAMhC,EAAY4J,EAAMF,SACxBG,YAAaD,EAAMD,QACnB/D,gBAAiB1F,EAAoB0J,EAAME,SAC3C3E,KAAMlC,EAAO2G,EAAMG,SACnB/E,aAAc4E,EAAMI,QACpB/H,eAAgB9B,EAAcyJ,EAAMK,SACpC3H,MAAOsH,EAAMM,QACbC,UAAWP,EAAMQ,QACjBC,SAAUT,EAAMU,QAChBnI,aAAcoI,EAAYX,EAAMY,aAG5BC,EAAe9F,EAAO+F,OAAOC,GAAKA,EAAE3I,MACpC4I,EAAWpB,EAASM,QACpBe,EAAarB,EAASO,QAAQlH,IAAwBiI,IAAQ,CAClEC,QAASD,EAASpB,QAAQ7G,IAAI,CAAoBmI,EAA8BC,KAAW,CACzFC,UAAWjI,EAAO+H,EAAOtB,SACzByB,YAAaH,EAAOrB,QACpByB,UAAWJ,EAAOlB,SAAW,CAC3B9H,KAAMhC,EAAYgL,EAAOlB,QAAQJ,SACjC2B,UAAWL,EAAOlB,QAAQH,SAAS9G,IAA2B8H,GAAM1K,EAAS0K,IAC7EW,eAAgBN,EAAOlB,QAAQA,QAAQjH,IAAII,GAC3CsI,MAAOnL,EAAiB4K,EAAOlB,QAAQC,SACvCyB,WAAYR,EAAOlB,QAAQE,QAC3ByB,wBAAyBT,EAAOlB,QAAQG,QACxCyB,sBAAuBV,EAAOlB,QAAQI,QACtCyB,mBAAoBX,EAAOlB,QAAQM,QACnCwB,iBAAkBZ,EAAOlB,QAAQQ,QACjCuB,kBAAmBb,EAAOlB,QAAQU,SAClCsB,uBAAwBd,EAAOlB,QAAQiC,SACvCC,WAAYC,EAAajB,EAAOlB,QAAQoC,SAAUzB,EAAaQ,GAAcnJ,GAC7EqK,eAAgBnB,EAAOlB,QAAQsC,UAAUvJ,IAAwBwJ,IAAY,CAC3EC,UAAWjM,EAASgM,EAAa3C,SACjCtI,SAAUnB,EAASoM,EAAa1C,SAChC5E,MAAOsH,EAAavC,WAEtByC,oBAAqBvB,EAAOlB,QAAQ0C,SACpCC,oBAAqBzB,EAAOlB,QAAQ4C,SACpCC,gBAAiB3B,EAAOlB,QAAQ8C,UAAY,CAC1CC,gCAAiC7B,EAAOlB,QAAQ8C,SAASlD,QACzDoD,2BAA4B9B,EAAOlB,QAAQ8C,SAASjD,QACpDoD,2BAA4B/B,EAAOlB,QAAQ8C,SAAS9C,UAGxDkD,oBAAqBhC,EAAOjB,QAC5BkD,oBAAqBjC,EAAOhB,QAC5BkD,oBAAqBlC,EAAOf,QAC5BkD,oBAAqBnC,EAAOd,QAC5BkD,gBAAiBpC,EAAOZ,QACxBiD,0BAA2BrC,EAAOV,WAEpCgD,gBAAiBxC,EAASnB,QAC1BiB,SAAUE,EAAShB,QACnByD,gBAAiBzC,EAASf,SAASlH,IAAwB2K,IAAa,CACtEC,WAAYD,EAAc9D,QAC1BgE,WAAYF,EAAc7D,QAC1BgE,YAAaH,EAAc1D,WAE7BqB,YAAaL,EAASd,QACtB0B,sBAAuBZ,EAASb,QAChC2D,QAAS9C,EAASZ,WAEdyB,EAAqBnC,EAASQ,SAASnH,IAAwBgL,IAAQ,CAC3EC,IAAK7K,EAAO4K,EAASnE,SACrBzF,MAAOhB,EAAO4K,EAASlE,YAIzB,MAAO,CACLF,UACA9E,SACAiG,WACAC,aACAc,qBACAoC,WARiB9K,EAAOuG,EAASS,SASjC+D,gBAAiBlF,EAErB,CAQO,SAASmF,GAActJ,OAAEA,IAC9B,OAAOU,EAAcV,EAAQ,IAAI,EACnC,CAMA,SAAS4F,EAAYA,GACnB,OAAIA,GAAab,QAAgB,CAAE1H,KAAM,UACrCuI,GAAaZ,QAAgB,CAAE3H,KAAM,OACrCuI,GAAaT,QAAgB,CAAE9H,KAAM,QACrCuI,GAAaR,QAAgB,CAAE/H,KAAM,QACrCuI,GAAaP,QAAgB,CAC/BhI,KAAM,UACNM,MAAOiI,EAAYP,QAAQN,QAC3BS,UAAWI,EAAYP,QAAQL,SAE7BY,GAAaN,QAAgB,CAAEjI,KAAM,QACrCuI,GAAaL,QAAgB,CAC/BlI,KAAM,OACNkM,gBAAiB3D,EAAYL,QAAQR,QACrC5F,KAAMqK,EAAS5D,EAAYL,QAAQP,UAEjCY,GAAaH,QAAgB,CAC/BpI,KAAM,YACNkM,gBAAiB3D,EAAYH,QAAQV,QACrC5F,KAAMqK,EAAS5D,EAAYH,QAAQT,UAEjCY,GAAaC,SAAiB,CAChCxI,KAAM,UACNmB,SAAUoH,EAAYC,SAASd,QAC/BtG,SAAUmH,EAAYC,SAASb,SAE7BY,GAAawB,SAAiB,CAAE/J,KAAM,QACtCuI,GAAa2B,SAAiB,CAAElK,KAAM,QACtCuI,GAAa6B,SAAiB,CAAEpK,KAAM,QACtCuI,GAAaiC,SAAiB,CAAExK,KAAM,QACtCuI,GAAamC,SAAiB,CAAE1K,KAAM,WACnCuI,CACT,CAMA,SAAS4D,EAASrK,GAChB,GAAIA,EAAK4F,QAAS,MAAO,SACzB,GAAI5F,EAAK6F,QAAS,MAAO,SACzB,GAAI7F,EAAKgG,QAAS,MAAO,QACzB,MAAM,IAAI5G,MAAM,6BAClB,CAWA,SAAS+I,EAAamC,EAAOzJ,EAAQ7C,GACnC,OAAOsM,GAAS,CACd3F,IAAK4F,EAAgBD,EAAM1E,QAAS/E,EAAQ7C,GAC5CwM,IAAKD,EAAgBD,EAAMzE,QAAShF,EAAQ7C,GAC5CyM,WAAYH,EAAMtE,QAClB0E,eAAgBJ,EAAMrE,QACtB0E,UAAWJ,EAAgBD,EAAMpE,QAASrF,EAAQ7C,GAClD4M,UAAWL,EAAgBD,EAAMnE,QAAStF,EAAQ7C,GAClD6M,mBAAoBP,EAAMlE,QAC1B0E,mBAAoBR,EAAMhE,QAE9B,CAQO,SAASiE,EAAgBpK,EAAOU,EAAQ7C,GAC7C,MAAME,KAAEA,EAAIC,eAAEA,EAAcE,aAAEA,GAAiBwC,EAC/C,QAAckK,IAAV5K,EAAqB,OAAOA,EAChC,GAAa,YAATjC,EAAoB,OAAoB,IAAbiC,EAAM,GACrC,GAAa,eAATjC,EAAuB,OAAO,IAAIY,aAAcK,OAAOgB,GAC3D,MAAMmC,EAAO,IAAIwC,SAAS3E,EAAMV,OAAQU,EAAMT,WAAYS,EAAMoC,YAChE,MAAa,UAATrE,GAAwC,IAApBoE,EAAKC,WAAyBD,EAAK0I,WAAW,GAAG,GAC5D,WAAT9M,GAAyC,IAApBoE,EAAKC,WAAyBD,EAAKW,WAAW,GAAG,GAC7D,UAAT/E,GAAuC,SAAnBC,EAAkCH,EAAQf,aAAaqF,EAAK2I,SAAS,GAAG,IACnF,UAAT/M,GAAuC,qBAAnBC,EAA8CH,EAAQvB,0BAA0B6F,EAAK4I,YAAY,GAAG,IAC/G,UAAThN,GAAuC,qBAAnBC,EAA8CH,EAAQnB,0BAA0ByF,EAAK4I,YAAY,GAAG,IAC/G,UAAThN,GAA2C,cAAvBG,GAAcH,MAA+C,UAAvBG,GAAc2B,KAAyBhC,EAAQjB,yBAAyBuF,EAAK4I,YAAY,GAAG,IAC7I,UAAThN,GAA2C,cAAvBG,GAAcH,MAA+C,WAAvBG,GAAc2B,KAA0BhC,EAAQnB,0BAA0ByF,EAAK4I,YAAY,GAAG,IAC/I,UAAThN,GAA2C,cAAvBG,GAAcH,KAA6BF,EAAQvB,0BAA0B6F,EAAK4I,YAAY,GAAG,IAC5G,UAAThN,GAAwC,IAApBoE,EAAKC,WAAyBD,EAAK2I,SAAS,GAAG,GAC1D,UAAT/M,GAAwC,IAApBoE,EAAKC,WAAyBD,EAAK4I,YAAY,GAAG,GACnD,YAAnB/M,EAAqCQ,EAAawB,GAAS,MAAQU,EAAOrC,OAAS,GAC5D,YAAvBH,GAAcH,KAA2B6B,EAAaI,GACdA,CAG9C,CC1QO,SAASgL,EAAOC,EAAKC,GAE1B,IAAK,IAAIxN,EAAI,EAAGA,EAAIwN,EAAIzN,OAAQC,GADlB,IAEZuN,EAAI9J,QAAQ+J,EAAIrJ,MAAMnE,EAAGA,EAFb,KAIhB,CASO,SAASyN,EAAOC,EAAGC,GACxB,OAAID,IAAMC,IACND,aAAa7N,YAAc8N,aAAa9N,WAAmB4N,EAAO5M,MAAMoB,KAAKyL,GAAI7M,MAAMoB,KAAK0L,OAC3FD,IAAMC,UAAYD,UAAaC,KAC7B9M,MAAM+M,QAAQF,IAAM7M,MAAM+M,QAAQD,GACrCD,EAAE3N,SAAW4N,EAAE5N,QAAU2N,EAAEG,MAAM,CAAC1M,EAAGnB,IAAMyN,EAAOtM,EAAGwM,EAAE3N,KAC1C,iBAAN0N,GAAkBI,OAAOC,KAAKL,GAAG3N,SAAW+N,OAAOC,KAAKJ,GAAG5N,QAAU+N,OAAOC,KAAKL,GAAGG,MAAMG,GAAKP,EAAOC,EAAEM,GAAIL,EAAEK,MAC3H,CAmCOxH,eAAeyH,GAAmBC,IAAEA,EAAGxJ,WAAEA,EAAUyJ,YAAEA,EAAaC,MAAOC,IAE9E,MAAMD,EAAQC,GAAeC,WAAWF,MAQxC,IAAIxM,EANJ8C,UA5BK8B,eAAiC0H,EAAKC,EAAaE,GACxD,MAAMD,EAAQC,GAAeC,WAAWF,MACxC,aAAaA,EAAMF,EAAK,IAAKC,EAAaI,OAAQ,SAC/CC,KAAKC,IACJ,IAAKA,EAAIC,GAAI,MAAM,IAAInN,MAAM,qBAAqBkN,EAAIE,UACtD,MAAM5O,EAAS0O,EAAIG,QAAQC,IAAI,kBAC/B,IAAK9O,EAAQ,MAAM,IAAIwB,MAAM,0BAC7B,OAAOuN,SAAS/O,IAEtB,CAmBuBgP,CAAkBb,EAAKC,EAAaC,GAOzD,MAAMY,EAAOb,GAAe,CAAA,EAE5B,MAAO,CACLzJ,aACA,WAAMP,CAAM8K,EAAOC,GACjB,GAAItN,EACF,OAAOA,EAAO4M,KAAK5M,GAAUA,EAAOuC,MAAM8K,EAAOC,IAGnD,MAAMN,EAAU,IAAIO,QAAQH,EAAKJ,SAC3BQ,OAAiBlC,IAARgC,EAAoB,GAAKA,EAAM,EAC9CN,EAAQnH,IAAI,QAAS,SAASwH,KAASG,KAEvC,MAAMX,QAAYL,EAAMF,EAAK,IAAKc,EAAMJ,YACxC,IAAKH,EAAIC,KAAOD,EAAIY,KAAM,MAAM,IAAI9N,MAAM,gBAAgBkN,EAAIE,UAE9D,GAAmB,MAAfF,EAAIE,OAGN,OADA/M,EAAS6M,EAAI9G,cACN/F,EAAO4M,KAAK5M,GAAUA,EAAOuC,MAAM8K,EAAOC,IAC5C,GAAmB,MAAfT,EAAIE,OAEb,OAAOF,EAAI9G,cAEX,MAAM,IAAIpG,MAAM,yCAAyCkN,EAAIE,SAEjE,EAEJ,CAyEO,SAASW,EAAQC,GACtB,IAAKA,EAAQ,MAAO,GACpB,GAAsB,IAAlBA,EAAOxP,OAAc,OAAOwP,EAAO,GAEvC,MAAM3P,EAAS,GACf,IAAK,MAAM4P,KAASD,EAClBjC,EAAO1N,EAAQ4P,GAEjB,OAAO5P,CACT,CC3IO,SAAS6P,GAAetF,uBAAEA,EAAsBF,iBAAEA,EAAgBF,sBAAEA,IACzE,MAAM2F,EAAevF,GAA0BF,EAC/C,MAAO,CACL0F,UAAW5Q,OAAO2Q,GAClBE,QAAS7Q,OAAO2Q,EAAe3F,GAEnC,CC/DO,SAAS8F,EAAcjQ,EAAQkQ,EAAkBC,EAAkBhK,EAAQhC,GAChF,MAAMiM,EAAIF,GAAkB/P,QAAUgQ,EAAiBhQ,OACvD,IAAKiQ,EAAG,OAAOjK,EACf,MAAMkK,EAAqB/L,EAAsBH,GAC3CmM,EAAiBnM,EAAW7C,IAAI,EAAGhB,aAAcA,EAAQ+D,iBAC/D,IAAIkM,EAAa,EAGjB,MAAMC,EAAiB,CAACxQ,GACxB,IAAIyQ,EAAmBzQ,EACnB0Q,EAAe,EACfC,EAAkB,EAClBC,EAAkB,EAEtB,GAAIT,EAAiB,GAEnB,KAAOO,EAAeJ,EAAenQ,OAAS,GAAKyQ,EAAkBT,EAAiB,IACpFO,IACqC,aAAjCJ,EAAeI,KAEjBD,EAAmBA,EAAiBI,IAAG,GACvCL,EAAe3M,KAAK4M,GACpBE,KAEmC,aAAjCL,EAAeI,IAA8BE,IAIrD,IAAK,IAAIxQ,EAAI,EAAGA,EAAIgQ,EAAGhQ,IAAK,CAE1B,MAAM0Q,EAAMZ,GAAkB/P,OAAS+P,EAAiB9P,GAAKiQ,EACvDU,EAAMZ,EAAiB/P,GAG7B,KAAOsQ,IAAiBK,EAAMH,GAAoD,aAAjCN,EAAeI,KACzB,aAAjCJ,EAAeI,KACjBF,EAAeQ,MACfL,KAEmC,aAAjCL,EAAeI,IAA8BE,IACjDF,IAMF,IAHAD,EAAmBD,EAAeK,IAAG,IAIlCH,EAAeJ,EAAenQ,OAAS,GAA0C,aAArCmQ,EAAeI,EAAe,MAC1EC,EAAkBG,GAA4C,aAArCR,EAAeI,EAAe,KACxD,CAEA,GADAA,IACqC,aAAjCJ,EAAeI,GAA8B,CAE/C,MAAMO,EAAU,GAChBR,EAAiB5M,KAAKoN,GACtBR,EAAmBQ,EACnBT,EAAe3M,KAAKoN,GACpBN,GACF,CACqC,aAAjCL,EAAeI,IAA8BE,GACnD,CAGIE,IAAQT,EAEVI,EAAiB5M,KAAKsC,EAAOoK,MACpBG,IAAiBJ,EAAenQ,OAAS,EAClDsQ,EAAiB5M,KAAK,MAEtB4M,EAAiB5M,KAAK,GAE1B,CAGA,IAAK7D,EAAOG,OAEV,IAAK,IAAIC,EAAI,EAAGA,EAAIiQ,EAAoBjQ,IAAK,CAE3C,MAAM6Q,EAAU,GAChBR,EAAiB5M,KAAKoN,GACtBR,EAAmBQ,CACrB,CAGF,OAAOjR,CACT,CAUO,SAASkR,EAAeC,EAAe/N,EAAQgO,EAAQ,GAC5D,MAAM9N,EAAOF,EAAOE,KAAK+N,KAAK,KACxBC,EAA8C,aAAnClO,EAAO9C,QAAQ+D,gBAC1BkN,EAAYD,EAAWF,EAAQ,EAAIA,EAEzC,GL7BK,SAAoBhO,GACzB,IAAKA,EAAQ,OAAO,EACpB,GAAsC,SAAlCA,EAAO9C,QAAQI,eAA2B,OAAO,EACrD,GAAI0C,EAAOG,SAASpD,OAAS,EAAG,OAAO,EAEvC,MAAMqR,EAAapO,EAAOG,SAAS,GACnC,QAAIiO,EAAWjO,SAASpD,OAAS,IACU,aAAvCqR,EAAWlR,QAAQ+D,eAGzB,CKmBMoN,CAAWrO,GAAS,CACtB,IAAIsO,EAAUtO,EAAOG,SAAS,GAC1BoO,EAAWJ,EACiB,IAA5BG,EAAQnO,SAASpD,SACnBuR,EAAUA,EAAQnO,SAAS,GAC3BoO,KAEFT,EAAeC,EAAeO,EAASC,GAEvC,MAAMC,EAAYF,EAAQpO,KAAK+N,KAAK,KAC9BlL,EAASgL,EAAclC,IAAI2C,GACjC,IAAKzL,EAAQ,MAAM,IAAIxE,MAAM,sCAI7B,OAHI2P,GAAUO,EAAe1L,EAAQiL,GACrCD,EAActJ,IAAIvE,EAAM6C,QACxBgL,EAAcW,OAAOF,EAEvB,CAEA,GL7BK,SAAmBxO,GACxB,IAAKA,EAAQ,OAAO,EACpB,GAAsC,QAAlCA,EAAO9C,QAAQI,eAA0B,OAAO,EACpD,GAAI0C,EAAOG,SAASpD,OAAS,EAAG,OAAO,EAEvC,MAAMqR,EAAapO,EAAOG,SAAS,GACnC,GAAmC,IAA/BiO,EAAWjO,SAASpD,OAAc,OAAO,EAC7C,GAA2C,aAAvCqR,EAAWlR,QAAQ+D,gBAAgC,OAAO,EAE9D,MAAM0N,EAAWP,EAAWjO,SAASU,KAAKN,GAAgC,QAAvBA,EAAMrD,QAAQsD,MACjE,GAA0C,aAAtCmO,GAAUzR,QAAQ+D,gBAAgC,OAAO,EAE7D,MAAM2N,EAAaR,EAAWjO,SAASU,KAAKN,GAAgC,UAAvBA,EAAMrD,QAAQsD,MACnE,MAA4C,aAAxCoO,GAAY1R,QAAQ+D,eAG1B,CKaM4N,CAAU7O,GAAS,CACrB,MAAM8O,EAAU9O,EAAOG,SAAS,GAAGjD,QAAQsD,KAG3CsN,EAAeC,EAAe/N,EAAOG,SAAS,GAAGA,SAAS,GAAIgO,EAAY,GAC1EL,EAAeC,EAAe/N,EAAOG,SAAS,GAAGA,SAAS,GAAIgO,EAAY,GAE1E,MAAMpD,EAAOgD,EAAclC,IAAI,GAAG3L,KAAQ4O,SACpC/L,EAASgL,EAAclC,IAAI,GAAG3L,KAAQ4O,WAE5C,IAAK/D,EAAM,MAAM,IAAIxM,MAAM,mCAC3B,IAAKwE,EAAQ,MAAM,IAAIxE,MAAM,qCAC7B,GAAIwM,EAAKhO,SAAWgG,EAAOhG,OACzB,MAAM,IAAIwB,MAAM,gDAGlB,MAAMwQ,EAAMC,EAAajE,EAAMhI,EAAQoL,GAMvC,OALID,GAAUO,EAAeM,EAAKf,GAElCD,EAAcW,OAAO,GAAGxO,KAAQ4O,SAChCf,EAAcW,OAAO,GAAGxO,KAAQ4O,gBAChCf,EAActJ,IAAIvE,EAAM6O,EAE1B,CAGA,GAAI/O,EAAOG,SAASpD,OAAQ,CAE1B,MAAMkS,EAAiD,aAAnCjP,EAAO9C,QAAQ+D,gBAAiC+M,EAAQA,EAAQ,EAE9EkB,EAAS,CAAA,EACf,IAAK,MAAM3O,KAASP,EAAOG,SAAU,CACnC2N,EAAeC,EAAexN,EAAO0O,GACrC,MAAME,EAAYpB,EAAclC,IAAItL,EAAML,KAAK+N,KAAK,MACpD,IAAKkB,EAAW,MAAM,IAAI5Q,MAAM,qCAChC2Q,EAAO3O,EAAMrD,QAAQsD,MAAQ2O,CAC/B,CAEA,IAAK,MAAM5O,KAASP,EAAOG,SACzB4N,EAAcW,OAAOnO,EAAML,KAAK+N,KAAK,MAGvC,MAAMmB,EAAWC,EAAaH,EAAQD,GAClCf,GAAUO,EAAeW,EAAUpB,GACvCD,EAActJ,IAAIvE,EAAMkP,EAC1B,CACF,CAOA,SAASX,EAAe7Q,EAAKoQ,GAC3B,IAAK,IAAIhR,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC1BgR,EACFS,EAAe7Q,EAAIZ,GAAIgR,EAAQ,GAE/BpQ,EAAIZ,GAAKY,EAAIZ,GAAG,EAGtB,CAQA,SAASgS,EAAajE,EAAMhI,EAAQiL,GAClC,MAAMe,EAAM,GACZ,IAAK,IAAI/R,EAAI,EAAGA,EAAI+N,EAAKhO,OAAQC,IAC/B,GAAIgR,EACFe,EAAItO,KAAKuO,EAAajE,EAAK/N,GAAI+F,EAAO/F,GAAIgR,EAAQ,SAElD,GAAIjD,EAAK/N,GAAI,CAEX,MAAMsS,EAAM,CAAA,EACZ,IAAK,IAAIC,EAAI,EAAGA,EAAIxE,EAAK/N,GAAGD,OAAQwS,IAAK,CACvC,MAAMjQ,EAAQyD,EAAO/F,GAAGuS,GACxBD,EAAIvE,EAAK/N,GAAGuS,SAAgBrF,IAAV5K,EAAsB,KAAOA,CACjD,CACAyP,EAAItO,KAAK6O,EACX,MACEP,EAAItO,UAAKyJ,GAIf,OAAO6E,CACT,CASA,SAASM,EAAaH,EAAQlB,GAC5B,MAAMjD,EAAOD,OAAOC,KAAKmE,GACnBnS,EAASmS,EAAOnE,EAAK,KAAKhO,OAC1BgS,EAAM,GACZ,IAAK,IAAI/R,EAAI,EAAGA,EAAID,EAAQC,IAAK,CAE/B,MAAMsS,EAAM,CAAA,EACZ,IAAK,MAAMnG,KAAO4B,EAAM,CACtB,GAAImE,EAAO/F,GAAKpM,SAAWA,EAAQ,MAAM,IAAIwB,MAAM,gCACnD+Q,EAAInG,GAAO+F,EAAO/F,GAAKnM,EACzB,CACIgR,EACFe,EAAItO,KAAK4O,EAAaC,EAAKtB,EAAQ,IAEnCe,EAAItO,KAAK6O,EAEb,CACA,OAAOP,CACT,CC/OO,SAASS,EAAkBlO,EAAQlB,EAAOxD,GAC/C,MAAM6S,EAAQ7S,aAAkBmC,WAC1B2Q,EAAYzN,EAAWX,GACvBqO,EAAoB1N,EAAWX,GACrCW,EAAWX,GACX,IAAIhC,EAAQ6C,EAAiBb,GACzBsO,EAAc,EAClBhT,EAAOgT,KAAiBH,EAAQ1T,OAAOuD,GAASA,EAEhD,MAAMuQ,EAAqBH,EAAYC,EAEvC,KAAOC,EAAcxP,GAAO,CAE1B,MAAM0P,EAAW3N,EAAiBb,GAC5ByO,EAAY,IAAIlT,WAAW8S,GACjC,IAAK,IAAI3S,EAAI,EAAGA,EAAI2S,EAAmB3S,IACrC+S,EAAU/S,GAAKsE,EAAOG,KAAKiB,SAASpB,EAAOE,UAG7C,IAAK,IAAIxE,EAAI,EAAGA,EAAI2S,GAAqBC,EAAcxP,EAAOpD,IAAK,CAEjE,MAAMwB,EAAWM,OAAOiR,EAAU/S,IAClC,GAAIwB,EAAU,CACZ,IAAIwR,EAAa,GACbC,EAAiBJ,EACrB,MAAMK,GAAQ,IAAM1R,GAAY,GAChC,KAAOyR,GAAkBL,EAAcxP,GAAO,CAC5C,IAAIZ,EAAOV,OAAOwC,EAAOG,KAAKiB,SAASpB,EAAOE,UAAYwO,EAAaE,EAEvE,IADAF,GAAcxR,EACPwR,GAAc,GACnBA,GAAc,GACd1O,EAAOE,SACHwO,IACFxQ,GAAQV,OAAOwC,EAAOG,KAAKiB,SAASpB,EAAOE,UAAYhD,EAAWwR,EAAaE,GAInF5Q,GADcwQ,EAAWtQ,EAEzB5C,EAAOgT,KAAiBH,EAAQ1T,OAAOuD,GAASA,EAChD2Q,GACF,CACIA,IAEF3O,EAAOE,QAAUqC,KAAKsM,MAAMF,EAAiBlU,OAAOyC,GAAYzC,OAAOiU,IAAe,GAE1F,MACE,IAAK,IAAIT,EAAI,EAAGA,EAAIM,GAAsBD,EAAcxP,EAAOmP,IAC7DjQ,GAASwQ,EACTlT,EAAOgT,KAAiBH,EAAQ1T,OAAOuD,GAASA,CAGtD,CACF,CACF,CAOO,SAAS8Q,EAAqB9O,EAAQlB,EAAOxD,GAClD,MAAMyT,EAAU,IAAItR,WAAWqB,GAC/BoP,EAAkBlO,EAAQlB,EAAOiQ,GACjC,IAAK,IAAIrT,EAAI,EAAGA,EAAIoD,EAAOpD,IACzBJ,EAAOI,GAAK,IAAIH,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQ6O,EAAQrT,IAC/FsE,EAAOE,QAAU6O,EAAQrT,EAE7B,CCnEO,SAASwB,EAASc,GACvB,OAAO,GAAKuE,KAAKyM,MAAMhR,EACzB,CAYO,SAASiR,EAAuBjP,EAAQkP,EAAO5T,EAAQG,QAC7CmN,IAAXnN,IACFA,EAASuE,EAAOG,KAAKyC,UAAU5C,EAAOE,QAAQ,GAC9CF,EAAOE,QAAU,GAEnB,MAAMiP,EAAcnP,EAAOE,OAC3B,IAAIkP,EAAO,EACX,KAAOA,EAAO9T,EAAOG,QAAQ,CAC3B,MAAM4T,EAAS1O,EAAWX,GAC1B,GAAa,EAATqP,EAEFD,EAAOE,GAActP,EAAQqP,EAAQH,EAAO5T,EAAQ8T,OAC/C,CAEL,MAAMtQ,EAAQuQ,IAAW,EACzBE,GAAQvP,EAAQlB,EAAOoQ,EAAO5T,EAAQ8T,GACtCA,GAAQtQ,CACV,CACF,CACAkB,EAAOE,OAASiP,EAAc1T,CAChC,CAWA,SAAS8T,GAAQvP,EAAQlB,EAAO5B,EAAU5B,EAAQ8T,GAChD,MAAMF,EAAQhS,EAAW,GAAK,EAC9B,IAAIc,EAAQ,EACZ,IAAK,IAAItC,EAAI,EAAGA,EAAIwT,EAAOxT,IACzBsC,GAASgC,EAAOG,KAAKiB,SAASpB,EAAOE,YAAcxE,GAAK,GAK1D,IAAK,IAAIA,EAAI,EAAGA,EAAIoD,EAAOpD,IACzBJ,EAAO8T,EAAO1T,GAAKsC,CAEvB,CAaA,SAASsR,GAActP,EAAQqP,EAAQnS,EAAU5B,EAAQ8T,GACvD,IAAItQ,EAAQuQ,GAAU,GAAK,EAC3B,MAAMT,GAAQ,GAAK1R,GAAY,EAE/B,IAAIjC,EAAO,EACX,GAAI+E,EAAOE,OAASF,EAAOG,KAAKC,WAC9BnF,EAAO+E,EAAOG,KAAKiB,SAASpB,EAAOE,eAC9B,GAAI0O,EAET,MAAM,IAAI3R,MAAM,0BAA0B+C,EAAOE,uBAEnD,IAAIsP,EAAO,EACPC,EAAQ,EAGZ,KAAO3Q,GAED2Q,EAAQ,GACVA,GAAS,EACTD,GAAQ,EACRvU,KAAU,GACDuU,EAAOC,EAAQvS,GAExBjC,GAAQ+E,EAAOG,KAAKiB,SAASpB,EAAOE,SAAWsP,EAC/CxP,EAAOE,SACPsP,GAAQ,IAEJJ,EAAO9T,EAAOG,SAEhBH,EAAO8T,KAAUnU,GAAQwU,EAAQb,GAEnC9P,IACA2Q,GAASvS,GAIb,OAAOkS,CACT,CASO,SAASM,GAAgB1P,EAAQlB,EAAO/C,EAAM4T,GACnD,MAAMT,EA6BR,SAAmBnT,EAAM4T,GACvB,OAAQ5T,GACR,IAAK,QACL,IAAK,QACH,OAAO,EACT,IAAK,QACL,IAAK,SACH,OAAO,EACT,IAAK,uBACH,IAAK4T,EAAY,MAAM,IAAI1S,MAAM,yCACjC,OAAO0S,EACT,QACE,MAAM,IAAI1S,MAAM,6BAA6BlB,KAEjD,CA3CgB6T,CAAU7T,EAAM4T,GACxB5R,EAAQ,IAAIxC,WAAWuD,EAAQoQ,GACrC,IAAK,IAAI7F,EAAI,EAAGA,EAAI6F,EAAO7F,IACzB,IAAK,IAAI3N,EAAI,EAAGA,EAAIoD,EAAOpD,IACzBqC,EAAMrC,EAAIwT,EAAQ7F,GAAKrJ,EAAOG,KAAKiB,SAASpB,EAAOE,UAIvD,GAAa,UAATnE,EAAkB,OAAO,IAAI8T,aAAa9R,EAAMT,QAC/C,GAAa,WAATvB,EAAmB,OAAO,IAAI+T,aAAa/R,EAAMT,QACrD,GAAa,UAATvB,EAAkB,OAAO,IAAI0B,WAAWM,EAAMT,QAClD,GAAa,UAATvB,EAAkB,OAAO,IAAIqB,cAAcW,EAAMT,QACrD,GAAa,yBAATvB,EAAiC,CAExC,MAAMgU,EAAQ,IAAIxT,MAAMuC,GACxB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IACzBqU,EAAMrU,GAAKqC,EAAMiS,SAAStU,EAAIwT,GAAQxT,EAAI,GAAKwT,GAEjD,OAAOa,CACT,CACA,MAAM,IAAI9S,MAAM,+CAA+ClB,IACjE,CCzIO,SAASkU,GAAUjQ,EAAQjE,EAAM+C,EAAOoR,GAC7C,GAAc,IAAVpR,EAAa,MAAO,GACxB,GAAa,YAAT/C,EACF,OA4BJ,SAA0BiE,EAAQlB,GAChC,MAAM2C,EAAS,IAAIlF,MAAMuC,GACzB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IAAK,CAC9B,MAAM6B,EAAayC,EAAOE,QAAUxE,EAAI,EAAI,GACtCyU,EAAYzU,EAAI,EAChBuC,EAAO+B,EAAOG,KAAKiB,SAAS7D,GAClCkE,EAAO/F,MAAMuC,EAAO,GAAKkS,EAC3B,CAEA,OADAnQ,EAAOE,QAAUqC,KAAKsM,KAAK/P,EAAQ,GAC5B2C,CACT,CAtCW2O,CAAiBpQ,EAAQlB,GAC3B,GAAa,UAAT/C,EACT,OA6CJ,SAAwBiE,EAAQlB,GAC9B,MAAM2C,GAAUzB,EAAOG,KAAK5C,WAAayC,EAAOE,QAAU,EACtD,IAAIzC,WAAW4S,GAAMrQ,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAgB,EAARpB,IACjF,IAAIrB,WAAWuC,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQpB,GAE/E,OADAkB,EAAOE,QAAkB,EAARpB,EACV2C,CACT,CAnDW6O,CAAetQ,EAAQlB,GACzB,GAAa,UAAT/C,EACT,OA0DJ,SAAwBiE,EAAQlB,GAC9B,MAAM2C,GAAUzB,EAAOG,KAAK5C,WAAayC,EAAOE,QAAU,EACtD,IAAI9C,cAAciT,GAAMrQ,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAgB,EAARpB,IACpF,IAAI1B,cAAc4C,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQpB,GAElF,OADAkB,EAAOE,QAAkB,EAARpB,EACV2C,CACT,CAhEW8O,CAAevQ,EAAQlB,GACzB,GAAa,UAAT/C,EACT,OAuEJ,SAAwBiE,EAAQlB,GAC9B,MAAM2C,EAAS,IAAIlF,MAAMuC,GACzB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IAAK,CAC9B,MAAM8U,EAAMxQ,EAAOG,KAAK4I,YAAY/I,EAAOE,OAAa,GAAJxE,GAAQ,GACtD+U,EAAOzQ,EAAOG,KAAK2I,SAAS9I,EAAOE,OAAa,GAAJxE,EAAS,GAAG,GAC9D+F,EAAO/F,GAAK8B,OAAOiT,IAAS,IAAMD,CACpC,CAEA,OADAxQ,EAAOE,QAAkB,GAARpB,EACV2C,CACT,CAhFWiP,CAAe1Q,EAAQlB,GACzB,GAAa,UAAT/C,EACT,OAuFJ,SAAwBiE,EAAQlB,GAC9B,MAAM2C,GAAUzB,EAAOG,KAAK5C,WAAayC,EAAOE,QAAU,EACtD,IAAI2P,aAAaQ,GAAMrQ,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAgB,EAARpB,IACnF,IAAI+Q,aAAa7P,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQpB,GAEjF,OADAkB,EAAOE,QAAkB,EAARpB,EACV2C,CACT,CA7FWkP,CAAe3Q,EAAQlB,GACzB,GAAa,WAAT/C,EACT,OAoGJ,SAAyBiE,EAAQlB,GAC/B,MAAM2C,GAAUzB,EAAOG,KAAK5C,WAAayC,EAAOE,QAAU,EACtD,IAAI4P,aAAaO,GAAMrQ,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAgB,EAARpB,IACnF,IAAIgR,aAAa9P,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQpB,GAEjF,OADAkB,EAAOE,QAAkB,EAARpB,EACV2C,CACT,CA1GWmP,CAAgB5Q,EAAQlB,GAC1B,GAAa,eAAT/C,EACT,OAiHJ,SAA4BiE,EAAQlB,GAClC,MAAM2C,EAAS,IAAIlF,MAAMuC,GACzB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IAAK,CAC9B,MAAMD,EAASuE,EAAOG,KAAKyC,UAAU5C,EAAOE,QAAQ,GACpDF,EAAOE,QAAU,EACjBuB,EAAO/F,GAAK,IAAIH,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQzE,GACvFuE,EAAOE,QAAUzE,CACnB,CACA,OAAOgG,CACT,CA1HWoP,CAAmB7Q,EAAQlB,GAC7B,GAAa,yBAAT/C,EAAiC,CAC1C,IAAKmU,EAAa,MAAM,IAAIjT,MAAM,gCAClC,OAiIJ,SAAiC+C,EAAQlB,EAAOoR,GAE9C,MAAMzO,EAAS,IAAIlF,MAAMuC,GACzB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IACzB+F,EAAO/F,GAAK,IAAIH,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQgQ,GACvFlQ,EAAOE,QAAUgQ,EAEnB,OAAOzO,CACT,CAzIWqP,CAAwB9Q,EAAQlB,EAAOoR,EAChD,CACE,MAAM,IAAIjT,MAAM,2BAA2BlB,IAE/C,CAgJA,SAASsU,GAAM/S,EAAQ4C,EAAQmB,GAC7B,MAAM0P,EAAU,IAAI9N,YAAY5B,GAEhC,OADA,IAAI9F,WAAWwV,GAAS5N,IAAI,IAAI5H,WAAW+B,EAAQ4C,EAAQmB,IACpD0P,CACT,CC7KA,MAAMC,GAAY,CAAC,EAAG,IAAM,MAAQ,SAAU,YAW9C,SAASC,GAAUC,EAAWC,EAASC,EAASC,EAAO5V,GACrD,IAAK,IAAIC,EAAI,EAAGA,EAAID,EAAQC,IAC1B0V,EAAQC,EAAQ3V,GAAKwV,EAAUC,EAAUzV,EAE7C,CCPO,SAAS4V,GAAavT,EAAOwT,GAAMxV,KAAEA,EAAIH,QAAEA,EAAO6D,WAAEA,IACzD,MAAMU,EAAO,IAAIwC,SAAS5E,EAAMT,OAAQS,EAAMR,WAAYQ,EAAMqC,YAC1DJ,EAAS,CAAEG,OAAMD,OAAQ,GAE/B,IAAIsR,EAGJ,MAAM/F,EAkDR,SAA8BzL,EAAQuR,EAAM9R,GAC1C,GAAIA,EAAWhE,OAAS,EAAG,CACzB,MAAMgW,EAAqBjS,EAAsBC,GACjD,GAAIgS,EAAoB,CACtB,MAAMhQ,EAAS,IAAIlF,MAAMgV,EAAKhM,YAE9B,OADA0J,EAAuBjP,EAAQ9C,EAASuU,GAAqBhQ,GACtDA,CACT,CACF,CACA,MAAO,EACT,CA5D2BiQ,CAAqB1R,EAAQuR,EAAM9R,IAEtD+L,iBAAEA,EAAgBmG,SAAEA,GAkE5B,SAA8B3R,EAAQuR,EAAM9R,GAC1C,MAAMkM,EAAqB/L,EAAsBH,GACjD,IAAKkM,EAAoB,MAAO,CAAEH,iBAAkB,GAAImG,SAAU,GAElE,MAAMnG,EAAmB,IAAIjP,MAAMgV,EAAKhM,YACxC0J,EAAuBjP,EAAQ9C,EAASyO,GAAqBH,GAG7D,IAAImG,EAAWJ,EAAKhM,WACpB,IAAK,MAAM6G,KAAOZ,EACZY,IAAQT,GAAoBgG,IAEjB,IAAbA,IAAgBnG,EAAiB/P,OAAS,GAE9C,MAAO,CAAE+P,mBAAkBmG,WAC7B,CAjFyCC,CAAqB5R,EAAQuR,EAAM9R,GAIpEoS,EAAUN,EAAKhM,WAAaoM,EAClC,GAAsB,UAAlBJ,EAAKpW,SACPqW,EAAWvB,GAAUjQ,EAAQjE,EAAM8V,EAASjW,EAAQgI,kBAC/C,GACa,qBAAlB2N,EAAKpW,UACa,mBAAlBoW,EAAKpW,UACa,QAAlBoW,EAAKpW,SACL,CACA,MAAM+B,EAAoB,YAATnB,EAAqB,EAAIoE,EAAKiB,SAASpB,EAAOE,UAC3DhD,GACFsU,EAAW,IAAIjV,MAAMsV,GACR,YAAT9V,GACFkT,EAAuBjP,EAAQ9C,EAAUsU,GACzCA,EAAWA,EAAS5U,IAAIkV,KAAOA,IAG/B7C,EAAuBjP,EAAQ9C,EAAUsU,EAAUrR,EAAKC,WAAaJ,EAAOE,SAG9EsR,EAAW,IAAIjW,WAAWsW,EAE9B,MAAO,GAAsB,sBAAlBN,EAAKpW,SACdqW,EAAW9B,GAAgB1P,EAAQ6R,EAAS9V,EAAMH,EAAQgI,kBACrD,GAAsB,wBAAlB2N,EAAKpW,SAAoC,CAElDqW,EADuB,UAATzV,EACK,IAAI0B,WAAWoU,GAAW,IAAIzU,cAAcyU,GAC/D3D,EAAkBlO,EAAQ6R,EAASL,EACrC,KAAO,IAAsB,4BAAlBD,EAAKpW,SAId,MAAM,IAAI8B,MAAM,iCAAiCsU,EAAKpW,YAHtDqW,EAAW,IAAIjV,MAAMsV,GACrB/C,EAAqB9O,EAAQ6R,EAASL,EAGxC,CAEA,MAAO,CAAEhG,mBAAkBC,mBAAkB+F,WAC/C,CAmDO,SAASO,GAAeC,EAAiBC,EAAwB3M,EAAO4M,GAE7E,IAAIC,EACJ,MAAMC,EAAqBF,IAAc5M,GACzC,GAAc,iBAAVA,EACF6M,EAAOH,OACF,GAAII,EACTD,EAAOC,EAAmBJ,EAAiBC,OACtC,IAAc,WAAV3M,EAIT,MAAM,IAAIrI,MAAM,0CAA0CqI,KAH1D6M,EAAO,IAAI5W,WAAW0W,GD5FnB,SAA0BI,EAAO/W,GACtC,MAAMgX,EAAcD,EAAMjS,WACpBmS,EAAejX,EAAO8E,WAC5B,IAAIoS,EAAM,EACNC,EAAS,EAGb,KAAOD,EAAMF,GAAa,CACxB,MAAMI,EAAIL,EAAMG,GAEhB,GADAA,IACIE,EAAI,IACN,KAEJ,CACA,GAAIH,GAAgBC,GAAOF,EACzB,MAAM,IAAIrV,MAAM,gCAGlB,KAAOuV,EAAMF,GAAa,CACxB,MAAMI,EAAIL,EAAMG,GAChB,IAAIG,EAAM,EAGV,GAFAH,IAEIA,GAAOF,EACT,MAAM,IAAIrV,MAAM,sBAIlB,GAAS,EAAJyV,EAsBE,CAEL,IAAIxS,EAAS,EACb,OAAY,EAAJwS,GACR,KAAK,EAEHC,EAAwB,GAAjBD,IAAM,EAAI,GACjBxS,EAASmS,EAAMG,IAAQE,IAAM,GAAK,GAClCF,IACA,MACF,KAAK,EAEH,GAAIF,GAAeE,EAAM,EACvB,MAAM,IAAIvV,MAAM,6BAElB0V,GAAOD,IAAM,GAAK,EAClBxS,EAASmS,EAAMG,IAAQH,EAAMG,EAAM,IAAM,GACzCA,GAAO,EACP,MACF,KAAK,EAEH,GAAIF,GAAeE,EAAM,EACvB,MAAM,IAAIvV,MAAM,6BAElB0V,GAAOD,IAAM,GAAK,EAClBxS,EAASmS,EAAMG,IACVH,EAAMG,EAAM,IAAM,IAClBH,EAAMG,EAAM,IAAM,KAClBH,EAAMG,EAAM,IAAM,IACvBA,GAAO,EAKT,GAAe,IAAXtS,GAAgB0S,MAAM1S,GACxB,MAAM,IAAIjD,MAAM,kBAAkBiD,SAAcsS,iBAAmBF,KAErE,GAAIpS,EAASuS,EACX,MAAM,IAAIxV,MAAM,2CAElBgU,GAAU3V,EAAQmX,EAASvS,EAAQ5E,EAAQmX,EAAQE,GACnDF,GAAUE,CACZ,KAhEqB,CAEnB,IAAIA,GAAOD,IAAM,GAAK,EAEtB,GAAIC,EAAM,GAAI,CACZ,GAAIH,EAAM,GAAKF,EACb,MAAM,IAAIrV,MAAM,+CAElB,MAAM4V,EAAaF,EAAM,GACzBA,EAAMN,EAAMG,IACPH,EAAMG,EAAM,IAAM,IAClBH,EAAMG,EAAM,IAAM,KAClBH,EAAMG,EAAM,IAAM,IACvBG,EAAsC,GAA/BA,EAAM3B,GAAU6B,IACvBL,GAAOK,CACT,CACA,GAAIL,EAAMG,EAAML,EACd,MAAM,IAAIrV,MAAM,6CAElBgU,GAAUoB,EAAOG,EAAKlX,EAAQmX,EAAQE,GACtCH,GAAOG,EACPF,GAAUE,CACZ,CA2CF,CAEA,GAAIF,IAAWF,EAAc,MAAM,IAAItV,MAAM,yBAC/C,CCHI6V,CAAiBd,EAAiBG,EAGpC,CACA,GAAIA,GAAM1W,SAAWwW,EACnB,MAAM,IAAIhV,MAAM,oCAAoCkV,GAAM1W,gCAAgCwW,KAE5F,OAAOE,CACT,CAWO,SAASY,GAAef,EAAiBgB,EAAI5X,GAClD,MACM4E,EAAS,CAAEG,KADJ,IAAIwC,SAASqP,EAAgB1U,OAAQ0U,EAAgBzU,WAAYyU,EAAgB5R,YACvEF,OAAQ,IACzBnE,KAAEA,EAAIH,QAAEA,EAAO6D,WAAEA,EAAU6F,MAAEA,EAAK4M,YAAEA,GAAgB9W,EACpD6X,EAAQD,EAAGE,oBACjB,IAAKD,EAAO,MAAM,IAAIhW,MAAM,4CAG5B,MAAMwO,EA2DR,SAAgCzL,EAAQiT,EAAOxT,GAC7C,MAAMgS,EAAqBjS,EAAsBC,GACjD,IAAKgS,EAAoB,MAAO,GAEhC,MAAMhQ,EAAS,IAAIlF,MAAM0W,EAAM1N,YAE/B,OADA0J,EAAuBjP,EAAQ9C,EAASuU,GAAqBhQ,EAAQwR,EAAME,+BACpE1R,CACT,CAlE2B2R,CAAuBpT,EAAQiT,EAAOxT,GAC/DO,EAAOE,OAAS+S,EAAME,8BAGtB,MAAM3H,EAsER,SAAgCxL,EAAQiT,EAAOxT,GAC7C,MAAMkM,EAAqB/L,EAAsBH,GACjD,GAAIkM,EAAoB,CAEtB,MAAMlK,EAAS,IAAIlF,MAAM0W,EAAM1N,YAE/B,OADA0J,EAAuBjP,EAAQ9C,EAASyO,GAAqBlK,EAAQwR,EAAMI,+BACpE5R,CACT,CACF,CA9E2B6R,CAAuBtT,EAAQiT,EAAOxT,GAGzD8T,EAAuBP,EAAGf,uBAAyBgB,EAAMI,8BAAgCJ,EAAME,8BAErG,IAAIhB,EAAOH,EAAgBhC,SAAShQ,EAAOE,SACf,IAAxB+S,EAAMO,gBACRrB,EAAOJ,GAAeI,EAAMoB,EAAsBjO,EAAO4M,IAE3D,MAAMuB,EAAW,IAAI9Q,SAASwP,EAAK7U,OAAQ6U,EAAK5U,WAAY4U,EAAK/R,YAC3DsT,EAAa,CAAEvT,KAAMsT,EAAUvT,OAAQ,GAI7C,IAAIsR,EACJ,MAAMK,EAAUoB,EAAM1N,WAAa0N,EAAMU,UACzC,GAAuB,UAAnBV,EAAM9X,SACRqW,EAAWvB,GAAUyD,EAAY3X,EAAM8V,EAASjW,EAAQgI,kBACnD,GAAuB,QAAnBqP,EAAM9X,SAEfqW,EAAW,IAAIjV,MAAMsV,GACrB5C,EAAuByE,EAAY,EAAGlC,GACtCA,EAAWA,EAAS5U,IAAIkV,KAAOA,QAC1B,GACc,qBAAnBmB,EAAM9X,UACa,mBAAnB8X,EAAM9X,SACN,CACA,MAAM+B,EAAWuW,EAASrS,SAASsS,EAAWxT,UAC9CsR,EAAW,IAAIjV,MAAMsV,GACrB5C,EAAuByE,EAAYxW,EAAUsU,EAAU+B,EAAuB,EAChF,MAAO,GAAuB,wBAAnBN,EAAM9X,SAAoC,CAEnDqW,EADuB,UAATzV,EACK,IAAI0B,WAAWoU,GAAW,IAAIzU,cAAcyU,GAC/D3D,EAAkBwF,EAAY7B,EAASL,EACzC,MAAO,GAAuB,4BAAnByB,EAAM9X,SACfqW,EAAW,IAAIjV,MAAMsV,GACrB/C,EAAqB4E,EAAY7B,EAASL,QACrC,GAAuB,qBAAnByB,EAAM9X,SACfqW,EAAW,IAAIjV,MAAMsV,GJ9GlB,SAAwB7R,EAAQlB,EAAOxD,GAC5C,MAAMsY,EAAa,IAAInW,WAAWqB,GAClCoP,EAAkBlO,EAAQlB,EAAO8U,GACjC,MAAMC,EAAa,IAAIpW,WAAWqB,GAClCoP,EAAkBlO,EAAQlB,EAAO+U,GAEjC,IAAK,IAAInY,EAAI,EAAGA,EAAIoD,EAAOpD,IAAK,CAC9B,MAAMoY,EAAS,IAAIvY,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQ2T,EAAWnY,IACjGkY,EAAWlY,IAEbJ,EAAOI,GAAK,IAAIH,WAAWqY,EAAWlY,GAAKmY,EAAWnY,IACtDJ,EAAOI,GAAGyH,IAAI7H,EAAOI,EAAI,GAAGsU,SAAS,EAAG4D,EAAWlY,KACnDJ,EAAOI,GAAGyH,IAAI2Q,EAAQF,EAAWlY,KAEjCJ,EAAOI,GAAKoY,EAEd9T,EAAOE,QAAU2T,EAAWnY,EAC9B,CACF,CI6FIqY,CAAeL,EAAY7B,EAASL,OAC/B,IAAuB,sBAAnByB,EAAM9X,SAGf,MAAM,IAAI8B,MAAM,iCAAiCgW,EAAM9X,YAFvDqW,EAAW9B,GAAgB1P,EAAQ6R,EAAS9V,EAAMH,EAAQgI,YAG5D,CAEA,MAAO,CAAE4H,mBAAkBC,mBAAkB+F,WAC/C,CCxLO,SAASwC,GAAWhU,GAAQiU,WAAEA,EAAUC,YAAEA,EAAWC,UAAEA,GAAa/Y,EAAegZ,GACxF,MAAMC,WAAEA,GAAejZ,EAEjB6P,EAAS,GAEf,IAAI/P,EAEAoZ,EACAC,EAAW,EAEf,MAAMC,EAAgBJ,SACpBE,GAAaF,EAAO,CAClBC,aACAI,WAAYH,EACZI,SAAUT,EAAaM,EAAWD,EAAU7Y,OAC5CkZ,OAAQV,EAAaM,GAExB,GAED,KAAOA,EAAWJ,KACZnU,EAAOE,QAAUF,EAAOG,KAAKC,WAAa,IADnB,CAI3B,MAAMiP,EAASuF,GAAc5U,GAC7B,GAAoB,oBAAhBqP,EAAOtT,KAETb,EAAa2Z,GAAS7U,EAAQqP,EAAQjU,EAAeF,OAAY0N,EAAW,GAC5E1N,EAAaS,EAAQT,EAAYE,OAC5B,CACL,MAAM0Z,EAAkBR,GAAW7Y,QAAU,EACvCgG,EAASoT,GAAS7U,EAAQqP,EAAQjU,EAAeF,EAAYoZ,EAAWJ,EAAcK,GACxFD,IAAc7S,EAEhB8S,GAAY9S,EAAOhG,OAASqZ,GAE5BN,MACAvJ,EAAO9L,KAAKsC,GACZ8S,GAAY9S,EAAOhG,OACnB6Y,EAAY7S,EAEhB,CACF,CAOA,OANA+S,MAEID,EAAWJ,GAAaG,IAE1BrJ,EAAOA,EAAOxP,OAAS,GAAK6Y,EAAUzU,MAAM,EAAGsU,GAAaI,EAAWD,EAAU7Y,UAE5EwP,CACT,CAaO,SAAS4J,GAAS7U,EAAQqP,EAAQjU,EAAeF,EAAY6Z,EAAeC,GACjF,MAAMjZ,KAAEA,EAAIH,QAAEA,EAAO6D,WAAEA,EAAU6F,MAAEA,EAAK4M,YAAEA,GAAgB9W,EAEpD4W,EAAkB,IAAIzW,WAC1ByE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQmP,EAAO4F,sBAKrE,GAHAjV,EAAOE,QAAUmP,EAAO4F,qBAGJ,cAAhB5F,EAAOtT,KAAsB,CAC/B,MAAMwV,EAAOlC,EAAO6F,iBACpB,IAAK3D,EAAM,MAAM,IAAItU,MAAM,yCAG3B,GAAI+X,EAAYzD,EAAKhM,YXiClB,SAAsB9F,GAC3B,GAA0B,IAAtBA,EAAWhE,OAAc,OAAO,EACpC,MAAM,CAAGsJ,GAAUtF,EACnB,MAAuC,aAAnCsF,EAAOnJ,QAAQ+D,kBACfoF,EAAOlG,SAASpD,MAEtB,CWvCuC0Z,CAAa1V,GAC9C,OAAO,IAAIlD,MAAMgV,EAAKhM,YAGxB,MAAM4M,EAAOJ,GAAeC,EAAiBvX,OAAO4U,EAAO4C,wBAAyB3M,EAAO4M,IACrF1G,iBAAEA,EAAgBC,iBAAEA,EAAgB+F,SAAEA,GAAaF,GAAaa,EAAMZ,EAAMnW,GAIlF,IAAIqG,EAASzG,EAAsBwW,EAAUtW,EAAYqW,EAAKpW,SAAUC,GACxE,GAAIqQ,EAAiBhQ,QAAU+P,GAAkB/P,OAAQ,CAEvD,OAAO8P,EADQhP,MAAM+M,QAAQyL,GAAiBA,EAAgB,GACjCvJ,EAAkBC,EAAkBhK,EAAQhC,EAC3E,CAEE,IAAK,IAAI/D,EAAI,EAAGA,EAAI+D,EAAWhE,OAAQC,IACS,aAA1C+D,EAAW/D,GAAGE,QAAQ+D,kBACxB8B,EAASlF,MAAMoB,KAAK8D,EAAQiD,GAAK,CAACA,KAGtC,OAAOjD,CAEX,CAAO,GAAoB,iBAAhB4N,EAAOtT,KAAyB,CACzC,MAAMkX,EAAQ5D,EAAO6D,oBACrB,IAAKD,EAAO,MAAM,IAAIhW,MAAM,4CAG5B,GAAI+X,EAAY/B,EAAMtO,SACpB,OAAO,IAAIpI,MAAM0W,EAAM1N,YAGzB,MAAMiG,iBAAEA,EAAgBC,iBAAEA,EAAgB+F,SAAEA,GAC1CuB,GAAef,EAAiB3C,EAAQjU,GAGpCqG,EAASzG,EAAsBwW,EAAUtW,EAAY+X,EAAM9X,SAAUC,GAE3E,OAAOmQ,EADQhP,MAAM+M,QAAQyL,GAAiBA,EAAgB,GACjCvJ,EAAkBC,EAAkBhK,EAAQhC,EAC3E,CAAO,GAAoB,oBAAhB4P,EAAOtT,KAA4B,CAC5C,MAAMqZ,EAAO/F,EAAOgG,uBACpB,IAAKD,EAAM,MAAM,IAAInY,MAAM,+CAE3B,MAAMkV,EAAOJ,GACXC,EAAiBvX,OAAO4U,EAAO4C,wBAAyB3M,EAAO4M,GAIjE,OAAOjC,GADQ,CAAE9P,KAAM,IAAIwC,SAASwP,EAAK7U,OAAQ6U,EAAK5U,WAAY4U,EAAK/R,YAAaF,OAAQ,GACnEnE,EAAMqZ,EAAK7P,WAAY3J,EAAQgI,YAC1D,CACE,MAAM,IAAI3G,MAAM,kCAAkCoS,EAAOtT,OAE7D,CASA,SAAS6Y,GAAc5U,GACrB,MAAMqP,EAAStP,EAA4BC,GAsC3C,MAAO,CACLjE,KApCW3B,EAASiV,EAAO5L,SAqC3BwO,uBApC6B5C,EAAO3L,QAqCpCuR,qBApC2B5F,EAAOxL,QAqClCyR,IApCUjG,EAAOvL,QAqCjBoR,iBApCuB7F,EAAOtL,SAAW,CACzCwB,WAAY8J,EAAOtL,QAAQN,QAC3BtI,SAAUnB,EAASqV,EAAOtL,QAAQL,SAClC6R,0BAA2Bvb,EAASqV,EAAOtL,QAAQF,SACnD2R,0BAA2Bxb,EAASqV,EAAOtL,QAAQD,SACnDiC,WAAYsJ,EAAOtL,QAAQA,SAAW,CACpCvB,IAAK6M,EAAOtL,QAAQA,QAAQN,QAC5B4E,IAAKgH,EAAOtL,QAAQA,QAAQL,QAC5B4E,WAAY+G,EAAOtL,QAAQA,QAAQF,QACnC0E,eAAgB8G,EAAOtL,QAAQA,QAAQD,QACvC0E,UAAW6G,EAAOtL,QAAQA,QAAQA,QAClC0E,UAAW4G,EAAOtL,QAAQA,QAAQC,UA0BpCyR,kBAvBwBpG,EAAOrL,QAwB/BqR,uBAvB6BhG,EAAOpL,SAAW,CAC/CsB,WAAY8J,EAAOpL,QAAQR,QAC3BtI,SAAUnB,EAASqV,EAAOpL,QAAQP,SAClCgS,UAAWrG,EAAOpL,QAAQJ,SAqB1BqP,oBAnB0B7D,EAAOlL,SAAW,CAC5CoB,WAAY8J,EAAOlL,QAAQV,QAC3BkQ,UAAWtE,EAAOlL,QAAQT,QAC1BiB,SAAU0K,EAAOlL,QAAQN,QACzB1I,SAAUnB,EAASqV,EAAOlL,QAAQL,SAClCuP,8BAA+BhE,EAAOlL,QAAQJ,QAC9CoP,8BAA+B9D,EAAOlL,QAAQH,QAC9CwP,mBAA0C5K,IAA3ByG,EAAOlL,QAAQF,SAA+BoL,EAAOlL,QAAQF,QAC5E8B,WAAYsJ,EAAOlL,QAAQA,SAa/B,CCvHOjC,eAAeyT,IAAiBC,aAAEA,GAAgB1B,EAAaC,EAAWrP,EAAS+Q,GACxF,MAAMC,EAAY,IAAIvZ,MAAM4X,GAItB4B,QAAoBC,QAAQC,IAAIL,EAAahZ,IAAI,EAAG3B,UAAWA,EAAKiP,KAAKc,KAGzEkL,EAAsBN,EACzBhZ,IAAIqC,GAASA,EAAMkX,aAAa,IAChC1R,OAAOvF,IAAS4F,GAAWA,EAAQsR,SAASlX,IACzCmX,EAAcvR,GAAWoR,EACzBI,EAAgBD,EAAYzZ,IAAIsC,GAAQ0W,EAAaW,UAAUxR,GAAUA,EAAOoR,aAAa,KAAOjX,IAG1G,IAAK,IAAIsX,EAAMtC,EAAasC,EAAMrC,EAAWqC,IAC3C,GAAkB,WAAdX,EAAwB,CAG1B,MAAMY,EAAU,CAAA,EAChB,IAAK,IAAI/a,EAAI,EAAGA,EAAIka,EAAana,OAAQC,IACvC+a,EAAQb,EAAala,GAAGya,aAAa,IAAMJ,EAAYra,GAAG8a,GAE5DV,EAAUU,GAAOC,CACnB,KAAO,CAEL,MAAMA,EAAU,IAAIla,MAAMqZ,EAAana,QACvC,IAAK,IAAIC,EAAI,EAAGA,EAAI2a,EAAY5a,OAAQC,IAClC4a,EAAc5a,IAAM,IACtB+a,EAAQ/a,GAAKqa,EAAYO,EAAc5a,IAAI8a,IAG/CV,EAAUU,GAAOC,CACnB,CAEF,OAAOX,CACT,CASO,SAASY,GAAcC,EAAelY,GAC3C,MAAMmX,aAAEA,GAAiBe,EAEnBC,EAAY,GAClB,IAAK,MAAM3X,KAASR,EAAWI,SAC7B,GAAII,EAAMJ,SAASpD,OAAQ,CACzB,MAAMob,EAAejB,EAAanR,OAAOM,GAAUA,EAAOoR,aAAa,KAAOlX,EAAMrD,QAAQsD,MAC5F,IAAK2X,EAAapb,OAAQ,SAI1B,MAAMqb,EAAW,IAAIC,IACf9b,EAAO+a,QAAQC,IAAIY,EAAaja,IAAImI,GACjCA,EAAO9J,KAAKiP,KAAKuK,IACtBqC,EAAS3T,IAAI4B,EAAOoR,aAAaxJ,KAAK,KAAM3B,EAAQyJ,QAEpDvK,KAAK,KAEPsC,EAAesK,EAAU7X,GACzB,MAAM+X,EAAaF,EAASvM,IAAItL,EAAML,KAAK+N,KAAK,MAChD,IAAKqK,EAAY,MAAM,IAAI/Z,MAAM,qCACjC,MAAO,CAAC+Z,KAGVJ,EAAUzX,KAAK,CAAEgX,aAAclX,EAAML,KAAM3D,QAC7C,KAAO,CAEL,MAAMgc,EAAcrB,EAAarW,KAAKwF,GAAUA,EAAOoR,aAAa,KAAOlX,EAAMrD,QAAQsD,MACrF+X,GACFL,EAAUzX,KAAK8X,EAEnB,CAEF,MAAO,IAAKN,EAAef,aAAcgB,EAC3C,CC5EO,SAASM,GAAiBC,GAC/B,IAAKA,EAAQ5T,SAAU,MAAM,IAAItG,MAAM,6BAIvC,MAAMma,ET5ED,UAAqB7T,SAAEA,EAAQmR,SAAEA,EAAW,EAACC,OAAEA,EAASnW,IAAQsG,QAAEA,IACvE,IAAKvB,EAAU,MAAM,IAAItG,MAAM,iCAE/B,MAAMoa,EAAS,GAETC,EAAU,GAGhB,IAAIrD,EAAa,EACjB,IAAK,MAAMpP,KAAYtB,EAASqB,WAAY,CAC1C,MAAM2S,EAAY9c,OAAOoK,EAASF,UAC5B6S,EAAWvD,EAAasD,EAE9B,GAAIA,EAAY,GAAKC,GAAY9C,GAAYT,EAAaU,EAAQ,CAEhE,MAAM8C,EAAS,GAEf,IAAK,MAAMxS,UAAEA,EAASE,UAAEA,KAAeN,EAASC,QAAS,CACvD,GAAIG,EAAW,MAAM,IAAIhI,MAAM,mCAC/B,IAAKkI,EAAW,MAAM,IAAIlI,MAAM,wCAE3B6H,IAAWA,EAAQsR,SAASjR,EAAUE,eAAe,KACxDoS,EAAOtY,KAAKgM,EAAehG,GAE/B,CACA,MAAM+O,EAAc3R,KAAKC,IAAIkS,EAAWT,EAAY,GAC9CE,EAAY5R,KAAK8F,IAAIsM,EAASV,EAAYsD,GAChDF,EAAOlY,KAAK,CAAEsY,SAAQ5S,WAAUoP,aAAYsD,YAAWrD,cAAaC,cAGpE,MAAMuD,EAAYD,EAAOA,EAAOhc,OAAS,IAAI6P,QAAUmM,EAAO,IAAIpM,UAClE,IAAKvG,GAAW4S,EA3CS,SA6CvBJ,EAAQnY,KAAK,CACXkM,UAAWoM,EAAO,GAAGpM,UACrBC,QAASmM,EAAOA,EAAOhc,OAAS,GAAG6P,eAEhC,GAAImM,EAAOhc,OAChBuN,EAAOsO,EAASG,QACX,GAAI3S,GAASrJ,OAClB,MAAM,IAAIwB,MAAM,8BAA8B6H,EAAQ6H,KAAK,QAE/D,CAEAsH,EAAauD,CACf,CAGA,OAFKG,SAAShD,KAASA,EAASV,GAEzB,CAAE1Q,WAAUmR,WAAUC,SAAQ7P,UAASwS,UAASD,SACzD,CS2BeO,CAAYT,GAIzB,OAHAA,EAAQU,KTPH,SAA6BA,GAAMP,QAAEA,IAE1C,MAAMQ,EAAWR,EAAQ1a,IAAI,EAAGyO,YAAWC,aAAcuM,EAAKhY,MAAMwL,EAAWC,IAC/E,MAAO,CACLlL,WAAYyX,EAAKzX,WACjB,KAAAP,CAAM8K,EAAOC,EAAMiN,EAAKzX,YAEtB,MAAM2X,EAAQT,EAAQf,UAAU,EAAGlL,YAAWC,aAAcD,GAAaV,GAASC,GAAOU,GACzF,GAAIyM,EAAQ,EAAG,MAAM,IAAI9a,MAAM,0BAA0B0N,MAAUC,MACnE,GAAI0M,EAAQS,GAAO1M,YAAcV,GAAS2M,EAAQS,GAAOzM,UAAYV,EAAK,CAExE,MAAMuE,EAAcxE,EAAQ2M,EAAQS,GAAO1M,UACrC2M,EAAYpN,EAAM0M,EAAQS,GAAO1M,UACvC,OAAIyM,EAASC,aAAkB/B,QACtB8B,EAASC,GAAO7N,KAAK5M,GAAUA,EAAOuC,MAAMsP,EAAa6I,IAEzDF,EAASC,GAAOlY,MAAMsP,EAAa6I,EAE9C,CACE,OAAOF,EAASC,EAEpB,EAEJ,CShBiBE,CAAoBd,EAAQU,KAAMT,GAG1CA,EAAKC,OAAOza,IAAIsb,GD7ElB,SAAsBf,GAAS5T,SAAEA,EAAQuB,QAAEA,GAAWoT,GAC3D,MAAML,KAAEA,EAAI3F,YAAEA,EAAWpW,KAAEA,GAASqb,EAG9BvB,EAAe,GAEf/Z,EAAU,IAAKxB,KAAoB8c,EAAQtb,SAGjD,IAAK,MAAMoJ,UAAEA,EAASE,UAAEA,KAAe+S,EAAUrT,SAASC,QAAS,CACjE,GAAIG,EAAW,MAAM,IAAIhI,MAAM,mCAC/B,IAAKkI,EAAW,MAAM,IAAIlI,MAAM,wCAGhC,MAAMoX,EAAalP,EAAUE,eAAe,GAC5C,GAAIP,IAAYA,EAAQsR,SAAS/B,GAAa,SAE9C,MAAMhJ,UAAEA,EAASC,QAAEA,GAAYH,EAAehG,GACxCgT,EAAc7M,EAAUD,EAI9B,GAAI8M,EAAc,GAAK,GAAI,CACzBC,QAAQC,KAAK,iCAAiClT,EAAUE,mBAAmB8S,WAE3E,QACF,CAIA,MAAM7a,EAAS0Y,QAAQsC,QAAQT,EAAKhY,MAAMwL,EAAWC,IAGrDsK,EAAazW,KAAK,CAChBgX,aAAchR,EAAUE,eACxBpK,KAAMqC,EAAO4M,KAAK7G,IAChB,MAAM5D,EAAaL,EAAcmE,EAAS7E,OAAQyG,EAAUE,gBACtDrF,EAAS,CAAEG,KAAM,IAAIwC,SAASU,GAAcnD,OAAQ,GAEpD9E,EAAgB,CACpBiZ,WAFgBlP,EAAUE,eAAesH,KAAK,KAG9C5Q,KAAMoJ,EAAUpJ,KAChBH,QAAS6D,EAAWA,EAAWhE,OAAS,GAAGG,QAC3C6D,aACA6F,MAAOH,EAAUG,MACjBzJ,UACAqW,cACApW,QAEF,OAAOkY,GAAWhU,EAAQkY,EAAW9c,EAAe+b,EAAQ/C,WAGlE,CAEA,MAAO,CAAEH,WAAYiE,EAAUjE,WAAYsD,UAAWW,EAAUX,UAAW3B,eAC7E,CCsBsC2C,CAAapB,EAASC,EAAMc,GAClE,CClFOhW,eAAesW,GAAarB,GACjC,KAAKA,EAAQU,MAAUV,EAAQU,KAAKzX,YAAc,GAChD,MAAM,IAAInD,MAAM,gCAElBka,EAAQ5T,iBAAmBpB,EAAqBgV,EAAQU,MAExD,MAAMtU,SAAEA,EAAQmR,SAAEA,EAAW,EAAC5P,QAAEA,EAAO2T,QAAEA,EAAOhU,OAAEA,GAAW0S,EAC7D,GAAIzC,EAAW,EAAG,MAAM,IAAIzX,MAAM,qCAClC,MAAM0X,EAASwC,EAAQxC,QAAUla,OAAO8I,EAASoB,UAG3C+T,EAAgBC,GAAuBlU,GACvCmU,EAAa5Q,EAAcmP,EAAQ5T,UAAU1E,SAASjC,IAAI8V,GAAKA,EAAE9W,QAAQsD,MAEzE2Z,EAAiBH,EAAcjU,OAAOM,IAAW6T,EAAWxC,SAASrR,IAC3E,GAAI8T,EAAepd,OACjB,MAAM,IAAIwB,MAAM,qCAAqC4b,EAAelM,KAAK,SAE3E,GAAI8L,IAAYG,EAAWxC,SAASqC,GAClC,MAAM,IAAIxb,MAAM,qCAAqCwb,KAEvD,MAAMK,EAAkBhU,EAAU8T,EAAWnU,OAAOM,GAClDD,EAAQsR,SAASrR,IAAW2T,EAActC,SAASrR,IAAWA,IAAW0T,QACvE7P,EAEEmQ,KAAqBjU,IAAWgU,IAAkBhU,EAAQrJ,OAASqd,EAAgBrd,OAEzF,GAAIgJ,IAAWgU,GAAW9D,EAASpR,EAASoB,SAAU,CAEpD,MAAMqU,EAAe,IAAIzc,MACzB,IAAI0X,EAAa,EACjB,IAAK,MAAMgF,KAAS1V,EAASqB,WAAY,CACvC,MAAM4S,EAAWvD,EAAaxZ,OAAOwe,EAAMtU,UAErCmR,QAAkBoD,GAAmB,IACtC/B,EACHzC,SAAUT,EACVU,OAAQ6C,EACR1S,QAASgU,IAEX,IAAK,MAAMtC,KAAOV,EAChB,GAAIqD,GAAW3C,EAAK/R,GAAS,CAC3B,GAAIsU,GAAsBD,EACxB,IAAK,MAAM/T,KAAU+T,EACfhU,IAAYA,EAAQsR,SAASrR,WACxByR,EAAIzR,GAIjBiU,EAAa7Z,KAAKqX,EACpB,CAEF,GAAIwC,EAAavd,QAAUkZ,EAAQ,MACnCV,EAAauD,CACf,CACA,OAAOwB,EAAanZ,MAAM6U,EAAUC,EACtC,CAAO,GAAIlQ,EAAQ,CAEjB,MAAM2U,QAAgBF,GAAmB,IACpC/B,EACHzC,cAAU9L,EACV+L,YAAQ/L,EACR9D,QAASgU,IAEPL,GAASW,EAAQC,KAAK,CAACjQ,EAAGC,IAAMiQ,GAAQlQ,EAAEqP,GAAUpP,EAAEoP,KAC1D,MAAMO,EAAe,IAAIzc,MACzB,IAAK,MAAMia,KAAO4C,EAChB,GAAID,GAAW3C,EAAK/R,GAAS,CAC3B,GAAIsU,GAAsBD,EACxB,IAAK,MAAM/T,KAAU+T,EACfhU,IAAYA,EAAQsR,SAASrR,WACxByR,EAAIzR,GAIjBiU,EAAa7Z,KAAKqX,EACpB,CAEF,OAAOwC,EAAanZ,MAAM6U,EAAUC,EACtC,CAAO,GAAuB,iBAAZ8D,EAAsB,CAEtC,MAAMc,QDSHrX,eAAiCiV,GACtC,GAAgC,IAA5BA,EAAQrS,SAASrJ,OACnB,MAAM,IAAIwB,MAAM,oDAElBka,EAAQ5T,iBAAmBpB,EAAqBgV,EAAQU,MACxD,MAAM2B,EAActC,GAAiBC,GAG/B1Y,EAAauJ,EAAcmP,EAAQ5T,UACnCqT,EAAY4C,EAAY5c,IAAI6c,GAAO/C,GAAc+C,EAAKhb,IAGtDgW,EAAa,GACnB,IAAK,MAAMiF,KAAM9C,EACfnC,EAAWtV,KAAK6L,QAAc0O,EAAG9D,aAAa,GAAG3a,OAEnD,OAAO+P,EAAQyJ,EACjB,CC1B8BkF,CAAkB,IAAKxC,EAASzC,cAAU9L,EAAW+L,YAAQ/L,EAAW9D,QAAS,CAAC2T,KAGtGmB,EAAgBrd,MAAMoB,KAAK4b,EAAa,CAACM,EAAG9B,IAAUA,GACzDsB,KAAK,CAACjQ,EAAGC,IAAMiQ,GAAQC,EAAYnQ,GAAImQ,EAAYlQ,KACnDxJ,MAAM6U,EAAUC,GAEbmF,QAeV5X,eAA+BiV,GAC7B,MAAMU,KAAEA,EAAIkC,KAAEA,GAAS5C,EACvBA,EAAQ5T,iBAAmBpB,EAAqB0V,GAChD,MAAQjT,WAAYoV,GAAc7C,EAAQ5T,SAEpC0W,EAAgB1d,MAAMyd,EAAUve,QAAQye,MAAK,GACnD,IAAIjG,EAAa,EACjB,MAAMkG,EAAYH,EAAUpd,IAAIqc,GAAShF,GAAcxZ,OAAOwe,EAAMtU,WACpE,IAAK,MAAMoT,KAASgC,EAAM,CAExBE,EADmBE,EAAU5D,UAAU3L,GAAOmN,EAAQnN,KAC1B,CAC9B,CAGA,MAAMwP,EAAY,GAClB,IAAIC,EACJpG,EAAa,EACb,IAAK,IAAIvY,EAAI,EAAGA,EAAIue,EAAcxe,OAAQC,IAAK,CAC7C,MAAM8b,EAAWvD,EAAaxZ,OAAOuf,EAAUte,GAAGiJ,UAC9CsV,EAAcve,QACGkN,IAAfyR,IACFA,EAAapG,QAGIrL,IAAfyR,IACFD,EAAUjb,KAAK,CAACkb,EAAY7C,IAC5B6C,OAAazR,GAGjBqL,EAAauD,CACf,MACmB5O,IAAfyR,GACFD,EAAUjb,KAAK,CAACkb,EAAYpG,IAI9B,MAAM6F,EAAa,IAAIvd,MAAM9B,OAAO0c,EAAQ5T,SAASoB,WACrD,IAAK,MAAO0V,EAAYC,KAAaF,EAAW,CAE9C,MAAMtE,QAAkBoD,GAAmB,IAAK/B,EAASzC,SAAU2F,EAAY1F,OAAQ2F,IACvF,IAAK,IAAI5e,EAAI2e,EAAY3e,EAAI4e,EAAU5e,IACrCoe,EAAWpe,GAAKoa,EAAUpa,EAAI2e,GAC9BP,EAAWpe,GAAG6e,UAAY7e,CAE9B,CACA,OAAOoe,CACT,CA7D6BU,CAAgB,IAAKrD,EAAS4C,KAAMH,IAE7D,OADaA,EAAchd,IAAImb,GAAS+B,EAAW/B,GAErD,CACE,aAAamB,GAAmB/B,EAEpC,CA8DA,SAASmC,GAAQlQ,EAAGC,GAClB,OAAID,EAAIC,GAAU,EACdD,EAAIC,EAAU,EACX,CACT,CAUO,SAAS8P,GAAWsB,EAAQC,EAAQ,IACzC,MAAI,SAAUA,GAASne,MAAM+M,QAAQoR,EAAMC,MAClCD,EAAMC,KAAKpR,MAAMqR,GAAYzB,GAAWsB,EAAQG,IAErD,QAASF,GAASne,MAAM+M,QAAQoR,EAAMG,KACjCH,EAAMG,IAAIC,KAAKF,GAAYzB,GAAWsB,EAAQG,IAEnD,SAAUF,GAASne,MAAM+M,QAAQoR,EAAMK,OACjCL,EAAMK,KAAKD,KAAKF,GAAYzB,GAAWsB,EAAQG,IAGlDpR,OAAOwR,QAAQN,GAAOnR,MAAM,EAAE5F,EAAOsX,MAC1C,MAAMjd,EAAQyc,EAAO9W,GAGrB,MAAyB,iBAAdsX,GAAwC,OAAdA,GAAsB1e,MAAM+M,QAAQ2R,GAChE9R,EAAOnL,EAAOid,GAGhBzR,OAAOwR,QAAQC,GAAa,CAAA,GAAI1R,MAAM,EAAE2R,EAAUC,MACvD,OAAQD,GACR,IAAK,MACH,OAAOld,EAAQmd,EACjB,IAAK,OACH,OAAOnd,GAASmd,EAClB,IAAK,MACH,OAAOnd,EAAQmd,EACjB,IAAK,OACH,OAAOnd,GAASmd,EAClB,IAAK,MACH,OAAOhS,EAAOnL,EAAOmd,GACvB,IAAK,MACH,OAAQhS,EAAOnL,EAAOmd,GACxB,IAAK,MACH,OAAO5e,MAAM+M,QAAQ6R,IAAWA,EAAO/E,SAASpY,GAClD,IAAK,OACH,OAAOzB,MAAM+M,QAAQ6R,KAAYA,EAAO/E,SAASpY,GACnD,IAAK,OACH,OAAQmb,GAAW,CAAExV,CAACA,GAAQ3F,GAAS,CAAE2F,CAACA,GAAQwX,IACpD,QACE,OAAO,MAIf,CAQA,SAASxC,GAAuBlU,GAC9B,IAAKA,EAAQ,MAAO,GAEpB,MAAMK,EAAU,GAWhB,MAVI,SAAUL,GAAUlI,MAAM+M,QAAQ7E,EAAOkW,MAC3C7V,EAAQ3F,QAAQsF,EAAOkW,KAAKS,QAAQzC,KAC3B,QAASlU,GAAUlI,MAAM+M,QAAQ7E,EAAOoW,KACjD/V,EAAQ3F,QAAQsF,EAAOoW,IAAIO,QAAQzC,KAC1B,SAAUlU,GAAUlI,MAAM+M,QAAQ7E,EAAOsW,MAClDjW,EAAQ3F,QAAQsF,EAAOsW,KAAKK,QAAQzC,KAGpC7T,EAAQ3F,QAAQqK,OAAOC,KAAKhF,IAEvBK,CACT,CC5OO,SAASoU,GAAmB/B,GACjC,OAAO,IAAInB,QAAQ,CAACqF,EAAYC,MFI3BpZ,eAA2BiV,GAEhCA,EAAQ5T,iBAAmBpB,EAAqBgV,EAAQU,MAGxD,MAAM2B,QAAoBtC,GAAiBC,IAErCzC,SAAEA,EAAW,EAACC,OAAEA,EAAM7P,QAAEA,EAAOyW,QAAEA,EAAOF,WAAEA,EAAUxF,UAAEA,GAAcsB,EAG1E,IAAKkE,IAAeE,EAAS,CAC3B,IAAK,MAAM3F,aAAEA,KAAkB4D,EAC7B,IAAK,MAAMve,KAAEA,KAAU2a,QAAoB3a,EAE7C,MACF,CAGA,MAAMwD,EAAauJ,EAAcmP,EAAQ5T,UACnCqT,EAAY4C,EAAY5c,IAAI6c,GAAO/C,GAAc+C,EAAKhb,IAG5D,GAAI8c,EACF,IAAK,MAAMC,KAAc5E,EACvB,IAAK,MAAMK,KAAeuE,EAAW5F,aACnCqB,EAAYhc,KAAKiP,KAAK6L,IACpB,IAAIrB,EAAW8G,EAAWvH,WAC1B,IAAK,MAAMQ,KAAcsB,EACvBwF,EAAQ,CACNlH,WAAY4C,EAAYd,aAAa,GACrC1B,aACAC,WACAC,OAAQD,EAAWD,EAAWhZ,SAEhCiZ,GAAYD,EAAWhZ,SAQjC,GAAI4f,EAAY,CAEd,MAAMtB,EAAO,GACb,IAAK,MAAMyB,KAAc5E,EAAW,CAElC,MAAM1C,EAAc3R,KAAKC,IAAIkS,EAAW8G,EAAWvH,WAAY,GACzDE,EAAY5R,KAAK8F,KAAKsM,GAAUnW,KAAYgd,EAAWvH,WAAYuH,EAAWjE,WAGpFvO,EAAO+Q,SADiBpE,GAAiB6F,EAAYtH,EAAaC,EAAWrP,EAAS+Q,IAC/DhW,MAAMqU,EAAaC,GAC5C,CACAkH,EAAWtB,EACb,MAEE,IAAK,MAAMnE,aAAEA,KAAkBgB,EAC7B,IAAK,MAAM3b,KAAEA,KAAU2a,QAAoB3a,CAGjD,EE/DIwgB,CAAY,CACV5F,UAAW,YACRsB,EACHkE,eACCK,MAAMJ,IAEb,CCUO,SAASK,GAAUC,GACxB,MAAMC,EAAK,IAAIlZ,SAASiZ,EAAIte,OAAQse,EAAIre,WAAYqe,EAAIxb,YACxD,IAAIF,EAAS,EAGb,MAAM4b,EAAYF,EAAI1b,GAASA,GAAU,EACzC,MAAM6b,EAA+B,IAAdD,EAGjBE,EAAeH,EAAGjZ,UAAU1C,EAAQ6b,GAI1C,GAHA7b,GAAU,EA1Cc,IA6CpB8b,EAAoC,CAEtC,MAAMlK,EAAI+J,EAAG/a,WAAWZ,EAAQ6b,GAAiB7b,GAAU,EAC3D,MAAM+b,EAAIJ,EAAG/a,WAAWZ,EAAQ6b,GAChC,OADiD7b,GAAU,EACpD,CAAEnE,KAAM,QAASmgB,YAAa,CAACpK,EAAGmK,GAC3C,CAAO,GAjDsB,IAiDlBD,EAAyC,CAElD,MAAMG,EAAYN,EAAGjZ,UAAU1C,EAAQ6b,GAAiB7b,GAAU,EAClE,MAAMkc,EAAS,GACf,IAAK,IAAI1gB,EAAI,EAAGA,EAAIygB,EAAWzgB,IAAK,CAClC,MAAMoW,EAAI+J,EAAG/a,WAAWZ,EAAQ6b,GAAiB7b,GAAU,EAC3D,MAAM+b,EAAIJ,EAAG/a,WAAWZ,EAAQ6b,GAAiB7b,GAAU,EAC3Dkc,EAAOjd,KAAK,CAAC2S,EAAGmK,GAClB,CACA,MAAO,CAAElgB,KAAM,aAAcmgB,YAAaE,EAC5C,CAAO,GA1DmB,IA0DfJ,EAAsC,CAE/C,MAAMK,EAAWR,EAAGjZ,UAAU1C,EAAQ6b,GAAiB7b,GAAU,EACjE,MAAMkc,EAAS,GACf,IAAK,IAAIE,EAAI,EAAGA,EAAID,EAAUC,IAAK,CACjC,MAAMH,EAAYN,EAAGjZ,UAAU1C,EAAQ6b,GAAiB7b,GAAU,EAClE,MAAMqc,EAAO,GACb,IAAK,IAAIC,EAAI,EAAGA,EAAIL,EAAWK,IAAK,CAClC,MAAM1K,EAAI+J,EAAG/a,WAAWZ,EAAQ6b,GAAiB7b,GAAU,EAC3D,MAAM+b,EAAIJ,EAAG/a,WAAWZ,EAAQ6b,GAAiB7b,GAAU,EAC3Dqc,EAAKpd,KAAK,CAAC2S,EAAGmK,GAChB,CACAG,EAAOjd,KAAKod,EACd,CACA,MAAO,CAAExgB,KAAM,UAAWmgB,YAAaE,EACzC,CAAO,GAtEwB,IAsEpBJ,EAA2C,CAEpD,MAAMS,EAAcZ,EAAGjZ,UAAU1C,EAAQ6b,GAAiB7b,GAAU,EACpE,MAAMwc,EAAW,GACjB,IAAK,IAAIhhB,EAAI,EAAGA,EAAI+gB,EAAa/gB,IAAK,CAEpC,MAAMihB,EAAqC,IAAhBf,EAAI1b,GAAeA,GAAU,EACxD,MAAM0c,EAAWf,EAAGjZ,UAAU1C,EAAQyc,GACtC,GAD2Dzc,GAAU,EAhF/C,IAiFlB0c,EACF,MAAM,IAAI3f,MAAM,yCAAyC2f,KAE3D,MAAMP,EAAWR,EAAGjZ,UAAU1C,EAAQyc,GAAqBzc,GAAU,EAErE,MAAM2c,EAAW,GACjB,IAAK,IAAIP,EAAI,EAAGA,EAAID,EAAUC,IAAK,CACjC,MAAMH,EAAYN,EAAGjZ,UAAU1C,EAAQyc,GAAqBzc,GAAU,EACtE,MAAMqc,EAAO,GACb,IAAK,IAAIC,EAAI,EAAGA,EAAIL,EAAWK,IAAK,CAClC,MAAM1K,EAAI+J,EAAG/a,WAAWZ,EAAQyc,GAAqBzc,GAAU,EAC/D,MAAM+b,EAAIJ,EAAG/a,WAAWZ,EAAQyc,GAAqBzc,GAAU,EAC/Dqc,EAAKpd,KAAK,CAAC2S,EAAGmK,GAChB,CACAY,EAAS1d,KAAKod,EAChB,CACAG,EAASvd,KAAK0d,EAChB,CACA,MAAO,CAAE9gB,KAAM,eAAgBmgB,YAAaQ,EAC9C,CAAO,GAnGsB,IAmGlBV,EAAyC,CAElD,MAAMG,EAAYN,EAAGjZ,UAAU1C,EAAQ6b,GAAiB7b,GAAU,EAClE,MAAM4c,EAAS,GACf,IAAK,IAAIphB,EAAI,EAAGA,EAAIygB,EAAWzgB,IAAK,CAElC,MAAMqhB,EAAsC,IAAhBnB,EAAI1b,GAAeA,GAAU,EACzD,MAAM8c,EAAYnB,EAAGjZ,UAAU1C,EAAQ6c,GACvC,GAD6D7c,GAAU,EA7GnD,IA8GhB8c,EACF,MAAM,IAAI/f,MAAM,qCAAqC+f,KAEvD,MAAMlL,EAAI+J,EAAG/a,WAAWZ,EAAQ6c,GAAsB7c,GAAU,EAChE,MAAM+b,EAAIJ,EAAG/a,WAAWZ,EAAQ6c,GAAsB7c,GAAU,EAChE4c,EAAO3d,KAAK,CAAC2S,EAAGmK,GAClB,CACA,MAAO,CAAElgB,KAAM,aAAcmgB,YAAaY,EAC5C,CAAO,GAlH2B,IAkHvBd,EAA8C,CAEvD,MAAMiB,EAAiBpB,EAAGjZ,UAAU1C,EAAQ6b,GAAiB7b,GAAU,EACvE,MAAMgd,EAAc,GACpB,IAAK,IAAIxhB,EAAI,EAAGA,EAAIuhB,EAAgBvhB,IAAK,CAEvC,MAAMyhB,EAAqC,IAAhBvB,EAAI1b,GAAeA,GAAU,EACxD,MAAMkd,EAAWvB,EAAGjZ,UAAU1C,EAAQid,GACtC,GAD2Djd,GAAU,EA5H5C,IA6HrBkd,EACF,MAAM,IAAIngB,MAAM,+CAA+CmgB,KAEjE,MAAMjB,EAAYN,EAAGjZ,UAAU1C,EAAQ6b,GAAiB7b,GAAU,EAClE,MAAMkc,EAAS,GACf,IAAK,IAAII,EAAI,EAAGA,EAAIL,EAAWK,IAAK,CAClC,MAAM1K,EAAI+J,EAAG/a,WAAWZ,EAAQid,GAAqBjd,GAAU,EAC/D,MAAM+b,EAAIJ,EAAG/a,WAAWZ,EAAQid,GAAqBjd,GAAU,EAC/Dkc,EAAOjd,KAAK,CAAC2S,EAAGmK,GAClB,CACAiB,EAAY/d,KAAKid,EACnB,CACA,MAAO,CAAErgB,KAAM,kBAAmBmgB,YAAagB,EACjD,CACE,MAAM,IAAIjgB,MAAM,8BAA8B+e,IAElD,EC5IA9Z,iBAEE,MAAM6U,IAAEA,SAAcsG,OAAOC,KAAKC,cAAc,QAG1C3gB,EAAM,IAAIma,EAFsByG,SAASC,eAAe,OAErC,CACvBC,OAAQ,CAAEC,IAAK,GAAIC,KAAK,IACxBC,KAAM,IAMR,IAEE,MAAMhG,QAAalO,EAAmB,CAAEC,IAJvB,0DAIwCxJ,WAAY,QACrEgY,QAAQ0F,IAAI,mBAAoBjG,GAChC,MAAMkG,QCLH7b,gBAAyB2V,KAAEA,EAAI3F,YAAEA,IACtC,MAAM3O,QAAiBpB,EAAqB0V,GACtCmG,EAAcza,EAASmC,oBAAoBnG,KAAK0e,GAAiB,QAAXA,EAAGpW,KAC/D,IAAKmW,EACH,MAAM,IAAI/gB,MAAM,mDAIlB,MAAMihB,EAAYphB,KAAKC,MAAMihB,EAAYhgB,OAAS,MAG5C/C,QAAaud,GAAa,CAAEX,OAAM/b,MAAM,EAAOoW,gBAG/CiM,EAAW,GACXC,EAAgBF,EAAUG,gBAAkB,WAClD,IAAK,MAAM7H,KAAOvb,EAAM,CACtB,MAAM2gB,EAAMpF,EAAI4H,GAChB,IAAKxC,EAEH,SAGF,MAAM0C,EAAW3C,GAAUC,GAIrB2C,EAAa,CAAA,EACnB,IAAK,MAAM1W,KAAO2B,OAAOC,KAAK+M,GAAM,CAClC,IAAIxY,EAAQwY,EAAI3O,GAChB,GAAIA,IAAQuW,GAA2B,OAAVpgB,EAAgB,CAC3C,IAEEA,EAAQlB,KAAKC,MAAMiB,EACrB,CAAE,MAAOwgB,GAET,CACAD,EAAW1W,GAAO7J,CACpB,CACF,CAGA,MAAMygB,EAAU,CACd1iB,KAAM,UACNuiB,WACAC,cAGFJ,EAAShf,KAAKsf,EAChB,CAEA,MAAO,CACL1iB,KAAM,oBACNoiB,WAEJ,CDlD0BO,CAAU,CAAE7G,SAElCO,QAAQ0F,IAAI,WAAYC,GAGxBnhB,EAAI3B,KAAK0jB,WAAWZ,EACtB,CAAE,MAAOS,GACPpG,QAAQoG,MAAM,4CAA6CA,EAC7D,CACF,CACAI","x_google_ignoreList":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]}