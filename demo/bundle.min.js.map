{"version":3,"file":"bundle.min.js","sources":["../node_modules/hyparquet/src/constants.js","../node_modules/hyparquet/src/wkb.js","../node_modules/hyparquet/src/convert.js","../node_modules/hyparquet/src/schema.js","../node_modules/hyparquet/src/thrift.js","../node_modules/hyparquet/src/metadata.js","../node_modules/hyparquet/src/geoparquet.js","../node_modules/hyparquet/src/utils.js","../node_modules/hyparquet/src/plan.js","../node_modules/hyparquet/src/assemble.js","../node_modules/hyparquet/src/delta.js","../node_modules/hyparquet/src/encoding.js","../node_modules/hyparquet/src/plain.js","../node_modules/hyparquet/src/snappy.js","../node_modules/hyparquet/src/datapage.js","../node_modules/hyparquet/src/column.js","../node_modules/hyparquet/src/rowgroup.js","../node_modules/hyparquet/src/read.js","../src/toGeoJson.js","demo.js"],"sourcesContent":["/** @type {import('../src/types.d.ts').ParquetType[]} */\nexport const ParquetType = [\n  'BOOLEAN',\n  'INT32',\n  'INT64',\n  'INT96', // deprecated\n  'FLOAT',\n  'DOUBLE',\n  'BYTE_ARRAY',\n  'FIXED_LEN_BYTE_ARRAY',\n]\n\n/** @type {import('../src/types.d.ts').Encoding[]} */\nexport const Encoding = [\n  'PLAIN',\n  'GROUP_VAR_INT', // deprecated\n  'PLAIN_DICTIONARY',\n  'RLE',\n  'BIT_PACKED', // deprecated\n  'DELTA_BINARY_PACKED',\n  'DELTA_LENGTH_BYTE_ARRAY',\n  'DELTA_BYTE_ARRAY',\n  'RLE_DICTIONARY',\n  'BYTE_STREAM_SPLIT',\n]\n\n/** @type {import('../src/types.d.ts').FieldRepetitionType[]} */\nexport const FieldRepetitionType = [\n  'REQUIRED',\n  'OPTIONAL',\n  'REPEATED',\n]\n\n/** @type {import('../src/types.d.ts').ConvertedType[]} */\nexport const ConvertedType = [\n  'UTF8',\n  'MAP',\n  'MAP_KEY_VALUE',\n  'LIST',\n  'ENUM',\n  'DECIMAL',\n  'DATE',\n  'TIME_MILLIS',\n  'TIME_MICROS',\n  'TIMESTAMP_MILLIS',\n  'TIMESTAMP_MICROS',\n  'UINT_8',\n  'UINT_16',\n  'UINT_32',\n  'UINT_64',\n  'INT_8',\n  'INT_16',\n  'INT_32',\n  'INT_64',\n  'JSON',\n  'BSON',\n  'INTERVAL',\n]\n\n/** @type {import('../src/types.d.ts').CompressionCodec[]} */\nexport const CompressionCodec = [\n  'UNCOMPRESSED',\n  'SNAPPY',\n  'GZIP',\n  'LZO',\n  'BROTLI',\n  'LZ4',\n  'ZSTD',\n  'LZ4_RAW',\n]\n\n/** @type {import('../src/types.d.ts').PageType[]} */\nexport const PageType = [\n  'DATA_PAGE',\n  'INDEX_PAGE',\n  'DICTIONARY_PAGE',\n  'DATA_PAGE_V2',\n]\n\n/** @type {import('../src/types.d.ts').BoundaryOrder[]} */\nexport const BoundaryOrder = [\n  'UNORDERED',\n  'ASCENDING',\n  'DESCENDING',\n]\n\n/** @type {import('../src/types.d.ts').EdgeInterpolationAlgorithm[]} */\nexport const EdgeInterpolationAlgorithm = [\n  'SPHERICAL',\n  'VINCENTY',\n  'THOMAS',\n  'ANDOYER',\n  'KARNEY',\n]\n","/**\n * WKB (Well-Known Binary) decoder for geometry objects.\n *\n * @import {DataReader, Geometry} from '../src/types.js'\n * @param {DataReader} reader\n * @returns {Geometry} geometry object\n */\nexport function wkbToGeojson(reader) {\n  const flags = getFlags(reader)\n\n  if (flags.type === 1) { // Point\n    return { type: 'Point', coordinates: readPosition(reader, flags) }\n  } else if (flags.type === 2) { // LineString\n    return { type: 'LineString', coordinates: readLine(reader, flags) }\n  } else if (flags.type === 3) { // Polygon\n    return { type: 'Polygon', coordinates: readPolygon(reader, flags) }\n  } else if (flags.type === 4) { // MultiPoint\n    const points = []\n    for (let i = 0; i < flags.count; i++) {\n      points.push(readPosition(reader, getFlags(reader)))\n    }\n    return { type: 'MultiPoint', coordinates: points }\n  } else if (flags.type === 5) { // MultiLineString\n    const lines = []\n    for (let i = 0; i < flags.count; i++) {\n      lines.push(readLine(reader, getFlags(reader)))\n    }\n    return { type: 'MultiLineString', coordinates: lines }\n  } else if (flags.type === 6) { // MultiPolygon\n    const polygons = []\n    for (let i = 0; i < flags.count; i++) {\n      polygons.push(readPolygon(reader, getFlags(reader)))\n    }\n    return { type: 'MultiPolygon', coordinates: polygons }\n  } else if (flags.type === 7) { // GeometryCollection\n    const geometries = []\n    for (let i = 0; i < flags.count; i++) {\n      geometries.push(wkbToGeojson(reader))\n    }\n    return { type: 'GeometryCollection', geometries }\n  } else {\n    throw new Error(`Unsupported geometry type: ${flags.type}`)\n  }\n}\n\n/**\n * @typedef {object} WkbFlags\n * @property {boolean} littleEndian\n * @property {number} type\n * @property {number} dim\n * @property {number} count\n */\n\n/**\n * Extract ISO WKB flags and base geometry type.\n *\n * @param {DataReader} reader\n * @returns {WkbFlags}\n */\nfunction getFlags(reader) {\n  const { view } = reader\n  const littleEndian = view.getUint8(reader.offset++) === 1\n  const rawType = view.getUint32(reader.offset, littleEndian)\n  reader.offset += 4\n\n  const type = rawType % 1000\n  const flags = Math.floor(rawType / 1000)\n\n  let count = 0\n  if (type > 1 && type <= 7) {\n    count = view.getUint32(reader.offset, littleEndian)\n    reader.offset += 4\n  }\n\n  // XY, XYZ, XYM, XYZM\n  let dim = 2\n  if (flags) dim++\n  if (flags === 3) dim++\n\n  return { littleEndian, type, dim, count }\n}\n\n/**\n * @param {DataReader} reader\n * @param {WkbFlags} flags\n * @returns {number[]}\n */\nfunction readPosition(reader, flags) {\n  const points = []\n  for (let i = 0; i < flags.dim; i++) {\n    const coord = reader.view.getFloat64(reader.offset, flags.littleEndian)\n    reader.offset += 8\n    points.push(coord)\n  }\n  return points\n}\n\n/**\n * @param {DataReader} reader\n * @param {WkbFlags} flags\n * @returns {number[][]}\n */\nfunction readLine(reader, flags) {\n  const points = []\n  for (let i = 0; i < flags.count; i++) {\n    points.push(readPosition(reader, flags))\n  }\n  return points\n}\n\n/**\n * @param {DataReader} reader\n * @param {WkbFlags} flags\n * @returns {number[][][]}\n */\nfunction readPolygon(reader, flags) {\n  const { view } = reader\n  const rings = []\n  for (let r = 0; r < flags.count; r++) {\n    const count = view.getUint32(reader.offset, flags.littleEndian)\n    reader.offset += 4\n    rings.push(readLine(reader, { ...flags, count }))\n  }\n  return rings\n}\n","import { wkbToGeojson } from './wkb.js'\n\n/**\n * @import {ColumnDecoder, DecodedArray, Encoding, ParquetParsers} from '../src/types.js'\n */\n\nconst decoder = new TextDecoder()\n\n/**\n * Default type parsers when no custom ones are given\n * @type ParquetParsers\n */\nexport const DEFAULT_PARSERS = {\n  timestampFromMilliseconds(millis) {\n    return new Date(Number(millis))\n  },\n  timestampFromMicroseconds(micros) {\n    return new Date(Number(micros / 1000n))\n  },\n  timestampFromNanoseconds(nanos) {\n    return new Date(Number(nanos / 1000000n))\n  },\n  dateFromDays(days) {\n    return new Date(days * 86400000)\n  },\n  stringFromBytes(bytes) {\n    return bytes && decoder.decode(bytes)\n  },\n  geometryFromBytes(bytes) {\n    return bytes && wkbToGeojson({ view: new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength), offset: 0 })\n  },\n  geographyFromBytes(bytes) {\n    return bytes && wkbToGeojson({ view: new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength), offset: 0 })\n  },\n}\n\n/**\n * Convert known types from primitive to rich, and dereference dictionary.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {DecodedArray | undefined} dictionary\n * @param {Encoding} encoding\n * @param {ColumnDecoder} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convertWithDictionary(data, dictionary, encoding, columnDecoder) {\n  if (dictionary && encoding.endsWith('_DICTIONARY')) {\n    let output = data\n    if (data instanceof Uint8Array && !(dictionary instanceof Uint8Array)) {\n      // @ts-expect-error upgrade data to match dictionary type with fancy constructor\n      output = new dictionary.constructor(data.length)\n    }\n    for (let i = 0; i < data.length; i++) {\n      output[i] = dictionary[data[i]]\n    }\n    return output\n  } else {\n    return convert(data, columnDecoder)\n  }\n}\n\n/**\n * Convert known types from primitive to rich.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {Pick<ColumnDecoder, \"element\" | \"utf8\" | \"parsers\">} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convert(data, columnDecoder) {\n  const { element, parsers, utf8 = true } = columnDecoder\n  const { type, converted_type: ctype, logical_type: ltype } = element\n  if (ctype === 'DECIMAL') {\n    const scale = element.scale || 0\n    const factor = 10 ** -scale\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      if (data[i] instanceof Uint8Array) {\n        arr[i] = parseDecimal(data[i]) * factor\n      } else {\n        arr[i] = Number(data[i]) * factor\n      }\n    }\n    return arr\n  }\n  if (!ctype && type === 'INT96') {\n    return Array.from(data).map(v => parsers.timestampFromNanoseconds(parseInt96Nanos(v)))\n  }\n  if (ctype === 'DATE') {\n    return Array.from(data).map(v => parsers.dateFromDays(v))\n  }\n  if (ctype === 'TIMESTAMP_MILLIS') {\n    return Array.from(data).map(v => parsers.timestampFromMilliseconds(v))\n  }\n  if (ctype === 'TIMESTAMP_MICROS') {\n    return Array.from(data).map(v => parsers.timestampFromMicroseconds(v))\n  }\n  if (ctype === 'JSON') {\n    return data.map(v => JSON.parse(decoder.decode(v)))\n  }\n  if (ctype === 'BSON') {\n    throw new Error('parquet bson not supported')\n  }\n  if (ctype === 'INTERVAL') {\n    throw new Error('parquet interval not supported')\n  }\n  if (ltype?.type === 'GEOMETRY') {\n    return data.map(v => parsers.geometryFromBytes(v))\n  }\n  if (ltype?.type === 'GEOGRAPHY') {\n    return data.map(v => parsers.geographyFromBytes(v))\n  }\n  if (ctype === 'UTF8' || ltype?.type === 'STRING' || utf8 && type === 'BYTE_ARRAY') {\n    return data.map(v => parsers.stringFromBytes(v))\n  }\n  if (ctype === 'UINT_64' || ltype?.type === 'INTEGER' && ltype.bitWidth === 64 && !ltype.isSigned) {\n    if (data instanceof BigInt64Array) {\n      return new BigUint64Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new BigUint64Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = BigInt(data[i])\n    return arr\n  }\n  if (ctype === 'UINT_32' || ltype?.type === 'INTEGER' && ltype.bitWidth === 32 && !ltype.isSigned) {\n    if (data instanceof Int32Array) {\n      return new Uint32Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new Uint32Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = data[i]\n    return arr\n  }\n  if (ltype?.type === 'FLOAT16') {\n    return Array.from(data).map(parseFloat16)\n  }\n  if (ltype?.type === 'TIMESTAMP') {\n    const { unit } = ltype\n    /** @type {ParquetParsers[keyof ParquetParsers]} */\n    let parser = parsers.timestampFromMilliseconds\n    if (unit === 'MICROS') parser = parsers.timestampFromMicroseconds\n    if (unit === 'NANOS') parser = parsers.timestampFromNanoseconds\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parser(data[i])\n    }\n    return arr\n  }\n  return data\n}\n\n/**\n * @param {Uint8Array} bytes\n * @returns {number}\n */\nexport function parseDecimal(bytes) {\n  if (!bytes.length) return 0\n\n  let value = 0n\n  for (const byte of bytes) {\n    value = value * 256n + BigInt(byte)\n  }\n\n  // handle signed\n  const bits = bytes.length * 8\n  if (value >= 2n ** BigInt(bits - 1)) {\n    value -= 2n ** BigInt(bits)\n  }\n\n  return Number(value)\n}\n\n/**\n * Converts INT96 date format (hi 32bit days, lo 64bit nanos) to nanos since epoch\n * @param {bigint} value\n * @returns {bigint}\n */\nfunction parseInt96Nanos(value) {\n  const days = (value >> 64n) - 2440588n\n  const nano = value & 0xffffffffffffffffn\n  return days * 86400000000000n + nano\n}\n\n/**\n * @param {Uint8Array | undefined} bytes\n * @returns {number | undefined}\n */\nexport function parseFloat16(bytes) {\n  if (!bytes) return undefined\n  const int16 = bytes[1] << 8 | bytes[0]\n  const sign = int16 >> 15 ? -1 : 1\n  const exp = int16 >> 10 & 0x1f\n  const frac = int16 & 0x3ff\n  if (exp === 0) return sign * 2 ** -14 * (frac / 1024) // subnormals\n  if (exp === 0x1f) return frac ? NaN : sign * Infinity\n  return sign * 2 ** (exp - 15) * (1 + frac / 1024)\n}\n","/**\n * Build a tree from the schema elements.\n *\n * @import {SchemaElement, SchemaTree} from '../src/types.d.ts'\n * @param {SchemaElement[]} schema\n * @param {number} rootIndex index of the root element\n * @param {string[]} path path to the element\n * @returns {SchemaTree} tree of schema elements\n */\nfunction schemaTree(schema, rootIndex, path) {\n  const element = schema[rootIndex]\n  const children = []\n  let count = 1\n\n  // Read the specified number of children\n  if (element.num_children) {\n    while (children.length < element.num_children) {\n      const childElement = schema[rootIndex + count]\n      const child = schemaTree(schema, rootIndex + count, [...path, childElement.name])\n      count += child.count\n      children.push(child)\n    }\n  }\n\n  return { count, element, children, path }\n}\n\n/**\n * Get schema elements from the root to the given element name.\n *\n * @param {SchemaElement[]} schema\n * @param {string[]} name path to the element\n * @returns {SchemaTree[]} list of schema elements\n */\nexport function getSchemaPath(schema, name) {\n  let tree = schemaTree(schema, 0, [])\n  const path = [tree]\n  for (const part of name) {\n    const child = tree.children.find(child => child.element.name === part)\n    if (!child) throw new Error(`parquet schema element not found: ${name}`)\n    path.push(child)\n    tree = child\n  }\n  return path\n}\n\n/**\n * Get the max repetition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max repetition level\n */\nexport function getMaxRepetitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath) {\n    if (element.repetition_type === 'REPEATED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Get the max definition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max definition level\n */\nexport function getMaxDefinitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath.slice(1)) {\n    if (element.repetition_type !== 'REQUIRED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Check if a column is list-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if list-like\n */\nexport function isListLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'LIST') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length > 1) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Check if a column is map-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if map-like\n */\nexport function isMapLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'MAP') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length !== 2) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  const keyChild = firstChild.children.find(child => child.element.name === 'key')\n  if (keyChild?.element.repetition_type === 'REPEATED') return false\n\n  const valueChild = firstChild.children.find(child => child.element.name === 'value')\n  if (valueChild?.element.repetition_type === 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Returns true if a column is non-nested.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {boolean}\n */\nexport function isFlatColumn(schemaPath) {\n  if (schemaPath.length !== 2) return false\n  const [, column] = schemaPath\n  if (column.element.repetition_type === 'REPEATED') return false\n  if (column.children.length) return false\n  return true\n}\n","// TCompactProtocol types\nexport const CompactType = {\n  STOP: 0,\n  TRUE: 1,\n  FALSE: 2,\n  BYTE: 3,\n  I16: 4,\n  I32: 5,\n  I64: 6,\n  DOUBLE: 7,\n  BINARY: 8,\n  LIST: 9,\n  SET: 10,\n  MAP: 11,\n  STRUCT: 12,\n  UUID: 13,\n}\n\n/**\n * Parse TCompactProtocol\n *\n * @param {DataReader} reader\n * @returns {{ [key: `field_${number}`]: any }}\n */\nexport function deserializeTCompactProtocol(reader) {\n  let lastFid = 0\n  /** @type {ThriftObject} */\n  const value = {}\n\n  while (reader.offset < reader.view.byteLength) {\n    // Parse each field based on its type and add to the result object\n    const [type, fid, newLastFid] = readFieldBegin(reader, lastFid)\n    lastFid = newLastFid\n\n    if (type === CompactType.STOP) {\n      break\n    }\n\n    // Handle the field based on its type\n    value[`field_${fid}`] = readElement(reader, type)\n  }\n\n  return value\n}\n\n/**\n * Read a single element based on its type\n *\n * @import {DataReader, ThriftObject, ThriftType} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} type\n * @returns {ThriftType}\n */\nfunction readElement(reader, type) {\n  switch (type) {\n  case CompactType.TRUE:\n    return true\n  case CompactType.FALSE:\n    return false\n  case CompactType.BYTE:\n    // read byte directly\n    return reader.view.getInt8(reader.offset++)\n  case CompactType.I16:\n  case CompactType.I32:\n    return readZigZag(reader)\n  case CompactType.I64:\n    return readZigZagBigInt(reader)\n  case CompactType.DOUBLE: {\n    const value = reader.view.getFloat64(reader.offset, true)\n    reader.offset += 8\n    return value\n  }\n  case CompactType.BINARY: {\n    const stringLength = readVarInt(reader)\n    const strBytes = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, stringLength)\n    reader.offset += stringLength\n    return strBytes\n  }\n  case CompactType.LIST: {\n    const byte = reader.view.getUint8(reader.offset++)\n    const elemType = byte & 0x0f\n    let listSize = byte >> 4\n    if (listSize === 15) {\n      listSize = readVarInt(reader)\n    }\n    const boolType = elemType === CompactType.TRUE || elemType === CompactType.FALSE\n    const values = new Array(listSize)\n    for (let i = 0; i < listSize; i++) {\n      values[i] = boolType ? readElement(reader, CompactType.BYTE) === 1 : readElement(reader, elemType)\n    }\n    return values\n  }\n  case CompactType.STRUCT: {\n    /** @type {ThriftObject} */\n    const structValues = {}\n    let lastFid = 0\n    while (true) {\n      const [fieldType, fid, newLastFid] = readFieldBegin(reader, lastFid)\n      lastFid = newLastFid\n      if (fieldType === CompactType.STOP) {\n        break\n      }\n      structValues[`field_${fid}`] = readElement(reader, fieldType)\n    }\n    return structValues\n  }\n  // TODO: MAP, SET, UUID\n  default:\n    throw new Error(`thrift unhandled type: ${type}`)\n  }\n}\n\n/**\n * Var int aka Unsigned LEB128.\n * Reads groups of 7 low bits until high bit is 0.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readVarInt(reader) {\n  let result = 0\n  let shift = 0\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= (byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7\n  }\n}\n\n/**\n * Read a varint as a bigint.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nfunction readVarBigInt(reader) {\n  let result = 0n\n  let shift = 0n\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= BigInt(byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7n\n  }\n}\n\n/**\n * Values of type int32 and int64 are transformed to a zigzag int.\n * A zigzag int folds positive and negative numbers into the positive number space.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readZigZag(reader) {\n  const zigzag = readVarInt(reader)\n  // convert zigzag to int\n  return zigzag >>> 1 ^ -(zigzag & 1)\n}\n\n/**\n * A zigzag int folds positive and negative numbers into the positive number space.\n * This version returns a BigInt.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nexport function readZigZagBigInt(reader) {\n  const zigzag = readVarBigInt(reader)\n  // convert zigzag to int\n  return zigzag >> 1n ^ -(zigzag & 1n)\n}\n\n/**\n * Read field type and field id\n *\n * @param {DataReader} reader\n * @param {number} lastFid\n * @returns {[number, number, number]} [type, fid, newLastFid]\n */\nfunction readFieldBegin(reader, lastFid) {\n  const byte = reader.view.getUint8(reader.offset++)\n  const type = byte & 0x0f\n  if (type === CompactType.STOP) {\n    // STOP also ends a struct\n    return [0, 0, lastFid]\n  }\n  const delta = byte >> 4\n  const fid = delta ? lastFid + delta : readZigZag(reader)\n  return [type, fid, fid]\n}\n","import { CompressionCodec, ConvertedType, EdgeInterpolationAlgorithm, Encoding, FieldRepetitionType, PageType, ParquetType } from './constants.js'\nimport { DEFAULT_PARSERS, parseDecimal, parseFloat16 } from './convert.js'\nimport { getSchemaPath } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\nimport { markGeoColumns } from './geoparquet.js'\n\nexport const defaultInitialFetchSize = 1 << 19 // 512kb\n\nconst decoder = new TextDecoder()\nfunction decode(/** @type {Uint8Array} */ value) {\n  return value && decoder.decode(value)\n}\n\n/**\n * Read parquet metadata from an async buffer.\n *\n * An AsyncBuffer is like an ArrayBuffer, but the slices are loaded\n * asynchronously, possibly over the network.\n *\n * You must provide the byteLength of the buffer, typically from a HEAD request.\n *\n * In theory, you could use suffix-range requests to fetch the end of the file,\n * and save a round trip. But in practice, this doesn't work because chrome\n * deems suffix-range requests as a not-safe-listed header, and will require\n * a pre-flight. So the byteLength is required.\n *\n * To make this efficient, we initially request the last 512kb of the file,\n * which is likely to contain the metadata. If the metadata length exceeds the\n * initial fetch, 512kb, we request the rest of the metadata from the AsyncBuffer.\n *\n * This ensures that we either make one 512kb initial request for the metadata,\n * or a second request for up to the metadata size.\n *\n * @param {AsyncBuffer} asyncBuffer parquet file contents\n * @param {MetadataOptions & { initialFetchSize?: number }} options initial fetch size in bytes (default 512kb)\n * @returns {Promise<FileMetaData>} parquet metadata object\n */\nexport async function parquetMetadataAsync(asyncBuffer, { parsers, initialFetchSize = defaultInitialFetchSize, geoparquet = true } = {}) {\n  if (!asyncBuffer || !(asyncBuffer.byteLength >= 0)) throw new Error('parquet expected AsyncBuffer')\n\n  // fetch last bytes (footer) of the file\n  const footerOffset = Math.max(0, asyncBuffer.byteLength - initialFetchSize)\n  const footerBuffer = await asyncBuffer.slice(footerOffset, asyncBuffer.byteLength)\n\n  // Check for parquet magic number \"PAR1\"\n  const footerView = new DataView(footerBuffer)\n  if (footerView.getUint32(footerBuffer.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLength = footerView.getUint32(footerBuffer.byteLength - 8, true)\n  if (metadataLength > asyncBuffer.byteLength - 8) {\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${asyncBuffer.byteLength - 8}`)\n  }\n\n  // check if metadata size fits inside the initial fetch\n  if (metadataLength + 8 > initialFetchSize) {\n    // fetch the rest of the metadata\n    const metadataOffset = asyncBuffer.byteLength - metadataLength - 8\n    const metadataBuffer = await asyncBuffer.slice(metadataOffset, footerOffset)\n    // combine initial fetch with the new slice\n    const combinedBuffer = new ArrayBuffer(metadataLength + 8)\n    const combinedView = new Uint8Array(combinedBuffer)\n    combinedView.set(new Uint8Array(metadataBuffer))\n    combinedView.set(new Uint8Array(footerBuffer), footerOffset - metadataOffset)\n    return parquetMetadata(combinedBuffer, { parsers, geoparquet })\n  } else {\n    // parse metadata from the footer\n    return parquetMetadata(footerBuffer, { parsers, geoparquet })\n  }\n}\n\n/**\n * Read parquet metadata from a buffer synchronously.\n *\n * @import {KeyValue} from '../src/types.d.ts'\n * @param {ArrayBuffer} arrayBuffer parquet file footer\n * @param {MetadataOptions} options metadata parsing options\n * @returns {FileMetaData} parquet metadata object\n */\nexport function parquetMetadata(arrayBuffer, { parsers, geoparquet = true } = {}) {\n  if (!(arrayBuffer instanceof ArrayBuffer)) throw new Error('parquet expected ArrayBuffer')\n  const view = new DataView(arrayBuffer)\n\n  // Use default parsers if not given\n  parsers = { ...DEFAULT_PARSERS, ...parsers }\n\n  // Validate footer magic number \"PAR1\"\n  if (view.byteLength < 8) {\n    throw new Error('parquet file is too short')\n  }\n  if (view.getUint32(view.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLengthOffset = view.byteLength - 8\n  const metadataLength = view.getUint32(metadataLengthOffset, true)\n  if (metadataLength > view.byteLength - 8) {\n    // {metadata}, metadata_length, PAR1\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${view.byteLength - 8}`)\n  }\n\n  const metadataOffset = metadataLengthOffset - metadataLength\n  const reader = { view, offset: metadataOffset }\n  const metadata = deserializeTCompactProtocol(reader)\n\n  // Parse metadata from thrift data\n  const version = metadata.field_1\n  /** @type {SchemaElement[]} */\n  const schema = metadata.field_2.map((/** @type {any} */ field) => ({\n    type: ParquetType[field.field_1],\n    type_length: field.field_2,\n    repetition_type: FieldRepetitionType[field.field_3],\n    name: decode(field.field_4),\n    num_children: field.field_5,\n    converted_type: ConvertedType[field.field_6],\n    scale: field.field_7,\n    precision: field.field_8,\n    field_id: field.field_9,\n    logical_type: logicalType(field.field_10),\n  }))\n  // schema element per column index\n  const columnSchema = schema.filter(e => e.type)\n  const num_rows = metadata.field_3\n  const row_groups = metadata.field_4.map((/** @type {any} */ rowGroup) => ({\n    columns: rowGroup.field_1.map((/** @type {any} */ column, /** @type {number} */ columnIndex) => ({\n      file_path: decode(column.field_1),\n      file_offset: column.field_2,\n      meta_data: column.field_3 && {\n        type: ParquetType[column.field_3.field_1],\n        encodings: column.field_3.field_2?.map((/** @type {number} */ e) => Encoding[e]),\n        path_in_schema: column.field_3.field_3.map(decode),\n        codec: CompressionCodec[column.field_3.field_4],\n        num_values: column.field_3.field_5,\n        total_uncompressed_size: column.field_3.field_6,\n        total_compressed_size: column.field_3.field_7,\n        key_value_metadata: column.field_3.field_8,\n        data_page_offset: column.field_3.field_9,\n        index_page_offset: column.field_3.field_10,\n        dictionary_page_offset: column.field_3.field_11,\n        statistics: convertStats(column.field_3.field_12, columnSchema[columnIndex], parsers),\n        encoding_stats: column.field_3.field_13?.map((/** @type {any} */ encodingStat) => ({\n          page_type: PageType[encodingStat.field_1],\n          encoding: Encoding[encodingStat.field_2],\n          count: encodingStat.field_3,\n        })),\n        bloom_filter_offset: column.field_3.field_14,\n        bloom_filter_length: column.field_3.field_15,\n        size_statistics: column.field_3.field_16 && {\n          unencoded_byte_array_data_bytes: column.field_3.field_16.field_1,\n          repetition_level_histogram: column.field_3.field_16.field_2,\n          definition_level_histogram: column.field_3.field_16.field_3,\n        },\n        geospatial_statistics: column.field_3.field_17 && {\n          bbox: column.field_3.field_17.field_1 && {\n            xmin: column.field_3.field_17.field_1.field_1,\n            xmax: column.field_3.field_17.field_1.field_2,\n            ymin: column.field_3.field_17.field_1.field_3,\n            ymax: column.field_3.field_17.field_1.field_4,\n            zmin: column.field_3.field_17.field_1.field_5,\n            zmax: column.field_3.field_17.field_1.field_6,\n            mmin: column.field_3.field_17.field_1.field_7,\n            mmax: column.field_3.field_17.field_1.field_8,\n          },\n          geospatial_types: column.field_3.field_17.field_2,\n        },\n      },\n      offset_index_offset: column.field_4,\n      offset_index_length: column.field_5,\n      column_index_offset: column.field_6,\n      column_index_length: column.field_7,\n      crypto_metadata: column.field_8,\n      encrypted_column_metadata: column.field_9,\n    })),\n    total_byte_size: rowGroup.field_2,\n    num_rows: rowGroup.field_3,\n    sorting_columns: rowGroup.field_4?.map((/** @type {any} */ sortingColumn) => ({\n      column_idx: sortingColumn.field_1,\n      descending: sortingColumn.field_2,\n      nulls_first: sortingColumn.field_3,\n    })),\n    file_offset: rowGroup.field_5,\n    total_compressed_size: rowGroup.field_6,\n    ordinal: rowGroup.field_7,\n  }))\n  /** @type {KeyValue[] | undefined} */\n  const key_value_metadata = metadata.field_5?.map((/** @type {any} */ keyValue) => ({\n    key: decode(keyValue.field_1),\n    value: decode(keyValue.field_2),\n  }))\n  const created_by = decode(metadata.field_6)\n\n  if (geoparquet) {\n    markGeoColumns(schema, key_value_metadata)\n  }\n\n  return {\n    version,\n    schema,\n    num_rows,\n    row_groups,\n    key_value_metadata,\n    created_by,\n    metadata_length: metadataLength,\n  }\n}\n\n/**\n * Return a tree of schema elements from parquet metadata.\n *\n * @param {{schema: SchemaElement[]}} metadata parquet metadata object\n * @returns {SchemaTree} tree of schema elements\n */\nexport function parquetSchema({ schema }) {\n  return getSchemaPath(schema, [])[0]\n}\n\n/**\n * @param {any} logicalType\n * @returns {LogicalType | undefined}\n */\nfunction logicalType(logicalType) {\n  if (logicalType?.field_1) return { type: 'STRING' }\n  if (logicalType?.field_2) return { type: 'MAP' }\n  if (logicalType?.field_3) return { type: 'LIST' }\n  if (logicalType?.field_4) return { type: 'ENUM' }\n  if (logicalType?.field_5) return {\n    type: 'DECIMAL',\n    scale: logicalType.field_5.field_1,\n    precision: logicalType.field_5.field_2,\n  }\n  if (logicalType?.field_6) return { type: 'DATE' }\n  if (logicalType?.field_7) return {\n    type: 'TIME',\n    isAdjustedToUTC: logicalType.field_7.field_1,\n    unit: timeUnit(logicalType.field_7.field_2),\n  }\n  if (logicalType?.field_8) return {\n    type: 'TIMESTAMP',\n    isAdjustedToUTC: logicalType.field_8.field_1,\n    unit: timeUnit(logicalType.field_8.field_2),\n  }\n  if (logicalType?.field_10) return {\n    type: 'INTEGER',\n    bitWidth: logicalType.field_10.field_1,\n    isSigned: logicalType.field_10.field_2,\n  }\n  if (logicalType?.field_11) return { type: 'NULL' }\n  if (logicalType?.field_12) return { type: 'JSON' }\n  if (logicalType?.field_13) return { type: 'BSON' }\n  if (logicalType?.field_14) return { type: 'UUID' }\n  if (logicalType?.field_15) return { type: 'FLOAT16' }\n  if (logicalType?.field_16) return { type: 'VARIANT' }\n  if (logicalType?.field_17) return {\n    type: 'GEOMETRY',\n    crs: decode(logicalType.field_17.field_1),\n  }\n  if (logicalType?.field_18) return {\n    type: 'GEOGRAPHY',\n    crs: decode(logicalType.field_18.field_1),\n    algorithm: EdgeInterpolationAlgorithm[logicalType.field_18.field_2],\n  }\n  return logicalType\n}\n\n/**\n * @param {any} unit\n * @returns {TimeUnit}\n */\nfunction timeUnit(unit) {\n  if (unit.field_1) return 'MILLIS'\n  if (unit.field_2) return 'MICROS'\n  if (unit.field_3) return 'NANOS'\n  throw new Error('parquet time unit required')\n}\n\n/**\n * Convert column statistics based on column type.\n *\n * @import {AsyncBuffer, FileMetaData, LogicalType, MetadataOptions, MinMaxType, ParquetParsers, SchemaElement, SchemaTree, Statistics, TimeUnit} from '../src/types.d.ts'\n * @param {any} stats\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {Statistics}\n */\nfunction convertStats(stats, schema, parsers) {\n  return stats && {\n    max: convertMetadata(stats.field_1, schema, parsers),\n    min: convertMetadata(stats.field_2, schema, parsers),\n    null_count: stats.field_3,\n    distinct_count: stats.field_4,\n    max_value: convertMetadata(stats.field_5, schema, parsers),\n    min_value: convertMetadata(stats.field_6, schema, parsers),\n    is_max_value_exact: stats.field_7,\n    is_min_value_exact: stats.field_8,\n  }\n}\n\n/**\n * @param {Uint8Array | undefined} value\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {MinMaxType | undefined}\n */\nexport function convertMetadata(value, schema, parsers) {\n  const { type, converted_type, logical_type } = schema\n  if (value === undefined) return value\n  if (type === 'BOOLEAN') return value[0] === 1\n  if (type === 'BYTE_ARRAY') return parsers.stringFromBytes(value)\n  const view = new DataView(value.buffer, value.byteOffset, value.byteLength)\n  if (type === 'FLOAT' && view.byteLength === 4) return view.getFloat32(0, true)\n  if (type === 'DOUBLE' && view.byteLength === 8) return view.getFloat64(0, true)\n  if (type === 'INT32' && converted_type === 'DATE') return parsers.dateFromDays(view.getInt32(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MILLIS') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'NANOS') return parsers.timestampFromNanoseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT32' && view.byteLength === 4) return view.getInt32(0, true)\n  if (type === 'INT64' && view.byteLength === 8) return view.getBigInt64(0, true)\n  if (converted_type === 'DECIMAL') return parseDecimal(value) * 10 ** -(schema.scale || 0)\n  if (logical_type?.type === 'FLOAT16') return parseFloat16(value)\n  if (type === 'FIXED_LEN_BYTE_ARRAY') return value\n  // assert(false)\n  return value\n}\n","/**\n * @import {KeyValue, LogicalType, SchemaElement} from '../src/types.d.ts'\n * @param {SchemaElement[]} schema\n * @param {KeyValue[] | undefined} key_value_metadata\n * @returns {void}\n */\nexport function markGeoColumns(schema, key_value_metadata) {\n  // Prepare the list of GeoParquet columns\n  /** @type {Map<string, LogicalType>} */\n  const columns = new Map()\n  const geo = key_value_metadata?.find(({ key }) => key === 'geo')?.value\n  const decodedColumns = (geo && JSON.parse(geo)?.columns) ?? {}\n  for (const [name, column] of Object.entries(decodedColumns)) {\n    if (column.encoding !== 'WKB') {\n      continue\n    }\n    const type = column.edges === 'spherical' ? 'GEOGRAPHY' : 'GEOMETRY'\n    const id = column.crs?.id ?? column.crs?.ids?.[0]\n    const crs = id ? `${id.authority}:${id.code.toString()}` : undefined\n    // Note: we can't infer GEOGRAPHY's algorithm from GeoParquet\n    columns.set(name, { type, crs })\n  }\n\n  // Mark schema elements with logical type\n  // Only look at root-level columns of type BYTE_ARRAY without existing logical_type\n  for (let i = 1; i < schema.length; i++) { // skip root\n    const element = schema[i]\n    const { logical_type, name, num_children, repetition_type, type } = element\n    if (num_children) {\n      i += num_children\n      continue // skip the element and its children\n    }\n    if (type === 'BYTE_ARRAY' && logical_type === undefined && repetition_type !== 'REPEATED') {\n      element.logical_type = columns.get(name)\n    }\n  }\n}\n","import { defaultInitialFetchSize } from './metadata.js'\n\n/**\n * Replace bigint, date, etc with legal JSON types.\n *\n * @param {any} obj object to convert\n * @returns {unknown} converted object\n */\nexport function toJson(obj) {\n  if (obj === undefined) return null\n  if (typeof obj === 'bigint') return Number(obj)\n  if (Array.isArray(obj)) return obj.map(toJson)\n  if (obj instanceof Uint8Array) return Array.from(obj)\n  if (obj instanceof Date) return obj.toISOString()\n  if (obj instanceof Object) {\n    /** @type {Record<string, unknown>} */\n    const newObj = {}\n    for (const key of Object.keys(obj)) {\n      if (obj[key] === undefined) continue\n      newObj[key] = toJson(obj[key])\n    }\n    return newObj\n  }\n  return obj\n}\n\n/**\n * Concatenate two arrays fast.\n *\n * @param {any[]} aaa first array\n * @param {DecodedArray} bbb second array\n */\nexport function concat(aaa, bbb) {\n  const chunk = 10000\n  for (let i = 0; i < bbb.length; i += chunk) {\n    aaa.push(...bbb.slice(i, i + chunk))\n  }\n}\n\n/**\n * Deep equality comparison\n *\n * @param {any} a First object to compare\n * @param {any} b Second object to compare\n * @returns {boolean} true if objects are equal\n */\nexport function equals(a, b) {\n  if (a === b) return true\n  if (a instanceof Uint8Array && b instanceof Uint8Array) return equals(Array.from(a), Array.from(b))\n  if (!a || !b || typeof a !== typeof b) return false\n  return Array.isArray(a) && Array.isArray(b)\n    ? a.length === b.length && a.every((v, i) => equals(v, b[i]))\n    : typeof a === 'object' && Object.keys(a).length === Object.keys(b).length && Object.keys(a).every(k => equals(a[k], b[k]))\n}\n\n/**\n * Get the byte length of a URL using a HEAD request.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {string} url\n * @param {RequestInit} [requestInit] fetch options\n * @param {typeof globalThis.fetch} [customFetch] fetch function to use\n * @returns {Promise<number>}\n */\nexport async function byteLengthFromUrl(url, requestInit, customFetch) {\n  const fetch = customFetch ?? globalThis.fetch\n  return await fetch(url, { ...requestInit, method: 'HEAD' })\n    .then(res => {\n      if (!res.ok) throw new Error(`fetch head failed ${res.status}`)\n      const length = res.headers.get('Content-Length')\n      if (!length) throw new Error('missing content length')\n      return parseInt(length)\n    })\n}\n\n/**\n * Construct an AsyncBuffer for a URL.\n * If byteLength is not provided, will make a HEAD request to get the file size.\n * If fetch is provided, it will be used instead of the global fetch.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {object} options\n * @param {string} options.url\n * @param {number} [options.byteLength]\n * @param {typeof globalThis.fetch} [options.fetch] fetch function to use\n * @param {RequestInit} [options.requestInit]\n * @returns {Promise<AsyncBuffer>}\n */\nexport async function asyncBufferFromUrl({ url, byteLength, requestInit, fetch: customFetch }) {\n  if (!url) throw new Error('missing url')\n  const fetch = customFetch ?? globalThis.fetch\n  // byte length from HEAD request\n  byteLength ||= await byteLengthFromUrl(url, requestInit, fetch)\n\n  /**\n   * A promise for the whole buffer, if range requests are not supported.\n   * @type {Promise<ArrayBuffer>|undefined}\n   */\n  let buffer = undefined\n  const init = requestInit || {}\n\n  return {\n    byteLength,\n    async slice(start, end) {\n      if (buffer) {\n        return buffer.then(buffer => buffer.slice(start, end))\n      }\n\n      const headers = new Headers(init.headers)\n      const endStr = end === undefined ? '' : end - 1\n      headers.set('Range', `bytes=${start}-${endStr}`)\n\n      const res = await fetch(url, { ...init, headers })\n      if (!res.ok || !res.body) throw new Error(`fetch failed ${res.status}`)\n\n      if (res.status === 200) {\n        // Endpoint does not support range requests and returned the whole object\n        buffer = res.arrayBuffer()\n        return buffer.then(buffer => buffer.slice(start, end))\n      } else if (res.status === 206) {\n        // The endpoint supports range requests and sent us the requested range\n        return res.arrayBuffer()\n      } else {\n        throw new Error(`fetch received unexpected status code ${res.status}`)\n      }\n    },\n  }\n}\n\n/**\n * Returns a cached layer on top of an AsyncBuffer. For caching slices of a file\n * that are read multiple times, possibly over a network.\n *\n * @param {AsyncBuffer} file file-like object to cache\n * @param {{ minSize?: number }} [options]\n * @returns {AsyncBuffer} cached file-like object\n */\nexport function cachedAsyncBuffer({ byteLength, slice }, { minSize = defaultInitialFetchSize } = {}) {\n  if (byteLength < minSize) {\n    // Cache whole file if it's small\n    const buffer = slice(0, byteLength)\n    return {\n      byteLength,\n      async slice(start, end) {\n        return (await buffer).slice(start, end)\n      },\n    }\n  }\n  const cache = new Map()\n  return {\n    byteLength,\n    /**\n     * @param {number} start\n     * @param {number} [end]\n     * @returns {Awaitable<ArrayBuffer>}\n     */\n    slice(start, end) {\n      const key = cacheKey(start, end, byteLength)\n      const cached = cache.get(key)\n      if (cached) return cached\n      // cache miss, read from file\n      const promise = slice(start, end)\n      cache.set(key, promise)\n      return promise\n    },\n  }\n}\n\n\n/**\n * Returns canonical cache key for a byte range 'start,end'.\n * Normalize int-range and suffix-range requests to the same key.\n *\n * @import {AsyncBuffer, Awaitable, DecodedArray} from '../src/types.d.ts'\n * @param {number} start start byte of range\n * @param {number} [end] end byte of range, or undefined for suffix range\n * @param {number} [size] size of file, or undefined for suffix range\n * @returns {string}\n */\nfunction cacheKey(start, end, size) {\n  if (start < 0) {\n    if (end !== undefined) throw new Error(`invalid suffix range [${start}, ${end}]`)\n    if (size === undefined) return `${start},`\n    return `${size + start},${size}`\n  } else if (end !== undefined) {\n    if (start > end) throw new Error(`invalid empty range [${start}, ${end}]`)\n    return `${start},${end}`\n  } else if (size === undefined) {\n    return `${start},`\n  } else {\n    return `${start},${size}`\n  }\n}\n\n/**\n * Flatten a list of lists into a single list.\n *\n * @param {DecodedArray[]} [chunks]\n * @returns {DecodedArray}\n */\nexport function flatten(chunks) {\n  if (!chunks) return []\n  if (chunks.length === 1) return chunks[0]\n  /** @type {any[]} */\n  const output = []\n  for (const chunk of chunks) {\n    concat(output, chunk)\n  }\n  return output\n}\n","import { concat } from './utils.js'\n\n// Combine column chunks into a single byte range if less than 32mb\nconst columnChunkAggregation = 1 << 25 // 32mb\n\n/**\n * @import {AsyncBuffer, ByteRange, ColumnMetaData, GroupPlan, ParquetReadOptions, QueryPlan} from '../src/types.js'\n */\n/**\n * Plan which byte ranges to read to satisfy a read request.\n * Metadata must be non-null.\n *\n * @param {ParquetReadOptions} options\n * @returns {QueryPlan}\n */\nexport function parquetPlan({ metadata, rowStart = 0, rowEnd = Infinity, columns }) {\n  if (!metadata) throw new Error('parquetPlan requires metadata')\n  /** @type {GroupPlan[]} */\n  const groups = []\n  /** @type {ByteRange[]} */\n  const fetches = []\n\n  // find which row groups to read\n  let groupStart = 0 // first row index of the current group\n  for (const rowGroup of metadata.row_groups) {\n    const groupRows = Number(rowGroup.num_rows)\n    const groupEnd = groupStart + groupRows\n    // if row group overlaps with row range, add it to the plan\n    if (groupRows > 0 && groupEnd >= rowStart && groupStart < rowEnd) {\n      /** @type {ByteRange[]} */\n      const ranges = []\n      // loop through each column chunk\n      for (const { file_path, meta_data } of rowGroup.columns) {\n        if (file_path) throw new Error('parquet file_path not supported')\n        if (!meta_data) throw new Error('parquet column metadata is undefined')\n        // add included columns to the plan\n        if (!columns || columns.includes(meta_data.path_in_schema[0])) {\n          ranges.push(getColumnRange(meta_data))\n        }\n      }\n      const selectStart = Math.max(rowStart - groupStart, 0)\n      const selectEnd = Math.min(rowEnd - groupStart, groupRows)\n      groups.push({ ranges, rowGroup, groupStart, groupRows, selectStart, selectEnd })\n\n      // map group plan to ranges\n      const groupSize = ranges[ranges.length - 1]?.endByte - ranges[0]?.startByte\n      if (!columns && groupSize < columnChunkAggregation) {\n        // full row group\n        fetches.push({\n          startByte: ranges[0].startByte,\n          endByte: ranges[ranges.length - 1].endByte,\n        })\n      } else if (ranges.length) {\n        concat(fetches, ranges)\n      } else if (columns?.length) {\n        throw new Error(`parquet columns not found: ${columns.join(', ')}`)\n      }\n    }\n\n    groupStart = groupEnd\n  }\n  if (!isFinite(rowEnd)) rowEnd = groupStart\n\n  return { metadata, rowStart, rowEnd, columns, fetches, groups }\n}\n\n/**\n * @param {ColumnMetaData} columnMetadata\n * @returns {ByteRange}\n */\nexport function getColumnRange({ dictionary_page_offset, data_page_offset, total_compressed_size }) {\n  const columnOffset = dictionary_page_offset || data_page_offset\n  return {\n    startByte: Number(columnOffset),\n    endByte: Number(columnOffset + total_compressed_size),\n  }\n}\n\n/**\n * Prefetch byte ranges from an AsyncBuffer.\n *\n * @param {AsyncBuffer} file\n * @param {QueryPlan} plan\n * @returns {AsyncBuffer}\n */\nexport function prefetchAsyncBuffer(file, { fetches }) {\n  // fetch byte ranges from the file\n  const promises = fetches.map(({ startByte, endByte }) => file.slice(startByte, endByte))\n  return {\n    byteLength: file.byteLength,\n    slice(start, end = file.byteLength) {\n      // find matching slice\n      const index = fetches.findIndex(({ startByte, endByte }) => startByte <= start && end <= endByte)\n      if (index < 0) throw new Error(`no prefetch for range [${start}, ${end}]`)\n      if (fetches[index].startByte !== start || fetches[index].endByte !== end) {\n        // slice a subrange of the prefetch\n        const startOffset = start - fetches[index].startByte\n        const endOffset = end - fetches[index].startByte\n        if (promises[index] instanceof Promise) {\n          return promises[index].then(buffer => buffer.slice(startOffset, endOffset))\n        } else {\n          return promises[index].slice(startOffset, endOffset)\n        }\n      } else {\n        return promises[index]\n      }\n    },\n  }\n}\n","import { getMaxDefinitionLevel, isListLike, isMapLike } from './schema.js'\n\n/**\n * Reconstructs a complex nested structure from flat arrays of values and\n * definition and repetition levels, according to Dremel encoding.\n *\n * @param {any[]} output\n * @param {number[] | undefined} definitionLevels\n * @param {number[]} repetitionLevels\n * @param {DecodedArray} values\n * @param {SchemaTree[]} schemaPath\n * @returns {DecodedArray}\n */\nexport function assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath) {\n  const n = definitionLevels?.length || repetitionLevels.length\n  if (!n) return values\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  const repetitionPath = schemaPath.map(({ element }) => element.repetition_type)\n  let valueIndex = 0\n\n  // Track state of nested structures\n  const containerStack = [output]\n  let currentContainer = output\n  let currentDepth = 0 // schema depth\n  let currentDefLevel = 0 // list depth\n  let currentRepLevel = 0\n\n  if (repetitionLevels[0]) {\n    // continue previous row\n    while (currentDepth < repetitionPath.length - 2 && currentRepLevel < repetitionLevels[0]) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        // go into last list\n        currentContainer = currentContainer.at(-1)\n        containerStack.push(currentContainer)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n  }\n\n  for (let i = 0; i < n; i++) {\n    // assert(currentDefLevel === containerStack.length - 1)\n    const def = definitionLevels?.length ? definitionLevels[i] : maxDefinitionLevel\n    const rep = repetitionLevels[i]\n\n    // Pop up to start of rep level\n    while (currentDepth && (rep < currentRepLevel || repetitionPath[currentDepth] !== 'REPEATED')) {\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        containerStack.pop()\n        currentDefLevel--\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel--\n      currentDepth--\n    }\n    // @ts-expect-error won't be empty\n    currentContainer = containerStack.at(-1)\n\n    // Go deeper to end of definition level\n    while (\n      (currentDepth < repetitionPath.length - 2 || repetitionPath[currentDepth + 1] === 'REPEATED') &&\n      (currentDefLevel < def || repetitionPath[currentDepth + 1] === 'REQUIRED')\n    ) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        /** @type {any[]} */\n        const newList = []\n        currentContainer.push(newList)\n        currentContainer = newList\n        containerStack.push(newList)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n\n    // Add value or null based on definition level\n    if (def === maxDefinitionLevel) {\n      // assert(currentDepth === maxDefinitionLevel || currentDepth === repetitionPath.length - 2)\n      currentContainer.push(values[valueIndex++])\n    } else if (currentDepth === repetitionPath.length - 2) {\n      currentContainer.push(null)\n    } else {\n      currentContainer.push([])\n    }\n  }\n\n  // Handle edge cases for empty inputs or single-level data\n  if (!output.length) {\n    // return max definition level of nested lists\n    for (let i = 0; i < maxDefinitionLevel; i++) {\n      /** @type {any[]} */\n      const newList = []\n      currentContainer.push(newList)\n      currentContainer = newList\n    }\n  }\n\n  return output\n}\n\n/**\n * Assemble a nested structure from subcolumn data.\n * https://github.com/apache/parquet-format/blob/apache-parquet-format-2.10.0/LogicalTypes.md#nested-types\n *\n * @param {Map<string, DecodedArray>} subcolumnData\n * @param {SchemaTree} schema top-level schema element\n * @param {number} [depth] depth of nested structure\n */\nexport function assembleNested(subcolumnData, schema, depth = 0) {\n  const path = schema.path.join('.')\n  const optional = schema.element.repetition_type === 'OPTIONAL'\n  const nextDepth = optional ? depth + 1 : depth\n\n  if (isListLike(schema)) {\n    let sublist = schema.children[0]\n    let subDepth = nextDepth\n    if (sublist.children.length === 1) {\n      sublist = sublist.children[0]\n      subDepth++\n    }\n    assembleNested(subcolumnData, sublist, subDepth)\n\n    const subcolumn = sublist.path.join('.')\n    const values = subcolumnData.get(subcolumn)\n    if (!values) throw new Error('parquet list column missing values')\n    if (optional) flattenAtDepth(values, depth)\n    subcolumnData.set(path, values)\n    subcolumnData.delete(subcolumn)\n    return\n  }\n\n  if (isMapLike(schema)) {\n    const mapName = schema.children[0].element.name\n\n    // Assemble keys and values\n    assembleNested(subcolumnData, schema.children[0].children[0], nextDepth + 1)\n    assembleNested(subcolumnData, schema.children[0].children[1], nextDepth + 1)\n\n    const keys = subcolumnData.get(`${path}.${mapName}.key`)\n    const values = subcolumnData.get(`${path}.${mapName}.value`)\n\n    if (!keys) throw new Error('parquet map column missing keys')\n    if (!values) throw new Error('parquet map column missing values')\n    if (keys.length !== values.length) {\n      throw new Error('parquet map column key/value length mismatch')\n    }\n\n    const out = assembleMaps(keys, values, nextDepth)\n    if (optional) flattenAtDepth(out, depth)\n\n    subcolumnData.delete(`${path}.${mapName}.key`)\n    subcolumnData.delete(`${path}.${mapName}.value`)\n    subcolumnData.set(path, out)\n    return\n  }\n\n  // Struct-like column\n  if (schema.children.length) {\n    // construct a meta struct and then invert\n    const invertDepth = schema.element.repetition_type === 'REQUIRED' ? depth : depth + 1\n    /** @type {Record<string, any>} */\n    const struct = {}\n    for (const child of schema.children) {\n      assembleNested(subcolumnData, child, invertDepth)\n      const childData = subcolumnData.get(child.path.join('.'))\n      if (!childData) throw new Error('parquet struct missing child data')\n      struct[child.element.name] = childData\n    }\n    // remove children\n    for (const child of schema.children) {\n      subcolumnData.delete(child.path.join('.'))\n    }\n    // invert struct by depth\n    const inverted = invertStruct(struct, invertDepth)\n    if (optional) flattenAtDepth(inverted, depth)\n    subcolumnData.set(path, inverted)\n  }\n}\n\n/**\n * @import {DecodedArray, SchemaTree} from '../src/types.d.ts'\n * @param {DecodedArray} arr\n * @param {number} depth\n */\nfunction flattenAtDepth(arr, depth) {\n  for (let i = 0; i < arr.length; i++) {\n    if (depth) {\n      flattenAtDepth(arr[i], depth - 1)\n    } else {\n      arr[i] = arr[i][0]\n    }\n  }\n}\n\n/**\n * @param {DecodedArray} keys\n * @param {DecodedArray} values\n * @param {number} depth\n * @returns {any[]}\n */\nfunction assembleMaps(keys, values, depth) {\n  const out = []\n  for (let i = 0; i < keys.length; i++) {\n    if (depth) {\n      out.push(assembleMaps(keys[i], values[i], depth - 1)) // go deeper\n    } else {\n      if (keys[i]) {\n        /** @type {Record<string, any>} */\n        const obj = {}\n        for (let j = 0; j < keys[i].length; j++) {\n          const value = values[i][j]\n          obj[keys[i][j]] = value === undefined ? null : value\n        }\n        out.push(obj)\n      } else {\n        out.push(undefined)\n      }\n    }\n  }\n  return out\n}\n\n/**\n * Invert a struct-like object by depth.\n *\n * @param {Record<string, any[]>} struct\n * @param {number} depth\n * @returns {any[]}\n */\nfunction invertStruct(struct, depth) {\n  const keys = Object.keys(struct)\n  const length = struct[keys[0]]?.length\n  const out = []\n  for (let i = 0; i < length; i++) {\n    /** @type {Record<string, any>} */\n    const obj = {}\n    for (const key of keys) {\n      if (struct[key].length !== length) throw new Error('parquet struct parsing error')\n      obj[key] = struct[key][i]\n    }\n    if (depth) {\n      out.push(invertStruct(obj, depth - 1)) // deeper\n    } else {\n      out.push(obj)\n    }\n  }\n  return out\n}\n","import { readVarInt, readZigZagBigInt } from './thrift.js'\n\n/**\n * @import {DataReader} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} count number of values to read\n * @param {Int32Array | BigInt64Array} output\n */\nexport function deltaBinaryUnpack(reader, count, output) {\n  const int32 = output instanceof Int32Array\n  const blockSize = readVarInt(reader)\n  const miniblockPerBlock = readVarInt(reader)\n  readVarInt(reader) // assert(=== count)\n  let value = readZigZagBigInt(reader) // first value\n  let outputIndex = 0\n  output[outputIndex++] = int32 ? Number(value) : value\n\n  const valuesPerMiniblock = blockSize / miniblockPerBlock\n\n  while (outputIndex < count) {\n    // new block\n    const minDelta = readZigZagBigInt(reader)\n    const bitWidths = new Uint8Array(miniblockPerBlock)\n    for (let i = 0; i < miniblockPerBlock; i++) {\n      bitWidths[i] = reader.view.getUint8(reader.offset++)\n    }\n\n    for (let i = 0; i < miniblockPerBlock && outputIndex < count; i++) {\n      // new miniblock\n      const bitWidth = BigInt(bitWidths[i])\n      if (bitWidth) {\n        let bitpackPos = 0n\n        let miniblockCount = valuesPerMiniblock\n        const mask = (1n << bitWidth) - 1n\n        while (miniblockCount && outputIndex < count) {\n          let bits = BigInt(reader.view.getUint8(reader.offset)) >> bitpackPos & mask // TODO: don't re-read value every time\n          bitpackPos += bitWidth\n          while (bitpackPos >= 8) {\n            bitpackPos -= 8n\n            reader.offset++\n            if (bitpackPos) {\n              bits |= BigInt(reader.view.getUint8(reader.offset)) << bitWidth - bitpackPos & mask\n            }\n          }\n          const delta = minDelta + bits\n          value += delta\n          output[outputIndex++] = int32 ? Number(value) : value\n          miniblockCount--\n        }\n        if (miniblockCount) {\n          // consume leftover miniblock\n          reader.offset += Math.ceil((miniblockCount * Number(bitWidth) + Number(bitpackPos)) / 8)\n        }\n      } else {\n        for (let j = 0; j < valuesPerMiniblock && outputIndex < count; j++) {\n          value += minDelta\n          output[outputIndex++] = int32 ? Number(value) : value\n        }\n      }\n    }\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaLengthByteArray(reader, count, output) {\n  const lengths = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, lengths)\n  for (let i = 0; i < count; i++) {\n    output[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, lengths[i])\n    reader.offset += lengths[i]\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaByteArray(reader, count, output) {\n  const prefixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, prefixData)\n  const suffixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, suffixData)\n\n  for (let i = 0; i < count; i++) {\n    const suffix = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, suffixData[i])\n    if (prefixData[i]) {\n      // copy from previous value\n      output[i] = new Uint8Array(prefixData[i] + suffixData[i])\n      output[i].set(output[i - 1].subarray(0, prefixData[i]))\n      output[i].set(suffix, prefixData[i])\n    } else {\n      output[i] = suffix\n    }\n    reader.offset += suffixData[i]\n  }\n}\n","import { readVarInt } from './thrift.js'\n\n/**\n * Minimum bits needed to store value.\n *\n * @param {number} value\n * @returns {number}\n */\nexport function bitWidth(value) {\n  return 32 - Math.clz32(value)\n}\n\n/**\n * Read values from a run-length encoded/bit-packed hybrid encoding.\n *\n * If length is zero, then read int32 length at the start.\n *\n * @param {DataReader} reader\n * @param {number} width - bitwidth\n * @param {DecodedArray} output\n * @param {number} [length] - length of the encoded data\n */\nexport function readRleBitPackedHybrid(reader, width, output, length) {\n  if (length === undefined) {\n    length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n  }\n  const startOffset = reader.offset\n  let seen = 0\n  while (seen < output.length) {\n    const header = readVarInt(reader)\n    if (header & 1) {\n      // bit-packed\n      seen = readBitPacked(reader, header, width, output, seen)\n    } else {\n      // rle\n      const count = header >>> 1\n      readRle(reader, count, width, output, seen)\n      seen += count\n    }\n  }\n  reader.offset = startOffset + length // duckdb writes an empty block\n}\n\n/**\n * Run-length encoding: read value with bitWidth and repeat it count times.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n */\nfunction readRle(reader, count, bitWidth, output, seen) {\n  const width = bitWidth + 7 >> 3\n  let value = 0\n  for (let i = 0; i < width; i++) {\n    value |= reader.view.getUint8(reader.offset++) << (i << 3)\n  }\n  // assert(value < 1 << bitWidth)\n\n  // repeat value count times\n  for (let i = 0; i < count; i++) {\n    output[seen + i] = value\n  }\n}\n\n/**\n * Read a bit-packed run of the rle/bitpack hybrid.\n * Supports width > 8 (crossing bytes).\n *\n * @param {DataReader} reader\n * @param {number} header - bit-pack header\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n * @returns {number} total output values so far\n */\nfunction readBitPacked(reader, header, bitWidth, output, seen) {\n  let count = header >> 1 << 3 // values to read\n  const mask = (1 << bitWidth) - 1\n\n  let data = 0\n  if (reader.offset < reader.view.byteLength) {\n    data = reader.view.getUint8(reader.offset++)\n  } else if (mask) {\n    // sometimes out-of-bounds reads are masked out\n    throw new Error(`parquet bitpack offset ${reader.offset} out of range`)\n  }\n  let left = 8\n  let right = 0\n\n  // read values\n  while (count) {\n    // if we have crossed a byte boundary, shift the data\n    if (right > 8) {\n      right -= 8\n      left -= 8\n      data >>>= 8\n    } else if (left - right < bitWidth) {\n      // if we don't have bitWidth number of bits to read, read next byte\n      data |= reader.view.getUint8(reader.offset) << left\n      reader.offset++\n      left += 8\n    } else {\n      if (seen < output.length) {\n        // emit value\n        output[seen++] = data >> right & mask\n      }\n      count--\n      right += bitWidth\n    }\n  }\n\n  return seen\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {DecodedArray}\n */\nexport function byteStreamSplit(reader, count, type, typeLength) {\n  const width = byteWidth(type, typeLength)\n  const bytes = new Uint8Array(count * width)\n  for (let b = 0; b < width; b++) {\n    for (let i = 0; i < count; i++) {\n      bytes[i * width + b] = reader.view.getUint8(reader.offset++)\n    }\n  }\n  // interpret bytes as typed array\n  if (type === 'FLOAT') return new Float32Array(bytes.buffer)\n  else if (type === 'DOUBLE') return new Float64Array(bytes.buffer)\n  else if (type === 'INT32') return new Int32Array(bytes.buffer)\n  else if (type === 'INT64') return new BigInt64Array(bytes.buffer)\n  else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    // split into arrays of typeLength\n    const split = new Array(count)\n    for (let i = 0; i < count; i++) {\n      split[i] = bytes.subarray(i * width, (i + 1) * width)\n    }\n    return split\n  }\n  throw new Error(`parquet byte_stream_split unsupported type: ${type}`)\n}\n\n/**\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {number}\n */\nfunction byteWidth(type, typeLength) {\n  switch (type) {\n  case 'INT32':\n  case 'FLOAT':\n    return 4\n  case 'INT64':\n  case 'DOUBLE':\n    return 8\n  case 'FIXED_LEN_BYTE_ARRAY':\n    if (!typeLength) throw new Error('parquet byteWidth missing type_length')\n    return typeLength\n  default:\n    throw new Error(`parquet unsupported type: ${type}`)\n  }\n}\n","/**\n * Read `count` values of the given type from the reader.view.\n *\n * @param {DataReader} reader - buffer to read data from\n * @param {ParquetType} type - parquet type of the data\n * @param {number} count - number of values to read\n * @param {number | undefined} fixedLength - length of each fixed length byte array\n * @returns {DecodedArray} array of values\n */\nexport function readPlain(reader, type, count, fixedLength) {\n  if (count === 0) return []\n  if (type === 'BOOLEAN') {\n    return readPlainBoolean(reader, count)\n  } else if (type === 'INT32') {\n    return readPlainInt32(reader, count)\n  } else if (type === 'INT64') {\n    return readPlainInt64(reader, count)\n  } else if (type === 'INT96') {\n    return readPlainInt96(reader, count)\n  } else if (type === 'FLOAT') {\n    return readPlainFloat(reader, count)\n  } else if (type === 'DOUBLE') {\n    return readPlainDouble(reader, count)\n  } else if (type === 'BYTE_ARRAY') {\n    return readPlainByteArray(reader, count)\n  } else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    if (!fixedLength) throw new Error('parquet missing fixed length')\n    return readPlainByteArrayFixed(reader, count, fixedLength)\n  } else {\n    throw new Error(`parquet unhandled type: ${type}`)\n  }\n}\n\n/**\n * Read `count` boolean values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {boolean[]}\n */\nfunction readPlainBoolean(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const byteOffset = reader.offset + (i / 8 | 0)\n    const bitOffset = i % 8\n    const byte = reader.view.getUint8(byteOffset)\n    values[i] = (byte & 1 << bitOffset) !== 0\n  }\n  reader.offset += Math.ceil(count / 8)\n  return values\n}\n\n/**\n * Read `count` int32 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Int32Array}\n */\nfunction readPlainInt32(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Int32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Int32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` int64 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {BigInt64Array}\n */\nfunction readPlainInt64(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new BigInt64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new BigInt64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` int96 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {bigint[]}\n */\nfunction readPlainInt96(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const low = reader.view.getBigInt64(reader.offset + i * 12, true)\n    const high = reader.view.getInt32(reader.offset + i * 12 + 8, true)\n    values[i] = BigInt(high) << 64n | low\n  }\n  reader.offset += count * 12\n  return values\n}\n\n/**\n * Read `count` float values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float32Array}\n */\nfunction readPlainFloat(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Float32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Float32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` double values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float64Array}\n */\nfunction readPlainDouble(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new Float64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new Float64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` byte array values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArray(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, length)\n    reader.offset += length\n  }\n  return values\n}\n\n/**\n * Read a fixed length byte array.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} fixedLength\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArrayFixed(reader, count, fixedLength) {\n  // assert(reader.view.byteLength - reader.offset >= count * fixedLength)\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, fixedLength)\n    reader.offset += fixedLength\n  }\n  return values\n}\n\n/**\n * Create a new buffer with the offset and size.\n *\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ArrayBufferLike} buffer\n * @param {number} offset\n * @param {number} size\n * @returns {ArrayBuffer}\n */\nfunction align(buffer, offset, size) {\n  const aligned = new ArrayBuffer(size)\n  new Uint8Array(aligned).set(new Uint8Array(buffer, offset, size))\n  return aligned\n}\n","/**\n * The MIT License (MIT)\n * Copyright (c) 2016 Zhipeng Jia\n * https://github.com/zhipeng-jia/snappyjs\n */\n\nconst WORD_MASK = [0, 0xff, 0xffff, 0xffffff, 0xffffffff]\n\n/**\n * Copy bytes from one array to another\n *\n * @param {Uint8Array} fromArray source array\n * @param {number} fromPos source position\n * @param {Uint8Array} toArray destination array\n * @param {number} toPos destination position\n * @param {number} length number of bytes to copy\n */\nfunction copyBytes(fromArray, fromPos, toArray, toPos, length) {\n  for (let i = 0; i < length; i++) {\n    toArray[toPos + i] = fromArray[fromPos + i]\n  }\n}\n\n/**\n * Decompress snappy data.\n * Accepts an output buffer to avoid allocating a new buffer for each call.\n *\n * @param {Uint8Array} input compressed data\n * @param {Uint8Array} output output buffer\n */\nexport function snappyUncompress(input, output) {\n  const inputLength = input.byteLength\n  const outputLength = output.byteLength\n  let pos = 0\n  let outPos = 0\n\n  // skip preamble (contains uncompressed length as varint)\n  while (pos < inputLength) {\n    const c = input[pos]\n    pos++\n    if (c < 128) {\n      break\n    }\n  }\n  if (outputLength && pos >= inputLength) {\n    throw new Error('invalid snappy length header')\n  }\n\n  while (pos < inputLength) {\n    const c = input[pos]\n    let len = 0\n    pos++\n\n    if (pos >= inputLength) {\n      throw new Error('missing eof marker')\n    }\n\n    // There are two types of elements, literals and copies (back references)\n    if ((c & 0x3) === 0) {\n      // Literals are uncompressed data stored directly in the byte stream\n      let len = (c >>> 2) + 1\n      // Longer literal length is encoded in multiple bytes\n      if (len > 60) {\n        if (pos + 3 >= inputLength) {\n          throw new Error('snappy error literal pos + 3 >= inputLength')\n        }\n        const lengthSize = len - 60 // length bytes - 1\n        len = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        len = (len & WORD_MASK[lengthSize]) + 1\n        pos += lengthSize\n      }\n      if (pos + len > inputLength) {\n        throw new Error('snappy error literal exceeds input length')\n      }\n      copyBytes(input, pos, output, outPos, len)\n      pos += len\n      outPos += len\n    } else {\n      // Copy elements\n      let offset = 0 // offset back from current position to read\n      switch (c & 0x3) {\n      case 1:\n        // Copy with 1-byte offset\n        len = (c >>> 2 & 0x7) + 4\n        offset = input[pos] + (c >>> 5 << 8)\n        pos++\n        break\n      case 2:\n        // Copy with 2-byte offset\n        if (inputLength <= pos + 1) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos] + (input[pos + 1] << 8)\n        pos += 2\n        break\n      case 3:\n        // Copy with 4-byte offset\n        if (inputLength <= pos + 3) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        pos += 4\n        break\n      default:\n        break\n      }\n      if (offset === 0 || isNaN(offset)) {\n        throw new Error(`invalid offset ${offset} pos ${pos} inputLength ${inputLength}`)\n      }\n      if (offset > outPos) {\n        throw new Error('cannot copy from before start of buffer')\n      }\n      copyBytes(output, outPos - offset, output, outPos, len)\n      outPos += len\n    }\n  }\n\n  if (outPos !== outputLength) throw new Error('premature end of input')\n}\n","import { deltaBinaryUnpack, deltaByteArray, deltaLengthByteArray } from './delta.js'\nimport { bitWidth, byteStreamSplit, readRleBitPackedHybrid } from './encoding.js'\nimport { readPlain } from './plain.js'\nimport { getMaxDefinitionLevel, getMaxRepetitionLevel } from './schema.js'\nimport { snappyUncompress } from './snappy.js'\n\n/**\n * Read a data page from uncompressed reader.\n *\n * @param {Uint8Array} bytes raw page data (should already be decompressed)\n * @param {DataPageHeader} daph data page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPage(bytes, daph, { type, element, schemaPath }) {\n  const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength)\n  const reader = { view, offset: 0 }\n  /** @type {DecodedArray} */\n  let dataPage\n\n  // repetition and definition levels\n  const repetitionLevels = readRepetitionLevels(reader, daph, schemaPath)\n  // assert(!repetitionLevels.length || repetitionLevels.length === daph.num_values)\n  const { definitionLevels, numNulls } = readDefinitionLevels(reader, daph, schemaPath)\n  // assert(!definitionLevels.length || definitionLevels.length === daph.num_values)\n\n  // read values based on encoding\n  const nValues = daph.num_values - numNulls\n  if (daph.encoding === 'PLAIN') {\n    dataPage = readPlain(reader, type, nValues, element.type_length)\n  } else if (\n    daph.encoding === 'PLAIN_DICTIONARY' ||\n    daph.encoding === 'RLE_DICTIONARY' ||\n    daph.encoding === 'RLE'\n  ) {\n    const bitWidth = type === 'BOOLEAN' ? 1 : view.getUint8(reader.offset++)\n    if (bitWidth) {\n      dataPage = new Array(nValues)\n      if (type === 'BOOLEAN') {\n        readRleBitPackedHybrid(reader, bitWidth, dataPage)\n        dataPage = dataPage.map(x => !!x) // convert to boolean\n      } else {\n        // assert(daph.encoding.endsWith('_DICTIONARY'))\n        readRleBitPackedHybrid(reader, bitWidth, dataPage, view.byteLength - reader.offset)\n      }\n    } else {\n      dataPage = new Uint8Array(nValues) // nValue zeroes\n    }\n  } else if (daph.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(reader, nValues, type, element.type_length)\n  } else if (daph.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(reader, nValues, dataPage)\n  } else if (daph.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(reader, nValues, dataPage)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @import {ColumnDecoder, CompressionCodec, Compressors, DataPage, DataPageHeader, DataPageHeaderV2, DataReader, DecodedArray, PageHeader, SchemaTree} from '../src/types.d.ts'\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels and number of bytes read\n */\nfunction readRepetitionLevels(reader, daph, schemaPath) {\n  if (schemaPath.length > 1) {\n    const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n    if (maxRepetitionLevel) {\n      const values = new Array(daph.num_values)\n      readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values)\n      return values\n    }\n  }\n  return []\n}\n\n/**\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {{ definitionLevels: number[], numNulls: number }} definition levels\n */\nfunction readDefinitionLevels(reader, daph, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (!maxDefinitionLevel) return { definitionLevels: [], numNulls: 0 }\n\n  const definitionLevels = new Array(daph.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), definitionLevels)\n\n  // count nulls\n  let numNulls = daph.num_values\n  for (const def of definitionLevels) {\n    if (def === maxDefinitionLevel) numNulls--\n  }\n  if (numNulls === 0) definitionLevels.length = 0\n\n  return { definitionLevels, numNulls }\n}\n\n/**\n * @param {Uint8Array} compressedBytes\n * @param {number} uncompressed_page_size\n * @param {CompressionCodec} codec\n * @param {Compressors | undefined} compressors\n * @returns {Uint8Array}\n */\nexport function decompressPage(compressedBytes, uncompressed_page_size, codec, compressors) {\n  /** @type {Uint8Array} */\n  let page\n  const customDecompressor = compressors?.[codec]\n  if (codec === 'UNCOMPRESSED') {\n    page = compressedBytes\n  } else if (customDecompressor) {\n    page = customDecompressor(compressedBytes, uncompressed_page_size)\n  } else if (codec === 'SNAPPY') {\n    page = new Uint8Array(uncompressed_page_size)\n    snappyUncompress(compressedBytes, page)\n  } else {\n    throw new Error(`parquet unsupported compression codec: ${codec}`)\n  }\n  if (page?.length !== uncompressed_page_size) {\n    throw new Error(`parquet decompressed page length ${page?.length} does not match header ${uncompressed_page_size}`)\n  }\n  return page\n}\n\n\n/**\n * Read a data page from the given Uint8Array.\n *\n * @param {Uint8Array} compressedBytes raw page data\n * @param {PageHeader} ph page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPageV2(compressedBytes, ph, columnDecoder) {\n  const view = new DataView(compressedBytes.buffer, compressedBytes.byteOffset, compressedBytes.byteLength)\n  const reader = { view, offset: 0 }\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  const daph2 = ph.data_page_header_v2\n  if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n  // repetition levels\n  const repetitionLevels = readRepetitionLevelsV2(reader, daph2, schemaPath)\n  reader.offset = daph2.repetition_levels_byte_length // readVarInt() => len for boolean v2?\n\n  // definition levels\n  const definitionLevels = readDefinitionLevelsV2(reader, daph2, schemaPath)\n  // assert(reader.offset === daph2.repetition_levels_byte_length + daph2.definition_levels_byte_length)\n\n  const uncompressedPageSize = ph.uncompressed_page_size - daph2.definition_levels_byte_length - daph2.repetition_levels_byte_length\n\n  let page = compressedBytes.subarray(reader.offset)\n  if (daph2.is_compressed !== false) {\n    page = decompressPage(page, uncompressedPageSize, codec, compressors)\n  }\n  const pageView = new DataView(page.buffer, page.byteOffset, page.byteLength)\n  const pageReader = { view: pageView, offset: 0 }\n\n  // read values based on encoding\n  /** @type {DecodedArray} */\n  let dataPage\n  const nValues = daph2.num_values - daph2.num_nulls\n  if (daph2.encoding === 'PLAIN') {\n    dataPage = readPlain(pageReader, type, nValues, element.type_length)\n  } else if (daph2.encoding === 'RLE') {\n    // assert(type === 'BOOLEAN')\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, 1, dataPage)\n    dataPage = dataPage.map(x => !!x)\n  } else if (\n    daph2.encoding === 'PLAIN_DICTIONARY' ||\n    daph2.encoding === 'RLE_DICTIONARY'\n  ) {\n    const bitWidth = pageView.getUint8(pageReader.offset++)\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, bitWidth, dataPage, uncompressedPageSize - 1)\n  } else if (daph2.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(reader, nValues, type, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph2.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels\n */\nfunction readRepetitionLevelsV2(reader, daph2, schemaPath) {\n  const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n  if (!maxRepetitionLevel) return []\n\n  const values = new Array(daph2.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values, daph2.repetition_levels_byte_length)\n  return values\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {number[] | undefined} definition levels\n */\nfunction readDefinitionLevelsV2(reader, daph2, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (maxDefinitionLevel) {\n    // V2 we know the length\n    const values = new Array(daph2.num_values)\n    readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), values, daph2.definition_levels_byte_length)\n    return values\n  }\n}\n","import { assembleLists } from './assemble.js'\nimport { Encoding, PageType } from './constants.js'\nimport { convert, convertWithDictionary } from './convert.js'\nimport { decompressPage, readDataPage, readDataPageV2 } from './datapage.js'\nimport { readPlain } from './plain.js'\nimport { isFlatColumn } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\n\n/**\n * Parse column data from a buffer.\n *\n * @param {DataReader} reader\n * @param {RowGroupSelect} rowGroupSelect row group selection\n * @param {ColumnDecoder} columnDecoder column decoder params\n * @param {(chunk: ColumnData) => void} [onPage] callback for each page\n * @returns {DecodedArray[]}\n */\nexport function readColumn(reader, { groupStart, selectStart, selectEnd }, columnDecoder, onPage) {\n  const { columnName, schemaPath } = columnDecoder\n  const isFlat = isFlatColumn(schemaPath)\n  /** @type {DecodedArray[]} */\n  const chunks = []\n  /** @type {DecodedArray | undefined} */\n  let dictionary = undefined\n  /** @type {DecodedArray | undefined} */\n  let lastChunk = undefined\n  let rowCount = 0\n\n  const emitLastChunk = onPage && (() => {\n    lastChunk && onPage({\n      columnName,\n      columnData: lastChunk,\n      rowStart: groupStart + rowCount - lastChunk.length,\n      rowEnd: groupStart + rowCount,\n    })\n  })\n\n  while (isFlat ? rowCount < selectEnd : reader.offset < reader.view.byteLength - 1) {\n    if (reader.offset >= reader.view.byteLength - 1) break // end of reader\n\n    // read page header\n    const header = parquetHeader(reader)\n    if (header.type === 'DICTIONARY_PAGE') {\n      // assert(!dictionary)\n      dictionary = readPage(reader, header, columnDecoder, dictionary, undefined, 0)\n      dictionary = convert(dictionary, columnDecoder)\n    } else {\n      const lastChunkLength = lastChunk?.length || 0\n      const values = readPage(reader, header, columnDecoder, dictionary, lastChunk, selectStart - rowCount)\n      if (lastChunk === values) {\n        // continued from previous page\n        rowCount += values.length - lastChunkLength\n      } else {\n        emitLastChunk?.()\n        chunks.push(values)\n        rowCount += values.length\n        lastChunk = values\n      }\n    }\n  }\n  emitLastChunk?.()\n  // assert(rowCount >= selectEnd)\n  if (rowCount > selectEnd && lastChunk) {\n    // truncate last chunk to row limit\n    chunks[chunks.length - 1] = lastChunk.slice(0, selectEnd - (rowCount - lastChunk.length))\n  }\n  return chunks\n}\n\n/**\n * Read a page (data or dictionary) from a buffer.\n *\n * @param {DataReader} reader\n * @param {PageHeader} header\n * @param {ColumnDecoder} columnDecoder\n * @param {DecodedArray | undefined} dictionary\n * @param {DecodedArray | undefined} previousChunk\n * @param {number} pageStart skip this many rows in the page\n * @returns {DecodedArray}\n */\nexport function readPage(reader, header, columnDecoder, dictionary, previousChunk, pageStart) {\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  // read compressed_page_size bytes\n  const compressedBytes = new Uint8Array(\n    reader.view.buffer, reader.view.byteOffset + reader.offset, header.compressed_page_size\n  )\n  reader.offset += header.compressed_page_size\n\n  // parse page data by type\n  if (header.type === 'DATA_PAGE') {\n    const daph = header.data_page_header\n    if (!daph) throw new Error('parquet data page header is undefined')\n\n    // skip unnecessary non-nested pages\n    if (pageStart > daph.num_values && isFlatColumn(schemaPath)) {\n      return new Array(daph.num_values) // TODO: don't allocate array\n    }\n\n    const page = decompressPage(compressedBytes, Number(header.uncompressed_page_size), codec, compressors)\n    const { definitionLevels, repetitionLevels, dataPage } = readDataPage(page, daph, columnDecoder)\n    // assert(!daph.statistics?.null_count || daph.statistics.null_count === BigInt(daph.num_values - dataPage.length))\n\n    // convert types, dereference dictionary, and assemble lists\n    let values = convertWithDictionary(dataPage, dictionary, daph.encoding, columnDecoder)\n    if (repetitionLevels.length || definitionLevels?.length) {\n      const output = Array.isArray(previousChunk) ? previousChunk : []\n      return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n    } else {\n      // wrap nested flat data by depth\n      for (let i = 2; i < schemaPath.length; i++) {\n        if (schemaPath[i].element.repetition_type !== 'REQUIRED') {\n          values = Array.from(values, e => [e])\n        }\n      }\n      return values\n    }\n  } else if (header.type === 'DATA_PAGE_V2') {\n    const daph2 = header.data_page_header_v2\n    if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n    // skip unnecessary pages\n    if (pageStart > daph2.num_rows) {\n      return new Array(daph2.num_values) // TODO: don't allocate array\n    }\n\n    const { definitionLevels, repetitionLevels, dataPage } =\n      readDataPageV2(compressedBytes, header, columnDecoder)\n\n    // convert types, dereference dictionary, and assemble lists\n    const values = convertWithDictionary(dataPage, dictionary, daph2.encoding, columnDecoder)\n    const output = Array.isArray(previousChunk) ? previousChunk : []\n    return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n  } else if (header.type === 'DICTIONARY_PAGE') {\n    const diph = header.dictionary_page_header\n    if (!diph) throw new Error('parquet dictionary page header is undefined')\n\n    const page = decompressPage(\n      compressedBytes, Number(header.uncompressed_page_size), codec, compressors\n    )\n\n    const reader = { view: new DataView(page.buffer, page.byteOffset, page.byteLength), offset: 0 }\n    return readPlain(reader, type, diph.num_values, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported page type: ${header.type}`)\n  }\n}\n\n/**\n * Read parquet header from a buffer.\n *\n * @import {ColumnData, ColumnDecoder, DataReader, DecodedArray, PageHeader, RowGroupSelect} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @returns {PageHeader}\n */\nfunction parquetHeader(reader) {\n  const header = deserializeTCompactProtocol(reader)\n\n  // Parse parquet header from thrift data\n  const type = PageType[header.field_1]\n  const uncompressed_page_size = header.field_2\n  const compressed_page_size = header.field_3\n  const crc = header.field_4\n  const data_page_header = header.field_5 && {\n    num_values: header.field_5.field_1,\n    encoding: Encoding[header.field_5.field_2],\n    definition_level_encoding: Encoding[header.field_5.field_3],\n    repetition_level_encoding: Encoding[header.field_5.field_4],\n    statistics: header.field_5.field_5 && {\n      max: header.field_5.field_5.field_1,\n      min: header.field_5.field_5.field_2,\n      null_count: header.field_5.field_5.field_3,\n      distinct_count: header.field_5.field_5.field_4,\n      max_value: header.field_5.field_5.field_5,\n      min_value: header.field_5.field_5.field_6,\n    },\n  }\n  const index_page_header = header.field_6\n  const dictionary_page_header = header.field_7 && {\n    num_values: header.field_7.field_1,\n    encoding: Encoding[header.field_7.field_2],\n    is_sorted: header.field_7.field_3,\n  }\n  const data_page_header_v2 = header.field_8 && {\n    num_values: header.field_8.field_1,\n    num_nulls: header.field_8.field_2,\n    num_rows: header.field_8.field_3,\n    encoding: Encoding[header.field_8.field_4],\n    definition_levels_byte_length: header.field_8.field_5,\n    repetition_levels_byte_length: header.field_8.field_6,\n    is_compressed: header.field_8.field_7 === undefined ? true : header.field_8.field_7, // default true\n    statistics: header.field_8.field_8,\n  }\n\n  return {\n    type,\n    uncompressed_page_size,\n    compressed_page_size,\n    crc,\n    data_page_header,\n    index_page_header,\n    dictionary_page_header,\n    data_page_header_v2,\n  }\n}\n","import { assembleNested } from './assemble.js'\nimport { readColumn } from './column.js'\nimport { DEFAULT_PARSERS } from './convert.js'\nimport { getColumnRange } from './plan.js'\nimport { getSchemaPath } from './schema.js'\nimport { flatten } from './utils.js'\n\n/**\n * @import {AsyncColumn, AsyncRowGroup, DecodedArray, GroupPlan, ParquetParsers, ParquetReadOptions, QueryPlan, RowGroup, SchemaTree} from './types.js'\n */\n/**\n * Read a row group from a file-like object.\n *\n * @param {ParquetReadOptions} options\n * @param {QueryPlan} plan\n * @param {GroupPlan} groupPlan\n * @returns {AsyncRowGroup} resolves to column data\n */\nexport function readRowGroup(options, { metadata, columns }, groupPlan) {\n  const { file, compressors, utf8 } = options\n\n  /** @type {AsyncColumn[]} */\n  const asyncColumns = []\n  /** @type {ParquetParsers} */\n  const parsers = { ...DEFAULT_PARSERS, ...options.parsers }\n\n  // read column data\n  for (const { file_path, meta_data } of groupPlan.rowGroup.columns) {\n    if (file_path) throw new Error('parquet file_path not supported')\n    if (!meta_data) throw new Error('parquet column metadata is undefined')\n\n    // skip columns that are not requested\n    const columnName = meta_data.path_in_schema[0]\n    if (columns && !columns.includes(columnName)) continue\n\n    const { startByte, endByte } = getColumnRange(meta_data)\n    const columnBytes = endByte - startByte\n\n    // skip columns larger than 1gb\n    // TODO: stream process the data, returning only the requested rows\n    if (columnBytes > 1 << 30) {\n      console.warn(`parquet skipping huge column \"${meta_data.path_in_schema}\" ${columnBytes} bytes`)\n      // TODO: set column to new Error('parquet column too large')\n      continue\n    }\n\n    // wrap awaitable to ensure it's a promise\n    /** @type {Promise<ArrayBuffer>} */\n    const buffer = Promise.resolve(file.slice(startByte, endByte))\n\n    // read column data async\n    asyncColumns.push({\n      pathInSchema: meta_data.path_in_schema,\n      data: buffer.then(arrayBuffer => {\n        const schemaPath = getSchemaPath(metadata.schema, meta_data.path_in_schema)\n        const reader = { view: new DataView(arrayBuffer), offset: 0 }\n        const subcolumn = meta_data.path_in_schema.join('.')\n        const columnDecoder = {\n          columnName: subcolumn,\n          type: meta_data.type,\n          element: schemaPath[schemaPath.length - 1].element,\n          schemaPath,\n          codec: meta_data.codec,\n          parsers,\n          compressors,\n          utf8,\n        }\n        return readColumn(reader, groupPlan, columnDecoder, options.onPage)\n      }),\n    })\n  }\n\n  return { groupStart: groupPlan.groupStart, groupRows: groupPlan.groupRows, asyncColumns }\n}\n\n/**\n * @overload\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'object'} rowFormat\n * @returns {Promise<Record<string, any>[]>} resolves to row data\n */\n/**\n * @overload\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'array'} [rowFormat]\n * @returns {Promise<any[][]>} resolves to row data\n */\n/**\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'object' | 'array'} [rowFormat]\n * @returns {Promise<Record<string, any>[] | any[][]>} resolves to row data\n */\nexport async function asyncGroupToRows({ asyncColumns }, selectStart, selectEnd, columns, rowFormat) {\n  // columnData[i] for asyncColumns[i]\n  // TODO: do it without flatten\n  const columnDatas = await Promise.all(asyncColumns.map(({ data }) => data.then(flatten)))\n\n  // careful mapping of column order for rowFormat: array\n  const includedColumnNames = asyncColumns\n    .map(child => child.pathInSchema[0])\n    .filter(name => !columns || columns.includes(name))\n  const columnOrder = columns ?? includedColumnNames\n  const columnIndexes = columnOrder.map(name => asyncColumns.findIndex(column => column.pathInSchema[0] === name))\n\n  // transpose columns into rows\n  const selectCount = selectEnd - selectStart\n  if (rowFormat === 'object') {\n    /** @type {Record<string, any>[]} */\n    const groupData = new Array(selectCount)\n    for (let selectRow = 0; selectRow < selectCount; selectRow++) {\n      const row = selectStart + selectRow\n      // return each row as an object\n      /** @type {Record<string, any>} */\n      const rowData = {}\n      for (let i = 0; i < asyncColumns.length; i++) {\n        rowData[asyncColumns[i].pathInSchema[0]] = columnDatas[i][row]\n      }\n      groupData[selectRow] = rowData\n    }\n    return groupData\n  }\n\n  /** @type {any[][]} */\n  const groupData = new Array(selectCount)\n  for (let selectRow = 0; selectRow < selectCount; selectRow++) {\n    const row = selectStart + selectRow\n    // return each row as an array\n    const rowData = new Array(asyncColumns.length)\n    for (let i = 0; i < columnOrder.length; i++) {\n      if (columnIndexes[i] >= 0) {\n        rowData[i] = columnDatas[columnIndexes[i]][row]\n      }\n    }\n    groupData[selectRow] = rowData\n  }\n  return groupData\n}\n\n/**\n * Assemble physical columns into top-level columns asynchronously.\n *\n * @param {AsyncRowGroup} asyncRowGroup\n * @param {SchemaTree} schemaTree\n * @returns {AsyncRowGroup}\n */\nexport function assembleAsync(asyncRowGroup, schemaTree) {\n  const { asyncColumns } = asyncRowGroup\n  /** @type {AsyncColumn[]} */\n  const assembled = []\n  for (const child of schemaTree.children) {\n    if (child.children.length) {\n      const childColumns = asyncColumns.filter(column => column.pathInSchema[0] === child.element.name)\n      if (!childColumns.length) continue\n\n      // wait for all child columns to be read\n      /** @type {Map<string, DecodedArray>} */\n      const flatData = new Map()\n      const data = Promise.all(childColumns.map(column => {\n        return column.data.then(columnData => {\n          flatData.set(column.pathInSchema.join('.'), flatten(columnData))\n        })\n      })).then(() => {\n        // assemble the column\n        assembleNested(flatData, child)\n        const flatColumn = flatData.get(child.path.join('.'))\n        if (!flatColumn) throw new Error('parquet column data not assembled')\n        return [flatColumn]\n      })\n\n      assembled.push({ pathInSchema: child.path, data })\n    } else {\n      // leaf node, return the column\n      const asyncColumn = asyncColumns.find(column => column.pathInSchema[0] === child.element.name)\n      if (asyncColumn) {\n        assembled.push(asyncColumn)\n      }\n    }\n  }\n  return { ...asyncRowGroup, asyncColumns: assembled }\n}\n","import { parquetMetadataAsync, parquetSchema } from './metadata.js'\nimport { parquetPlan, prefetchAsyncBuffer } from './plan.js'\nimport { assembleAsync, asyncGroupToRows, readRowGroup } from './rowgroup.js'\nimport { concat, flatten } from './utils.js'\n\n/**\n * @import {AsyncRowGroup, DecodedArray, ParquetReadOptions, BaseParquetReadOptions} from '../src/types.js'\n */\n/**\n * Read parquet data rows from a file-like object.\n * Reads the minimal number of row groups and columns to satisfy the request.\n *\n * Returns a void promise when complete.\n * Errors are thrown on the returned promise.\n * Data is returned in callbacks onComplete, onChunk, onPage, NOT the return promise.\n * See parquetReadObjects for a more convenient API.\n *\n * @param {ParquetReadOptions} options read options\n * @returns {Promise<void>} resolves when all requested rows and columns are parsed, all errors are thrown here\n */\nexport async function parquetRead(options) {\n  // load metadata if not provided\n  options.metadata ??= await parquetMetadataAsync(options.file, options)\n\n  // read row groups\n  const asyncGroups = parquetReadAsync(options)\n\n  const { rowStart = 0, rowEnd, columns, onChunk, onComplete, rowFormat } = options\n\n  // skip assembly if no onComplete or onChunk, but wait for reading to finish\n  if (!onComplete && !onChunk) {\n    for (const { asyncColumns } of asyncGroups) {\n      for (const { data } of asyncColumns) await data\n    }\n    return\n  }\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  // onChunk emit all chunks (don't await)\n  if (onChunk) {\n    for (const asyncGroup of assembled) {\n      for (const asyncColumn of asyncGroup.asyncColumns) {\n        asyncColumn.data.then(columnDatas => {\n          let rowStart = asyncGroup.groupStart\n          for (const columnData of columnDatas) {\n            onChunk({\n              columnName: asyncColumn.pathInSchema[0],\n              columnData,\n              rowStart,\n              rowEnd: rowStart + columnData.length,\n            })\n            rowStart += columnData.length\n          }\n        })\n      }\n    }\n  }\n\n  // onComplete transpose column chunks to rows\n  if (onComplete) {\n    // loosen the types to avoid duplicate code\n    /** @type {any[]} */\n    const rows = []\n    for (const asyncGroup of assembled) {\n      // filter to rows in range\n      const selectStart = Math.max(rowStart - asyncGroup.groupStart, 0)\n      const selectEnd = Math.min((rowEnd ?? Infinity) - asyncGroup.groupStart, asyncGroup.groupRows)\n      // transpose column chunks to rows in output\n      const groupData = rowFormat === 'object' ?\n        await asyncGroupToRows(asyncGroup, selectStart, selectEnd, columns, 'object') :\n        await asyncGroupToRows(asyncGroup, selectStart, selectEnd, columns, 'array')\n      concat(rows, groupData)\n    }\n    onComplete(rows)\n  } else {\n    // wait for all async groups to finish (complete takes care of this)\n    for (const { asyncColumns } of assembled) {\n      for (const { data } of asyncColumns) await data\n    }\n  }\n}\n\n/**\n * @param {ParquetReadOptions} options read options\n * @returns {AsyncRowGroup[]}\n */\nexport function parquetReadAsync(options) {\n  if (!options.metadata) throw new Error('parquet requires metadata')\n  // TODO: validate options (start, end, columns, etc)\n\n  // prefetch byte ranges\n  const plan = parquetPlan(options)\n  options.file = prefetchAsyncBuffer(options.file, plan)\n\n  // read row groups\n  return plan.groups.map(groupPlan => readRowGroup(options, plan, groupPlan))\n}\n\n/**\n * Reads a single column from a parquet file.\n *\n * @param {BaseParquetReadOptions} options\n * @returns {Promise<DecodedArray>}\n */\nexport async function parquetReadColumn(options) {\n  if (options.columns?.length !== 1) {\n    throw new Error('parquetReadColumn expected columns: [columnName]')\n  }\n  options.metadata ??= await parquetMetadataAsync(options.file, options)\n  const asyncGroups = parquetReadAsync(options)\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  /** @type {DecodedArray[]} */\n  const columnData = []\n  for (const rg of assembled) {\n    columnData.push(flatten(await rg.asyncColumns[0].data))\n  }\n  return flatten(columnData)\n}\n\n/**\n * This is a helper function to read parquet row data as a promise.\n * It is a wrapper around the more configurable parquetRead function.\n *\n * @param {Omit<ParquetReadOptions, 'onComplete'>} options\n * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed\n */\nexport function parquetReadObjects(options) {\n  return new Promise((onComplete, reject) => {\n    parquetRead({\n      ...options,\n      rowFormat: 'object', // force object output\n      onComplete,\n    }).catch(reject)\n  })\n}\n","import { parquetMetadataAsync, parquetReadObjects } from 'hyparquet'\n\n/**\n * Convert a GeoParquet file to GeoJSON.\n * Input is an AsyncBuffer representing a GeoParquet file.\n * An AsyncBuffer is a buffer-like object that can be read asynchronously.\n *\n * @import { AsyncBuffer, Compressors } from 'hyparquet'\n * @import { Feature, GeoJSON } from '../src/geojson.js'\n * @param {Object} options\n * @param {AsyncBuffer} options.file\n * @param {Compressors} [options.compressors]\n * @returns {Promise<GeoJSON>}\n */\nexport async function toGeoJson({ file, compressors }) {\n  const metadata = await parquetMetadataAsync(file)\n  const geoMetadata = metadata.key_value_metadata?.find(kv => kv.key === 'geo')\n  if (!geoMetadata) {\n    throw new Error('Invalid GeoParquet file: missing \"geo\" metadata')\n  }\n\n  // Geoparquet metadata\n  const geoSchema = JSON.parse(geoMetadata.value || '{}')\n\n  // Read all parquet data\n  const data = await parquetReadObjects({ file, metadata, utf8: false, compressors })\n\n  /** @type {Feature[]} */\n  const features = []\n  const primaryColumn = geoSchema.primary_column || 'geometry'\n  for (const row of data) {\n    const geometry = row[primaryColumn]\n    if (!geometry) continue // No geometry\n\n    // Extract properties (all fields except geometry)\n    /** @type {Record<string, any>} */\n    const properties = {}\n    for (const key of Object.keys(row)) {\n      const value = row[key]\n      if (key !== primaryColumn && value !== null) {\n        properties[key] = value\n      }\n    }\n\n    /** @type {Feature} */\n    const feature = {\n      type: 'Feature',\n      geometry,\n      properties,\n    }\n\n    features.push(feature)\n  }\n\n  return {\n    type: 'FeatureCollection',\n    features,\n  }\n}\n","import { asyncBufferFromUrl, cachedAsyncBuffer } from 'hyparquet'\nimport { toGeoJson } from '../src/index.js'\n\nasync function initMap() {\n  // @ts-expect-error MapsLibrary\n  const { Map } = await google.maps.importLibrary('maps')\n  const div = /** @type {HTMLElement} */document.getElementById('map')\n  // Create a new map\n  const map = new Map(div, {\n    center: { lat: 39, lng: -98 },\n    zoom: 4,\n  })\n\n  // URL or path to your GeoParquet file\n  const parquetUrl = 'https://hyparam.github.io/geoparquet/demo/polys.parquet'\n\n  try {\n    // Read the GeoParquet file and convert to GeoJSON\n    const file = cachedAsyncBuffer(\n      await asyncBufferFromUrl({ url: parquetUrl, byteLength: 29838 })\n    )\n    console.log('GeoParquet file:', file)\n    const geojson = await toGeoJson({ file })\n\n    console.log('GeoJSON:', geojson)\n\n    // Add the GeoJSON data to the map\n    map.data.addGeoJson(geojson)\n  } catch (error) {\n    console.error('Error loading or parsing GeoParquet file:', error)\n  }\n}\ninitMap()\n"],"names":["ParquetType","Encoding","FieldRepetitionType","ConvertedType","CompressionCodec","PageType","EdgeInterpolationAlgorithm","wkbToGeojson","reader","flags","getFlags","type","coordinates","readPosition","readLine","readPolygon","points","i","count","push","lines","polygons","geometries","Error","view","littleEndian","getUint8","offset","rawType","getUint32","Math","floor","dim","coord","getFloat64","rings","r","decoder","TextDecoder","DEFAULT_PARSERS","timestampFromMilliseconds","millis","Date","Number","timestampFromMicroseconds","micros","timestampFromNanoseconds","nanos","dateFromDays","days","stringFromBytes","bytes","decode","geometryFromBytes","DataView","buffer","byteOffset","byteLength","geographyFromBytes","convertWithDictionary","data","dictionary","encoding","columnDecoder","endsWith","output","Uint8Array","constructor","length","convert","element","parsers","utf8","converted_type","ctype","logical_type","ltype","factor","scale","arr","Array","parseDecimal","from","map","v","value","JSON","parse","bitWidth","isSigned","BigInt64Array","BigUint64Array","BigInt","Int32Array","Uint32Array","parseFloat16","unit","parser","byte","bits","int16","sign","exp","frac","NaN","Infinity","schemaTree","schema","rootIndex","path","children","num_children","childElement","child","name","getSchemaPath","tree","part","find","getMaxRepetitionLevel","schemaPath","maxLevel","repetition_type","getMaxDefinitionLevel","slice","isFlatColumn","column","CompactType","deserializeTCompactProtocol","lastFid","fid","newLastFid","readFieldBegin","readElement","getInt8","readZigZag","readZigZagBigInt","stringLength","readVarInt","strBytes","elemType","listSize","boolType","values","structValues","fieldType","result","shift","zigzag","readVarBigInt","delta","async","parquetMetadataAsync","asyncBuffer","initialFetchSize","defaultInitialFetchSize","geoparquet","footerOffset","max","footerBuffer","footerView","metadataLength","metadataOffset","metadataBuffer","combinedBuffer","ArrayBuffer","combinedView","set","parquetMetadata","arrayBuffer","metadataLengthOffset","metadata","version","field_1","field_2","field","type_length","field_3","field_4","field_5","field_6","field_7","precision","field_8","field_id","field_9","logicalType","field_10","columnSchema","filter","e","num_rows","row_groups","rowGroup","columns","columnIndex","file_path","file_offset","meta_data","encodings","path_in_schema","codec","num_values","total_uncompressed_size","total_compressed_size","key_value_metadata","data_page_offset","index_page_offset","dictionary_page_offset","field_11","statistics","convertStats","field_12","encoding_stats","field_13","encodingStat","page_type","bloom_filter_offset","field_14","bloom_filter_length","field_15","size_statistics","field_16","unencoded_byte_array_data_bytes","repetition_level_histogram","definition_level_histogram","geospatial_statistics","field_17","bbox","xmin","xmax","ymin","ymax","zmin","zmax","mmin","mmax","geospatial_types","offset_index_offset","offset_index_length","column_index_offset","column_index_length","crypto_metadata","encrypted_column_metadata","total_byte_size","sorting_columns","sortingColumn","column_idx","descending","nulls_first","ordinal","keyValue","key","created_by","Map","geo","decodedColumns","Object","entries","edges","id","crs","ids","authority","code","toString","undefined","get","markGeoColumns","metadata_length","isAdjustedToUTC","timeUnit","field_18","algorithm","stats","convertMetadata","min","null_count","distinct_count","max_value","min_value","is_max_value_exact","is_min_value_exact","getFloat32","getInt32","getBigInt64","concat","aaa","bbb","asyncBufferFromUrl","url","requestInit","fetch","customFetch","globalThis","method","then","res","ok","status","headers","parseInt","byteLengthFromUrl","init","start","end","Headers","endStr","body","cachedAsyncBuffer","minSize","cache","size","cacheKey","cached","promise","flatten","chunks","chunk","getColumnRange","columnOffset","startByte","endByte","assembleLists","definitionLevels","repetitionLevels","n","maxDefinitionLevel","repetitionPath","valueIndex","containerStack","currentContainer","currentDepth","currentDefLevel","currentRepLevel","at","def","rep","pop","newList","assembleNested","subcolumnData","depth","join","optional","nextDepth","firstChild","isListLike","sublist","subDepth","subcolumn","flattenAtDepth","delete","keyChild","valueChild","isMapLike","mapName","keys","out","assembleMaps","invertDepth","struct","childData","inverted","invertStruct","obj","j","deltaBinaryUnpack","int32","blockSize","miniblockPerBlock","outputIndex","valuesPerMiniblock","minDelta","bitWidths","bitpackPos","miniblockCount","mask","ceil","deltaLengthByteArray","lengths","clz32","readRleBitPackedHybrid","width","startOffset","seen","header","readBitPacked","readRle","left","right","byteStreamSplit","typeLength","byteWidth","b","Float32Array","Float64Array","split","subarray","readPlain","fixedLength","bitOffset","readPlainBoolean","align","readPlainInt32","readPlainInt64","low","high","readPlainInt96","readPlainFloat","readPlainDouble","readPlainByteArray","readPlainByteArrayFixed","aligned","WORD_MASK","copyBytes","fromArray","fromPos","toArray","toPos","readDataPage","daph","dataPage","maxRepetitionLevel","readRepetitionLevels","numNulls","readDefinitionLevels","nValues","x","decompressPage","compressedBytes","uncompressed_page_size","compressors","page","customDecompressor","input","inputLength","outputLength","pos","outPos","c","len","isNaN","lengthSize","snappyUncompress","readDataPageV2","ph","daph2","data_page_header_v2","repetition_levels_byte_length","readRepetitionLevelsV2","definition_levels_byte_length","readDefinitionLevelsV2","uncompressedPageSize","is_compressed","pageView","pageReader","num_nulls","prefixData","suffixData","suffix","deltaByteArray","readColumn","groupStart","selectStart","selectEnd","onPage","columnName","isFlat","lastChunk","rowCount","emitLastChunk","columnData","rowStart","rowEnd","parquetHeader","readPage","lastChunkLength","previousChunk","pageStart","compressed_page_size","data_page_header","isArray","diph","dictionary_page_header","crc","definition_level_encoding","repetition_level_encoding","index_page_header","is_sorted","asyncGroupToRows","asyncColumns","rowFormat","columnDatas","Promise","all","includedColumnNames","pathInSchema","includes","columnOrder","columnIndexes","findIndex","selectCount","groupData","selectRow","row","rowData","parquetRead","options","file","asyncGroups","plan","groups","fetches","groupRows","groupEnd","ranges","groupSize","isFinite","parquetPlan","promises","index","endOffset","prefetchAsyncBuffer","groupPlan","columnBytes","console","warn","resolve","readRowGroup","parquetReadAsync","onChunk","onComplete","parquetSchema","assembled","arg","asyncRowGroup","childColumns","flatData","flatColumn","asyncColumn","assembleAsync","asyncGroup","rows","toGeoJson","geoMetadata","kv","geoSchema","reject","catch","features","primaryColumn","primary_column","geometry","properties","feature","google","maps","importLibrary","document","getElementById","center","lat","lng","zoom","log","geojson","addGeoJson","error","initMap"],"mappings":"AACO,MAAMA,EAAc,CACzB,UACA,QACA,QACA,QACA,QACA,SACA,aACA,wBAIWC,EAAW,CACtB,QACA,gBACA,mBACA,MACA,aACA,sBACA,0BACA,mBACA,iBACA,qBAIWC,EAAsB,CACjC,WACA,WACA,YAIWC,EAAgB,CAC3B,OACA,MACA,gBACA,OACA,OACA,UACA,OACA,cACA,cACA,mBACA,mBACA,SACA,UACA,UACA,UACA,QACA,SACA,SACA,SACA,OACA,OACA,YAIWC,EAAmB,CAC9B,eACA,SACA,OACA,MACA,SACA,MACA,OACA,WAIWC,EAAW,CACtB,YACA,aACA,kBACA,gBAWWC,EAA6B,CACxC,YACA,WACA,SACA,UACA,UCrFK,SAASC,EAAaC,GAC3B,MAAMC,EAAQC,EAASF,GAEvB,GAAmB,IAAfC,EAAME,KACR,MAAO,CAAEA,KAAM,QAASC,YAAaC,EAAaL,EAAQC,IACrD,GAAmB,IAAfA,EAAME,KACf,MAAO,CAAEA,KAAM,aAAcC,YAAaE,EAASN,EAAQC,IACtD,GAAmB,IAAfA,EAAME,KACf,MAAO,CAAEA,KAAM,UAAWC,YAAaG,EAAYP,EAAQC,IACtD,GAAmB,IAAfA,EAAME,KAAY,CAC3B,MAAMK,EAAS,GACf,IAAK,IAAIC,EAAI,EAAGA,EAAIR,EAAMS,MAAOD,IAC/BD,EAAOG,KAAKN,EAAaL,EAAQE,EAASF,KAE5C,MAAO,CAAEG,KAAM,aAAcC,YAAaI,EAC5C,CAAO,GAAmB,IAAfP,EAAME,KAAY,CAC3B,MAAMS,EAAQ,GACd,IAAK,IAAIH,EAAI,EAAGA,EAAIR,EAAMS,MAAOD,IAC/BG,EAAMD,KAAKL,EAASN,EAAQE,EAASF,KAEvC,MAAO,CAAEG,KAAM,kBAAmBC,YAAaQ,EACjD,CAAO,GAAmB,IAAfX,EAAME,KAAY,CAC3B,MAAMU,EAAW,GACjB,IAAK,IAAIJ,EAAI,EAAGA,EAAIR,EAAMS,MAAOD,IAC/BI,EAASF,KAAKJ,EAAYP,EAAQE,EAASF,KAE7C,MAAO,CAAEG,KAAM,eAAgBC,YAAaS,EAC9C,CAAO,GAAmB,IAAfZ,EAAME,KAAY,CAC3B,MAAMW,EAAa,GACnB,IAAK,IAAIL,EAAI,EAAGA,EAAIR,EAAMS,MAAOD,IAC/BK,EAAWH,KAAKZ,EAAaC,IAE/B,MAAO,CAAEG,KAAM,qBAAsBW,aACvC,CACE,MAAM,IAAIC,MAAM,8BAA8Bd,EAAME,OAExD,CAgBA,SAASD,EAASF,GAChB,MAAMgB,KAAEA,GAAShB,EACXiB,EAAkD,IAAnCD,EAAKE,SAASlB,EAAOmB,UACpCC,EAAUJ,EAAKK,UAAUrB,EAAOmB,OAAQF,GAC9CjB,EAAOmB,QAAU,EAEjB,MAAMhB,EAAOiB,EAAU,IACjBnB,EAAQqB,KAAKC,MAAMH,EAAU,KAEnC,IAAIV,EAAQ,EACRP,EAAO,GAAKA,GAAQ,IACtBO,EAAQM,EAAKK,UAAUrB,EAAOmB,OAAQF,GACtCjB,EAAOmB,QAAU,GAInB,IAAIK,EAAM,EAIV,OAHIvB,GAAOuB,IACG,IAAVvB,GAAauB,IAEV,CAAEP,eAAcd,OAAMqB,MAAKd,QACpC,CAOA,SAASL,EAAaL,EAAQC,GAC5B,MAAMO,EAAS,GACf,IAAK,IAAIC,EAAI,EAAGA,EAAIR,EAAMuB,IAAKf,IAAK,CAClC,MAAMgB,EAAQzB,EAAOgB,KAAKU,WAAW1B,EAAOmB,OAAQlB,EAAMgB,cAC1DjB,EAAOmB,QAAU,EACjBX,EAAOG,KAAKc,EACd,CACA,OAAOjB,CACT,CAOA,SAASF,EAASN,EAAQC,GACxB,MAAMO,EAAS,GACf,IAAK,IAAIC,EAAI,EAAGA,EAAIR,EAAMS,MAAOD,IAC/BD,EAAOG,KAAKN,EAAaL,EAAQC,IAEnC,OAAOO,CACT,CAOA,SAASD,EAAYP,EAAQC,GAC3B,MAAMe,KAAEA,GAAShB,EACX2B,EAAQ,GACd,IAAK,IAAIC,EAAI,EAAGA,EAAI3B,EAAMS,MAAOkB,IAAK,CACpC,MAAMlB,EAAQM,EAAKK,UAAUrB,EAAOmB,OAAQlB,EAAMgB,cAClDjB,EAAOmB,QAAU,EACjBQ,EAAMhB,KAAKL,EAASN,EAAQ,IAAKC,EAAOS,UAC1C,CACA,OAAOiB,CACT,CCtHA,MAAME,EAAU,IAAIC,YAMPC,EAAkB,CAC7BC,0BAA0BC,GACjB,IAAIC,KAAKC,OAAOF,IAEzBG,0BAA0BC,GACjB,IAAIH,KAAKC,OAAOE,EAAS,QAElCC,yBAAyBC,GAChB,IAAIL,KAAKC,OAAOI,EAAQ,WAEjCC,aAAaC,GACJ,IAAIP,KAAY,MAAPO,GAElBC,gBAAgBC,GACPA,GAASd,EAAQe,OAAOD,GAEjCE,kBAAkBF,GACTA,GAAS5C,EAAa,CAAEiB,KAAM,IAAI8B,SAASH,EAAMI,OAAQJ,EAAMK,WAAYL,EAAMM,YAAa9B,OAAQ,IAE/G+B,mBAAmBP,GACVA,GAAS5C,EAAa,CAAEiB,KAAM,IAAI8B,SAASH,EAAMI,OAAQJ,EAAMK,WAAYL,EAAMM,YAAa9B,OAAQ,KAa1G,SAASgC,EAAsBC,EAAMC,EAAYC,EAAUC,GAChE,GAAIF,GAAcC,EAASE,SAAS,eAAgB,CAClD,IAAIC,EAASL,EACTA,aAAgBM,cAAgBL,aAAsBK,cAExDD,EAAS,IAAIJ,EAAWM,YAAYP,EAAKQ,SAE3C,IAAK,IAAInD,EAAI,EAAGA,EAAI2C,EAAKQ,OAAQnD,IAC/BgD,EAAOhD,GAAK4C,EAAWD,EAAK3C,IAE9B,OAAOgD,CACT,CACE,OAAOI,EAAQT,EAAMG,EAEzB,CASO,SAASM,EAAQT,EAAMG,GAC5B,MAAMO,QAAEA,EAAOC,QAAEA,EAAOC,KAAEA,GAAO,GAAST,GACpCpD,KAAEA,EAAM8D,eAAgBC,EAAOC,aAAcC,GAAUN,EAC7D,GAAc,YAAVI,EAAqB,CACvB,MACMG,EAAS,MADDP,EAAQQ,OAAS,GAEzBC,EAAM,IAAIC,MAAMpB,EAAKQ,QAC3B,IAAK,IAAInD,EAAI,EAAGA,EAAI8D,EAAIX,OAAQnD,IAC1B2C,EAAK3C,aAAciD,WACrBa,EAAI9D,GAAKgE,EAAarB,EAAK3C,IAAM4D,EAEjCE,EAAI9D,GAAK0B,OAAOiB,EAAK3C,IAAM4D,EAG/B,OAAOE,CACT,CACA,IAAKL,GAAkB,UAAT/D,EACZ,OAAOqE,MAAME,KAAKtB,GAAMuB,KAAIC,IAAKb,SAAQzB,yBA4F7B,mBAHSuC,EAzF6DD,IA0F7D,KAAO,WACT,oBAARC,IAFf,IAAyBA,CAzFgE,IAEvF,GAAc,SAAVX,EACF,OAAOM,MAAME,KAAKtB,GAAMuB,KAAIC,GAAKb,EAAQvB,aAAaoC,KAExD,GAAc,qBAAVV,EACF,OAAOM,MAAME,KAAKtB,GAAMuB,KAAIC,GAAKb,EAAQ/B,0BAA0B4C,KAErE,GAAc,qBAAVV,EACF,OAAOM,MAAME,KAAKtB,GAAMuB,KAAIC,GAAKb,EAAQ3B,0BAA0BwC,KAErE,GAAc,SAAVV,EACF,OAAOd,EAAKuB,KAAIC,GAAKE,KAAKC,MAAMlD,EAAQe,OAAOgC,MAEjD,GAAc,SAAVV,EACF,MAAM,IAAInD,MAAM,8BAElB,GAAc,aAAVmD,EACF,MAAM,IAAInD,MAAM,kCAElB,GAAoB,aAAhBqD,GAAOjE,KACT,OAAOiD,EAAKuB,KAAIC,GAAKb,EAAQlB,kBAAkB+B,KAEjD,GAAoB,cAAhBR,GAAOjE,KACT,OAAOiD,EAAKuB,KAAIC,GAAKb,EAAQb,mBAAmB0B,KAElD,GAAc,SAAVV,GAAoC,WAAhBE,GAAOjE,MAAqB6D,GAAiB,eAAT7D,EAC1D,OAAOiD,EAAKuB,KAAIC,GAAKb,EAAQrB,gBAAgBkC,KAE/C,GAAc,YAAVV,GAAuC,YAAhBE,GAAOjE,MAAyC,KAAnBiE,EAAMY,WAAoBZ,EAAMa,SAAU,CAChG,GAAI7B,aAAgB8B,cAClB,OAAO,IAAIC,eAAe/B,EAAKL,OAAQK,EAAKJ,WAAYI,EAAKQ,QAE/D,MAAMW,EAAM,IAAIY,eAAe/B,EAAKQ,QACpC,IAAK,IAAInD,EAAI,EAAGA,EAAI8D,EAAIX,OAAQnD,IAAK8D,EAAI9D,GAAK2E,OAAOhC,EAAK3C,IAC1D,OAAO8D,CACT,CACA,GAAc,YAAVL,GAAuC,YAAhBE,GAAOjE,MAAyC,KAAnBiE,EAAMY,WAAoBZ,EAAMa,SAAU,CAChG,GAAI7B,aAAgBiC,WAClB,OAAO,IAAIC,YAAYlC,EAAKL,OAAQK,EAAKJ,WAAYI,EAAKQ,QAE5D,MAAMW,EAAM,IAAIe,YAAYlC,EAAKQ,QACjC,IAAK,IAAInD,EAAI,EAAGA,EAAI8D,EAAIX,OAAQnD,IAAK8D,EAAI9D,GAAK2C,EAAK3C,GACnD,OAAO8D,CACT,CACA,GAAoB,YAAhBH,GAAOjE,KACT,OAAOqE,MAAME,KAAKtB,GAAMuB,IAAIY,GAE9B,GAAoB,cAAhBnB,GAAOjE,KAAsB,CAC/B,MAAMqF,KAAEA,GAASpB,EAEjB,IAAIqB,EAAS1B,EAAQ/B,0BACR,WAATwD,IAAmBC,EAAS1B,EAAQ3B,2BAC3B,UAAToD,IAAkBC,EAAS1B,EAAQzB,0BACvC,MAAMiC,EAAM,IAAIC,MAAMpB,EAAKQ,QAC3B,IAAK,IAAInD,EAAI,EAAGA,EAAI8D,EAAIX,OAAQnD,IAC9B8D,EAAI9D,GAAKgF,EAAOrC,EAAK3C,IAEvB,OAAO8D,CACT,CACA,OAAOnB,CACT,CAMO,SAASqB,EAAa9B,GAC3B,IAAKA,EAAMiB,OAAQ,OAAO,EAE1B,IAAIiB,EAAQ,GACZ,IAAK,MAAMa,KAAQ/C,EACjBkC,EAAgB,KAARA,EAAeO,OAAOM,GAIhC,MAAMC,EAAsB,EAAfhD,EAAMiB,OAKnB,OAJIiB,GAAS,IAAMO,OAAOO,EAAO,KAC/Bd,GAAS,IAAMO,OAAOO,IAGjBxD,OAAO0C,EAChB,CAiBO,SAASU,EAAa5C,GAC3B,IAAKA,EAAO,OACZ,MAAMiD,EAAQjD,EAAM,IAAM,EAAIA,EAAM,GAC9BkD,EAAOD,GAAS,MAAU,EAC1BE,EAAMF,GAAS,GAAK,GACpBG,EAAe,KAARH,EACb,OAAY,IAARE,EAAkBD,EAAO,IAAK,IAAOE,EAAO,MACpC,KAARD,EAAqBC,EAAOC,IAAMH,GAAOI,KACtCJ,EAAO,IAAMC,EAAM,KAAO,EAAIC,EAAO,KAC9C,CCxLA,SAASG,EAAWC,EAAQC,EAAWC,GACrC,MAAMvC,EAAUqC,EAAOC,GACjBE,EAAW,GACjB,IAAI5F,EAAQ,EAGZ,GAAIoD,EAAQyC,aACV,KAAOD,EAAS1C,OAASE,EAAQyC,cAAc,CAC7C,MAAMC,EAAeL,EAAOC,EAAY1F,GAClC+F,EAAQP,EAAWC,EAAQC,EAAY1F,EAAO,IAAI2F,EAAMG,EAAaE,OAC3EhG,GAAS+F,EAAM/F,MACf4F,EAAS3F,KAAK8F,EAChB,CAGF,MAAO,CAAE/F,QAAOoD,UAASwC,WAAUD,OACrC,CASO,SAASM,EAAcR,EAAQO,GACpC,IAAIE,EAAOV,EAAWC,EAAQ,EAAG,IACjC,MAAME,EAAO,CAACO,GACd,IAAK,MAAMC,KAAQH,EAAM,CACvB,MAAMD,EAAQG,EAAKN,SAASQ,MAAKL,GAASA,EAAM3C,QAAQ4C,OAASG,IACjE,IAAKJ,EAAO,MAAM,IAAI1F,MAAM,qCAAqC2F,KACjEL,EAAK1F,KAAK8F,GACVG,EAAOH,CACT,CACA,OAAOJ,CACT,CAQO,SAASU,EAAsBC,GACpC,IAAIC,EAAW,EACf,IAAK,MAAMnD,QAAEA,KAAakD,EACQ,aAA5BlD,EAAQoD,iBACVD,IAGJ,OAAOA,CACT,CAQO,SAASE,EAAsBH,GACpC,IAAIC,EAAW,EACf,IAAK,MAAMnD,QAAEA,KAAakD,EAAWI,MAAM,GACT,aAA5BtD,EAAQoD,iBACVD,IAGJ,OAAOA,CACT,CAkDO,SAASI,EAAaL,GAC3B,GAA0B,IAAtBA,EAAWpD,OAAc,OAAO,EACpC,MAAM,CAAG0D,GAAUN,EACnB,MAAuC,aAAnCM,EAAOxD,QAAQoD,kBACfI,EAAOhB,SAAS1C,MAEtB,CCnIO,MAAM2D,EACL,EADKA,EAEL,EAFKA,EAGJ,EAHIA,EAIL,EAJKA,EAKN,EALMA,EAMN,EANMA,EAON,EAPMA,EAQH,EARGA,EASH,EATGA,EAUL,EAVKA,EAaH,GAUH,SAASC,EAA4BxH,GAC1C,IAAIyH,EAAU,EAEd,MAAM5C,EAAQ,CAAA,EAEd,KAAO7E,EAAOmB,OAASnB,EAAOgB,KAAKiC,YAAY,CAE7C,MAAO9C,EAAMuH,EAAKC,GAAcC,EAAe5H,EAAQyH,GAGvD,GAFAA,EAAUE,EAENxH,IAASoH,EACX,MAIF1C,EAAM,SAAS6C,KAASG,EAAY7H,EAAQG,EAC9C,CAEA,OAAO0E,CACT,CAUA,SAASgD,EAAY7H,EAAQG,GAC3B,OAAQA,GACR,KAAKoH,EACH,OAAO,EACT,KAAKA,EACH,OAAO,EACT,KAAKA,EAEH,OAAOvH,EAAOgB,KAAK8G,QAAQ9H,EAAOmB,UACpC,KAAKoG,EACL,KAAKA,EACH,OAAOQ,EAAW/H,GACpB,KAAKuH,EACH,OAAOS,EAAiBhI,GAC1B,KAAKuH,EAAoB,CACvB,MAAM1C,EAAQ7E,EAAOgB,KAAKU,WAAW1B,EAAOmB,QAAQ,GAEpD,OADAnB,EAAOmB,QAAU,EACV0D,CACT,CACA,KAAK0C,EAAoB,CACvB,MAAMU,EAAeC,EAAWlI,GAC1BmI,EAAW,IAAIzE,WAAW1D,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQ8G,GAE5F,OADAjI,EAAOmB,QAAU8G,EACVE,CACT,CACA,KAAKZ,EAAkB,CACrB,MAAM7B,EAAO1F,EAAOgB,KAAKE,SAASlB,EAAOmB,UACnCiH,EAAkB,GAAP1C,EACjB,IAAI2C,EAAW3C,GAAQ,EACN,KAAb2C,IACFA,EAAWH,EAAWlI,IAExB,MAAMsI,EAAWF,IAAab,GAAoBa,IAAab,EACzDgB,EAAS,IAAI/D,MAAM6D,GACzB,IAAK,IAAI5H,EAAI,EAAGA,EAAI4H,EAAU5H,IAC5B8H,EAAO9H,GAAK6H,EAAqD,IAA1CT,EAAY7H,EAAQuH,GAA0BM,EAAY7H,EAAQoI,GAE3F,OAAOG,CACT,CACA,KAAKhB,EAAoB,CAEvB,MAAMiB,EAAe,CAAA,EACrB,IAAIf,EAAU,EACd,OAAa,CACX,MAAOgB,EAAWf,EAAKC,GAAcC,EAAe5H,EAAQyH,GAE5D,GADAA,EAAUE,EACNc,IAAclB,EAChB,MAEFiB,EAAa,SAASd,KAASG,EAAY7H,EAAQyI,EACrD,CACA,OAAOD,CACT,CAEA,QACE,MAAM,IAAIzH,MAAM,0BAA0BZ,KAE9C,CASO,SAAS+H,EAAWlI,GACzB,IAAI0I,EAAS,EACTC,EAAQ,EACZ,OAAa,CACX,MAAMjD,EAAO1F,EAAOgB,KAAKE,SAASlB,EAAOmB,UAEzC,GADAuH,IAAkB,IAAPhD,IAAgBiD,IACd,IAAPjD,GACJ,OAAOgD,EAETC,GAAS,CACX,CACF,CA4BO,SAASZ,EAAW/H,GACzB,MAAM4I,EAASV,EAAWlI,GAE1B,OAAO4I,IAAW,IAAe,EAATA,EAC1B,CASO,SAASZ,EAAiBhI,GAC/B,MAAM4I,EAlCR,SAAuB5I,GACrB,IAAI0I,EAAS,GACTC,EAAQ,GACZ,OAAa,CACX,MAAMjD,EAAO1F,EAAOgB,KAAKE,SAASlB,EAAOmB,UAEzC,GADAuH,GAAUtD,OAAc,IAAPM,IAAgBiD,IACpB,IAAPjD,GACJ,OAAOgD,EAETC,GAAS,EACX,CACF,CAuBiBE,CAAc7I,GAE7B,OAAO4I,GAAU,KAAgB,GAATA,EAC1B,CASA,SAAShB,EAAe5H,EAAQyH,GAC9B,MAAM/B,EAAO1F,EAAOgB,KAAKE,SAASlB,EAAOmB,UACnChB,EAAc,GAAPuF,EACb,GAAIvF,IAASoH,EAEX,MAAO,CAAC,EAAG,EAAGE,GAEhB,MAAMqB,EAAQpD,GAAQ,EAChBgC,EAAMoB,EAAQrB,EAAUqB,EAAQf,EAAW/H,GACjD,MAAO,CAACG,EAAMuH,EAAKA,EACrB,CC5LO,MAED7F,EAAU,IAAIC,YACpB,SAASc,EAAiCiC,GACxC,OAAOA,GAAShD,EAAQe,OAAOiC,EACjC,CA0BOkE,eAAeC,EAAqBC,GAAalF,QAAEA,EAAOmF,iBAAEA,EAAmBC,OAAuBC,WAAEA,GAAa,GAAS,IACnI,KAAKH,GAAiBA,EAAYhG,YAAc,GAAI,MAAM,IAAIlC,MAAM,gCAGpE,MAAMsI,EAAe/H,KAAKgI,IAAI,EAAGL,EAAYhG,WAAaiG,GACpDK,QAAqBN,EAAY7B,MAAMiC,EAAcJ,EAAYhG,YAGjEuG,EAAa,IAAI1G,SAASyG,GAChC,GAAgE,YAA5DC,EAAWnI,UAAUkI,EAAatG,WAAa,GAAG,GACpD,MAAM,IAAIlC,MAAM,yCAKlB,MAAM0I,EAAiBD,EAAWnI,UAAUkI,EAAatG,WAAa,GAAG,GACzE,GAAIwG,EAAiBR,EAAYhG,WAAa,EAC5C,MAAM,IAAIlC,MAAM,2BAA2B0I,8BAA2CR,EAAYhG,WAAa,KAIjH,GAAIwG,EAAiB,EAAIP,EAAkB,CAEzC,MAAMQ,EAAiBT,EAAYhG,WAAawG,EAAiB,EAC3DE,QAAuBV,EAAY7B,MAAMsC,EAAgBL,GAEzDO,EAAiB,IAAIC,YAAYJ,EAAiB,GAClDK,EAAe,IAAIpG,WAAWkG,GAGpC,OAFAE,EAAaC,IAAI,IAAIrG,WAAWiG,IAChCG,EAAaC,IAAI,IAAIrG,WAAW6F,GAAeF,EAAeK,GACvDM,EAAgBJ,EAAgB,CAAE7F,UAASqF,cACpD,CAEE,OAAOY,EAAgBT,EAAc,CAAExF,UAASqF,cAEpD,CAUO,SAASY,EAAgBC,GAAalG,QAAEA,EAAOqF,WAAEA,GAAa,GAAS,IAC5E,KAAMa,aAAuBJ,aAAc,MAAM,IAAI9I,MAAM,gCAC3D,MAAMC,EAAO,IAAI8B,SAASmH,GAM1B,GAHAlG,EAAU,IAAKhC,KAAoBgC,GAG/B/C,EAAKiC,WAAa,EACpB,MAAM,IAAIlC,MAAM,6BAElB,GAAkD,YAA9CC,EAAKK,UAAUL,EAAKiC,WAAa,GAAG,GACtC,MAAM,IAAIlC,MAAM,yCAKlB,MAAMmJ,EAAuBlJ,EAAKiC,WAAa,EACzCwG,EAAiBzI,EAAKK,UAAU6I,GAAsB,GAC5D,GAAIT,EAAiBzI,EAAKiC,WAAa,EAErC,MAAM,IAAIlC,MAAM,2BAA2B0I,8BAA2CzI,EAAKiC,WAAa,KAG1G,MAEMkH,EAAW3C,EADF,CAAExG,OAAMG,OADA+I,EAAuBT,IAKxCW,EAAUD,EAASE,QAEnBlE,EAASgE,EAASG,QAAQ3F,KAAwB4F,IAAK,CAC3DpK,KAAMX,EAAY+K,EAAMF,SACxBG,YAAaD,EAAMD,QACnBpD,gBAAiBxH,EAAoB6K,EAAME,SAC3C/D,KAAM9D,EAAO2H,EAAMG,SACnBnE,aAAcgE,EAAMI,QACpB1G,eAAgBtE,EAAc4K,EAAMK,SACpCtG,MAAOiG,EAAMM,QACbC,UAAWP,EAAMQ,QACjBC,SAAUT,EAAMU,QAChB9G,aAAc+G,EAAYX,EAAMY,cAG5BC,EAAejF,EAAOkF,QAAOC,GAAKA,EAAEnL,OACpCoL,EAAWpB,EAASM,QACpBe,EAAarB,EAASO,QAAQ/F,KAAwB8G,IAAQ,CAClEC,QAASD,EAASpB,QAAQ1F,KAAI,CAAoB2C,EAA8BqE,KAAW,CACzFC,UAAWhJ,EAAO0E,EAAO+C,SACzBwB,YAAavE,EAAOgD,QACpBwB,UAAWxE,EAAOmD,SAAW,CAC3BtK,KAAMX,EAAY8H,EAAOmD,QAAQJ,SACjC0B,UAAWzE,EAAOmD,QAAQH,SAAS3F,KAA2B2G,GAAM7L,EAAS6L,KAC7EU,eAAgB1E,EAAOmD,QAAQA,QAAQ9F,IAAI/B,GAC3CqJ,MAAOrM,EAAiB0H,EAAOmD,QAAQC,SACvCwB,WAAY5E,EAAOmD,QAAQE,QAC3BwB,wBAAyB7E,EAAOmD,QAAQG,QACxCwB,sBAAuB9E,EAAOmD,QAAQI,QACtCwB,mBAAoB/E,EAAOmD,QAAQM,QACnCuB,iBAAkBhF,EAAOmD,QAAQQ,QACjCsB,kBAAmBjF,EAAOmD,QAAQU,SAClCqB,uBAAwBlF,EAAOmD,QAAQgC,SACvCC,WAAYC,EAAarF,EAAOmD,QAAQmC,SAAUxB,EAAaO,GAAc5H,GAC7E8I,eAAgBvF,EAAOmD,QAAQqC,UAAUnI,KAAwBoI,IAAY,CAC3EC,UAAWnN,EAASkN,EAAa1C,SACjC/G,SAAU7D,EAASsN,EAAazC,SAChC5J,MAAOqM,EAAatC,YAEtBwC,oBAAqB3F,EAAOmD,QAAQyC,SACpCC,oBAAqB7F,EAAOmD,QAAQ2C,SACpCC,gBAAiB/F,EAAOmD,QAAQ6C,UAAY,CAC1CC,gCAAiCjG,EAAOmD,QAAQ6C,SAASjD,QACzDmD,2BAA4BlG,EAAOmD,QAAQ6C,SAAShD,QACpDmD,2BAA4BnG,EAAOmD,QAAQ6C,SAAS7C,SAEtDiD,sBAAuBpG,EAAOmD,QAAQkD,UAAY,CAChDC,KAAMtG,EAAOmD,QAAQkD,SAAStD,SAAW,CACvCwD,KAAMvG,EAAOmD,QAAQkD,SAAStD,QAAQA,QACtCyD,KAAMxG,EAAOmD,QAAQkD,SAAStD,QAAQC,QACtCyD,KAAMzG,EAAOmD,QAAQkD,SAAStD,QAAQI,QACtCuD,KAAM1G,EAAOmD,QAAQkD,SAAStD,QAAQK,QACtCuD,KAAM3G,EAAOmD,QAAQkD,SAAStD,QAAQM,QACtCuD,KAAM5G,EAAOmD,QAAQkD,SAAStD,QAAQO,QACtCuD,KAAM7G,EAAOmD,QAAQkD,SAAStD,QAAQQ,QACtCuD,KAAM9G,EAAOmD,QAAQkD,SAAStD,QAAQU,SAExCsD,iBAAkB/G,EAAOmD,QAAQkD,SAASrD,UAG9CgE,oBAAqBhH,EAAOoD,QAC5B6D,oBAAqBjH,EAAOqD,QAC5B6D,oBAAqBlH,EAAOsD,QAC5B6D,oBAAqBnH,EAAOuD,QAC5B6D,gBAAiBpH,EAAOyD,QACxB4D,0BAA2BrH,EAAO2D,YAEpC2D,gBAAiBnD,EAASnB,QAC1BiB,SAAUE,EAAShB,QACnBoE,gBAAiBpD,EAASf,SAAS/F,KAAwBmK,IAAa,CACtEC,WAAYD,EAAczE,QAC1B2E,WAAYF,EAAcxE,QAC1B2E,YAAaH,EAAcrE,YAE7BoB,YAAaJ,EAASd,QACtByB,sBAAuBX,EAASb,QAChCsE,QAASzD,EAASZ,YAGdwB,EAAqBlC,EAASQ,SAAShG,KAAwBwK,IAAQ,CAC3EC,IAAKxM,EAAOuM,EAAS9E,SACrBxF,MAAOjC,EAAOuM,EAAS7E,aAEnB+E,EAAazM,EAAOuH,EAASS,SAMnC,OAJIxB,GC9LC,SAAwBjD,EAAQkG,GAGrC,MAAMX,EAAU,IAAI4D,IACdC,EAAMlD,GAAoBvF,MAAK,EAAGsI,SAAkB,QAARA,KAAgBvK,MAC5D2K,GAAkBD,GAAOzK,KAAKC,MAAMwK,IAAM7D,UAAY,CAAA,EAC5D,IAAK,MAAOhF,EAAMY,KAAWmI,OAAOC,QAAQF,GAAiB,CAC3D,GAAwB,QAApBlI,EAAOhE,SACT,SAEF,MAAMnD,EAAwB,cAAjBmH,EAAOqI,MAAwB,YAAc,WACpDC,EAAKtI,EAAOuI,KAAKD,IAAMtI,EAAOuI,KAAKC,MAAM,GACzCD,EAAMD,EAAK,GAAGA,EAAGG,aAAaH,EAAGI,KAAKC,kBAAeC,EAE3DxE,EAAQ3B,IAAIrD,EAAM,CAAEvG,OAAM0P,OAC5B,CAIA,IAAK,IAAIpP,EAAI,EAAGA,EAAI0F,EAAOvC,OAAQnD,IAAK,CACtC,MAAMqD,EAAUqC,EAAO1F,IACjB0D,aAAEA,EAAYuC,KAAEA,EAAIH,aAAEA,EAAYW,gBAAEA,EAAe/G,KAAEA,GAAS2D,EAChEyC,EACF9F,GAAK8F,EAGM,eAATpG,QAA0C+P,IAAjB/L,GAAkD,aAApB+C,IACzDpD,EAAQK,aAAeuH,EAAQyE,IAAIzJ,GAEvC,CACF,CDiKI0J,CAAejK,EAAQkG,GAGlB,CACLjC,UACAjE,SACAoF,WACAC,aACAa,qBACAgD,aACAgB,gBAAiB5G,EAErB,CAgBA,SAASyB,EAAYA,GACnB,OAAIA,GAAab,QAAgB,CAAElK,KAAM,UACrC+K,GAAaZ,QAAgB,CAAEnK,KAAM,OACrC+K,GAAaT,QAAgB,CAAEtK,KAAM,QACrC+K,GAAaR,QAAgB,CAAEvK,KAAM,QACrC+K,GAAaP,QAAgB,CAC/BxK,KAAM,UACNmE,MAAO4G,EAAYP,QAAQN,QAC3BS,UAAWI,EAAYP,QAAQL,SAE7BY,GAAaN,QAAgB,CAAEzK,KAAM,QACrC+K,GAAaL,QAAgB,CAC/B1K,KAAM,OACNmQ,gBAAiBpF,EAAYL,QAAQR,QACrC7E,KAAM+K,EAASrF,EAAYL,QAAQP,UAEjCY,GAAaH,QAAgB,CAC/B5K,KAAM,YACNmQ,gBAAiBpF,EAAYH,QAAQV,QACrC7E,KAAM+K,EAASrF,EAAYH,QAAQT,UAEjCY,GAAaC,SAAiB,CAChChL,KAAM,UACN6E,SAAUkG,EAAYC,SAASd,QAC/BpF,SAAUiG,EAAYC,SAASb,SAE7BY,GAAauB,SAAiB,CAAEtM,KAAM,QACtC+K,GAAa0B,SAAiB,CAAEzM,KAAM,QACtC+K,GAAa4B,SAAiB,CAAE3M,KAAM,QACtC+K,GAAagC,SAAiB,CAAE/M,KAAM,QACtC+K,GAAakC,SAAiB,CAAEjN,KAAM,WACtC+K,GAAaoC,SAAiB,CAAEnN,KAAM,WACtC+K,GAAayC,SAAiB,CAChCxN,KAAM,WACN0P,IAAKjN,EAAOsI,EAAYyC,SAAStD,UAE/Ba,GAAasF,SAAiB,CAChCrQ,KAAM,YACN0P,IAAKjN,EAAOsI,EAAYsF,SAASnG,SACjCoG,UAAW3Q,EAA2BoL,EAAYsF,SAASlG,UAEtDY,CACT,CAMA,SAASqF,EAAS/K,GAChB,GAAIA,EAAK6E,QAAS,MAAO,SACzB,GAAI7E,EAAK8E,QAAS,MAAO,SACzB,GAAI9E,EAAKiF,QAAS,MAAO,QACzB,MAAM,IAAI1J,MAAM,6BAClB,CAWA,SAAS4L,EAAa+D,EAAOvK,EAAQpC,GACnC,OAAO2M,GAAS,CACdpH,IAAKqH,EAAgBD,EAAMrG,QAASlE,EAAQpC,GAC5C6M,IAAKD,EAAgBD,EAAMpG,QAASnE,EAAQpC,GAC5C8M,WAAYH,EAAMjG,QAClBqG,eAAgBJ,EAAMhG,QACtBqG,UAAWJ,EAAgBD,EAAM/F,QAASxE,EAAQpC,GAClDiN,UAAWL,EAAgBD,EAAM9F,QAASzE,EAAQpC,GAClDkN,mBAAoBP,EAAM7F,QAC1BqG,mBAAoBR,EAAM3F,QAE9B,CAQO,SAAS4F,EAAgB9L,EAAOsB,EAAQpC,GAC7C,MAAM5D,KAAEA,EAAI8D,eAAEA,EAAcE,aAAEA,GAAiBgC,EAC/C,QAAc+J,IAAVrL,EAAqB,OAAOA,EAChC,GAAa,YAAT1E,EAAoB,OAAoB,IAAb0E,EAAM,GACrC,GAAa,eAAT1E,EAAuB,OAAO4D,EAAQrB,gBAAgBmC,GAC1D,MAAM7D,EAAO,IAAI8B,SAAS+B,EAAM9B,OAAQ8B,EAAM7B,WAAY6B,EAAM5B,YAChE,MAAa,UAAT9C,GAAwC,IAApBa,EAAKiC,WAAyBjC,EAAKmQ,WAAW,GAAG,GAC5D,WAAThR,GAAyC,IAApBa,EAAKiC,WAAyBjC,EAAKU,WAAW,GAAG,GAC7D,UAATvB,GAAuC,SAAnB8D,EAAkCF,EAAQvB,aAAaxB,EAAKoQ,SAAS,GAAG,IACnF,UAATjR,GAAuC,qBAAnB8D,EAA8CF,EAAQ/B,0BAA0BhB,EAAKqQ,YAAY,GAAG,IAC/G,UAATlR,GAAuC,qBAAnB8D,EAA8CF,EAAQ3B,0BAA0BpB,EAAKqQ,YAAY,GAAG,IAC/G,UAATlR,GAA2C,cAAvBgE,GAAchE,MAA+C,UAAvBgE,GAAcqB,KAAyBzB,EAAQzB,yBAAyBtB,EAAKqQ,YAAY,GAAG,IAC7I,UAATlR,GAA2C,cAAvBgE,GAAchE,MAA+C,WAAvBgE,GAAcqB,KAA0BzB,EAAQ3B,0BAA0BpB,EAAKqQ,YAAY,GAAG,IAC/I,UAATlR,GAA2C,cAAvBgE,GAAchE,KAA6B4D,EAAQ/B,0BAA0BhB,EAAKqQ,YAAY,GAAG,IAC5G,UAATlR,GAAwC,IAApBa,EAAKiC,WAAyBjC,EAAKoQ,SAAS,GAAG,GAC1D,UAATjR,GAAwC,IAApBa,EAAKiC,WAAyBjC,EAAKqQ,YAAY,GAAG,GACnD,YAAnBpN,EAAqCQ,EAAaI,GAAS,MAAQsB,EAAO7B,OAAS,GAC5D,YAAvBH,GAAchE,KAA2BoF,EAAaV,GACdA,CAG9C,CEzSO,SAASyM,EAAOC,EAAKC,GAE1B,IAAK,IAAI/Q,EAAI,EAAGA,EAAI+Q,EAAI5N,OAAQnD,GADlB,IAEZ8Q,EAAI5Q,QAAQ6Q,EAAIpK,MAAM3G,EAAGA,EAFb,KAIhB,CAmDOsI,eAAe0I,GAAmBC,IAAEA,EAAGzO,WAAEA,EAAU0O,YAAEA,EAAaC,MAAOC,IAE9E,MAAMD,EAAQC,GAAeC,WAAWF,MAQxC,IAAI7O,EANJE,UA5BK8F,eAAiC2I,EAAKC,EAAaE,GACxD,MAAMD,EAAQC,GAAeC,WAAWF,MACxC,aAAaA,EAAMF,EAAK,IAAKC,EAAaI,OAAQ,SAC/CC,MAAKC,IACJ,IAAKA,EAAIC,GAAI,MAAM,IAAInR,MAAM,qBAAqBkR,EAAIE,UACtD,MAAMvO,EAASqO,EAAIG,QAAQjC,IAAI,kBAC/B,IAAKvM,EAAQ,MAAM,IAAI7C,MAAM,0BAC7B,OAAOsR,SAASzO,EAAM,GAE5B,CAmBuB0O,CAAkBZ,EAAKC,EAAaC,GAOzD,MAAMW,EAAOZ,GAAe,CAAA,EAE5B,MAAO,CACL1O,aACA,WAAMmE,CAAMoL,EAAOC,GACjB,GAAI1P,EACF,OAAOA,EAAOiP,MAAKjP,GAAUA,EAAOqE,MAAMoL,EAAOC,KAGnD,MAAML,EAAU,IAAIM,QAAQH,EAAKH,SAC3BO,OAAiBzC,IAARuC,EAAoB,GAAKA,EAAM,EAC9CL,EAAQrI,IAAI,QAAS,SAASyI,KAASG,KAEvC,MAAMV,QAAYL,EAAMF,EAAK,IAAKa,EAAMH,YACxC,IAAKH,EAAIC,KAAOD,EAAIW,KAAM,MAAM,IAAI7R,MAAM,gBAAgBkR,EAAIE,UAE9D,GAAmB,MAAfF,EAAIE,OAGN,OADApP,EAASkP,EAAIhI,cACNlH,EAAOiP,MAAKjP,GAAUA,EAAOqE,MAAMoL,EAAOC,KAC5C,GAAmB,MAAfR,EAAIE,OAEb,OAAOF,EAAIhI,cAEX,MAAM,IAAIlJ,MAAM,yCAAyCkR,EAAIE,SAEjE,EAEJ,CAUO,SAASU,GAAkB5P,WAAEA,EAAUmE,MAAEA,IAAS0L,QAAEA,EAAU3J,QAA4B,IAC/F,GAAIlG,EAAa6P,EAAS,CAExB,MAAM/P,EAASqE,EAAM,EAAGnE,GACxB,MAAO,CACLA,aACA8F,MAAW,MAACyJ,EAAOC,WACH1P,GAAQqE,MAAMoL,EAAOC,GAGzC,CACA,MAAMM,EAAQ,IAAIzD,IAClB,MAAO,CACLrM,aAMA,KAAAmE,CAAMoL,EAAOC,GACX,MAAMrD,EAsBZ,SAAkBoD,EAAOC,EAAKO,GAC5B,GAAIR,EAAQ,EAAG,CACb,QAAYtC,IAARuC,EAAmB,MAAM,IAAI1R,MAAM,yBAAyByR,MAAUC,MAC1E,YAAavC,IAAT8C,EAA2B,GAAGR,KAC3B,GAAGQ,EAAOR,KAASQ,GAC5B,CAAO,QAAY9C,IAARuC,EAAmB,CAC5B,GAAID,EAAQC,EAAK,MAAM,IAAI1R,MAAM,wBAAwByR,MAAUC,MACnE,MAAO,GAAGD,KAASC,GACrB,CAAO,YAAavC,IAAT8C,EACF,GAAGR,KAEH,GAAGA,KAASQ,GAEvB,CAnCkBC,CAAST,EAAOC,EAAKxP,GAC3BiQ,EAASH,EAAM5C,IAAIf,GACzB,GAAI8D,EAAQ,OAAOA,EAEnB,MAAMC,EAAU/L,EAAMoL,EAAOC,GAE7B,OADAM,EAAMhJ,IAAIqF,EAAK+D,GACRA,CACT,EAEJ,CAkCO,SAASC,EAAQC,GACtB,IAAKA,EAAQ,MAAO,GACpB,GAAsB,IAAlBA,EAAOzP,OAAc,OAAOyP,EAAO,GAEvC,MAAM5P,EAAS,GACf,IAAK,MAAM6P,KAASD,EAClB/B,EAAO7N,EAAQ6P,GAEjB,OAAO7P,CACT,CC3IO,SAAS8P,GAAe/G,uBAAEA,EAAsBF,iBAAEA,EAAgBF,sBAAEA,IACzE,MAAMoH,EAAehH,GAA0BF,EAC/C,MAAO,CACLmH,UAAWtR,OAAOqR,GAClBE,QAASvR,OAAOqR,EAAepH,GAEnC,CC/DO,SAASuH,EAAclQ,EAAQmQ,EAAkBC,EAAkBtL,EAAQvB,GAChF,MAAM8M,EAAIF,GAAkBhQ,QAAUiQ,EAAiBjQ,OACvD,IAAKkQ,EAAG,OAAOvL,EACf,MAAMwL,EAAqB5M,EAAsBH,GAC3CgN,EAAiBhN,EAAWrC,KAAI,EAAGb,aAAcA,EAAQoD,kBAC/D,IAAI+M,EAAa,EAGjB,MAAMC,EAAiB,CAACzQ,GACxB,IAAI0Q,EAAmB1Q,EACnB2Q,EAAe,EACfC,EAAkB,EAClBC,EAAkB,EAEtB,GAAIT,EAAiB,GAEnB,KAAOO,EAAeJ,EAAepQ,OAAS,GAAK0Q,EAAkBT,EAAiB,IACpFO,IACqC,aAAjCJ,EAAeI,KAEjBD,EAAmBA,EAAiBI,IAAG,GACvCL,EAAevT,KAAKwT,GACpBE,KAEmC,aAAjCL,EAAeI,IAA8BE,IAIrD,IAAK,IAAI7T,EAAI,EAAGA,EAAIqT,EAAGrT,IAAK,CAE1B,MAAM+T,EAAMZ,GAAkBhQ,OAASgQ,EAAiBnT,GAAKsT,EACvDU,EAAMZ,EAAiBpT,GAG7B,KAAO2T,IAAiBK,EAAMH,GAAoD,aAAjCN,EAAeI,KACzB,aAAjCJ,EAAeI,KACjBF,EAAeQ,MACfL,KAEmC,aAAjCL,EAAeI,IAA8BE,IACjDF,IAMF,IAHAD,EAAmBD,EAAeK,IAAG,IAIlCH,EAAeJ,EAAepQ,OAAS,GAA0C,aAArCoQ,EAAeI,EAAe,MAC1EC,EAAkBG,GAA4C,aAArCR,EAAeI,EAAe,KACxD,CAEA,GADAA,IACqC,aAAjCJ,EAAeI,GAA8B,CAE/C,MAAMO,EAAU,GAChBR,EAAiBxT,KAAKgU,GACtBR,EAAmBQ,EACnBT,EAAevT,KAAKgU,GACpBN,GACF,CACqC,aAAjCL,EAAeI,IAA8BE,GACnD,CAGIE,IAAQT,EAEVI,EAAiBxT,KAAK4H,EAAO0L,MACpBG,IAAiBJ,EAAepQ,OAAS,EAClDuQ,EAAiBxT,KAAK,MAEtBwT,EAAiBxT,KAAK,GAE1B,CAGA,IAAK8C,EAAOG,OAEV,IAAK,IAAInD,EAAI,EAAGA,EAAIsT,EAAoBtT,IAAK,CAE3C,MAAMkU,EAAU,GAChBR,EAAiBxT,KAAKgU,GACtBR,EAAmBQ,CACrB,CAGF,OAAOlR,CACT,CAUO,SAASmR,GAAeC,EAAe1O,EAAQ2O,EAAQ,GAC5D,MAAMzO,EAAOF,EAAOE,KAAK0O,KAAK,KACxBC,EAA8C,aAAnC7O,EAAOrC,QAAQoD,gBAC1B+N,EAAYD,EAAWF,EAAQ,EAAIA,EAEzC,GN7BK,SAAoB3O,GACzB,IAAKA,EAAQ,OAAO,EACpB,GAAsC,SAAlCA,EAAOrC,QAAQG,eAA2B,OAAO,EACrD,GAAIkC,EAAOG,SAAS1C,OAAS,EAAG,OAAO,EAEvC,MAAMsR,EAAa/O,EAAOG,SAAS,GACnC,QAAI4O,EAAW5O,SAAS1C,OAAS,IACU,aAAvCsR,EAAWpR,QAAQoD,eAGzB,CMmBMiO,CAAWhP,GAAS,CACtB,IAAIiP,EAAUjP,EAAOG,SAAS,GAC1B+O,EAAWJ,EACiB,IAA5BG,EAAQ9O,SAAS1C,SACnBwR,EAAUA,EAAQ9O,SAAS,GAC3B+O,KAEFT,GAAeC,EAAeO,EAASC,GAEvC,MAAMC,EAAYF,EAAQ/O,KAAK0O,KAAK,KAC9BxM,EAASsM,EAAc1E,IAAImF,GACjC,IAAK/M,EAAQ,MAAM,IAAIxH,MAAM,sCAI7B,OAHIiU,GAAUO,GAAehN,EAAQuM,GACrCD,EAAc9K,IAAI1D,EAAMkC,QACxBsM,EAAcW,OAAOF,EAEvB,CAEA,GN7BK,SAAmBnP,GACxB,IAAKA,EAAQ,OAAO,EACpB,GAAsC,QAAlCA,EAAOrC,QAAQG,eAA0B,OAAO,EACpD,GAAIkC,EAAOG,SAAS1C,OAAS,EAAG,OAAO,EAEvC,MAAMsR,EAAa/O,EAAOG,SAAS,GACnC,GAAmC,IAA/B4O,EAAW5O,SAAS1C,OAAc,OAAO,EAC7C,GAA2C,aAAvCsR,EAAWpR,QAAQoD,gBAAgC,OAAO,EAE9D,MAAMuO,EAAWP,EAAW5O,SAASQ,MAAKL,GAAgC,QAAvBA,EAAM3C,QAAQ4C,OACjE,GAA0C,aAAtC+O,GAAU3R,QAAQoD,gBAAgC,OAAO,EAE7D,MAAMwO,EAAaR,EAAW5O,SAASQ,MAAKL,GAAgC,UAAvBA,EAAM3C,QAAQ4C,OACnE,MAA4C,aAAxCgP,GAAY5R,QAAQoD,eAG1B,CMaMyO,CAAUxP,GAAS,CACrB,MAAMyP,EAAUzP,EAAOG,SAAS,GAAGxC,QAAQ4C,KAG3CkO,GAAeC,EAAe1O,EAAOG,SAAS,GAAGA,SAAS,GAAI2O,EAAY,GAC1EL,GAAeC,EAAe1O,EAAOG,SAAS,GAAGA,SAAS,GAAI2O,EAAY,GAE1E,MAAMY,EAAOhB,EAAc1E,IAAI,GAAG9J,KAAQuP,SACpCrN,EAASsM,EAAc1E,IAAI,GAAG9J,KAAQuP,WAE5C,IAAKC,EAAM,MAAM,IAAI9U,MAAM,mCAC3B,IAAKwH,EAAQ,MAAM,IAAIxH,MAAM,qCAC7B,GAAI8U,EAAKjS,SAAW2E,EAAO3E,OACzB,MAAM,IAAI7C,MAAM,gDAGlB,MAAM+U,EAAMC,GAAaF,EAAMtN,EAAQ0M,GAMvC,OALID,GAAUO,GAAeO,EAAKhB,GAElCD,EAAcW,OAAO,GAAGnP,KAAQuP,SAChCf,EAAcW,OAAO,GAAGnP,KAAQuP,gBAChCf,EAAc9K,IAAI1D,EAAMyP,EAE1B,CAGA,GAAI3P,EAAOG,SAAS1C,OAAQ,CAE1B,MAAMoS,EAAiD,aAAnC7P,EAAOrC,QAAQoD,gBAAiC4N,EAAQA,EAAQ,EAE9EmB,EAAS,CAAA,EACf,IAAK,MAAMxP,KAASN,EAAOG,SAAU,CACnCsO,GAAeC,EAAepO,EAAOuP,GACrC,MAAME,EAAYrB,EAAc1E,IAAI1J,EAAMJ,KAAK0O,KAAK,MACpD,IAAKmB,EAAW,MAAM,IAAInV,MAAM,qCAChCkV,EAAOxP,EAAM3C,QAAQ4C,MAAQwP,CAC/B,CAEA,IAAK,MAAMzP,KAASN,EAAOG,SACzBuO,EAAcW,OAAO/O,EAAMJ,KAAK0O,KAAK,MAGvC,MAAMoB,EAAWC,GAAaH,EAAQD,GAClChB,GAAUO,GAAeY,EAAUrB,GACvCD,EAAc9K,IAAI1D,EAAM8P,EAC1B,CACF,CAOA,SAASZ,GAAehR,EAAKuQ,GAC3B,IAAK,IAAIrU,EAAI,EAAGA,EAAI8D,EAAIX,OAAQnD,IAC1BqU,EACFS,GAAehR,EAAI9D,GAAIqU,EAAQ,GAE/BvQ,EAAI9D,GAAK8D,EAAI9D,GAAG,EAGtB,CAQA,SAASsV,GAAaF,EAAMtN,EAAQuM,GAClC,MAAMgB,EAAM,GACZ,IAAK,IAAIrV,EAAI,EAAGA,EAAIoV,EAAKjS,OAAQnD,IAC/B,GAAIqU,EACFgB,EAAInV,KAAKoV,GAAaF,EAAKpV,GAAI8H,EAAO9H,GAAIqU,EAAQ,SAElD,GAAIe,EAAKpV,GAAI,CAEX,MAAM4V,EAAM,CAAA,EACZ,IAAK,IAAIC,EAAI,EAAGA,EAAIT,EAAKpV,GAAGmD,OAAQ0S,IAAK,CACvC,MAAMzR,EAAQ0D,EAAO9H,GAAG6V,GACxBD,EAAIR,EAAKpV,GAAG6V,SAAgBpG,IAAVrL,EAAsB,KAAOA,CACjD,CACAiR,EAAInV,KAAK0V,EACX,MACEP,EAAInV,UAAKuP,GAIf,OAAO4F,CACT,CASA,SAASM,GAAaH,EAAQnB,GAC5B,MAAMe,EAAOpG,OAAOoG,KAAKI,GACnBrS,EAASqS,EAAOJ,EAAK,KAAKjS,OAC1BkS,EAAM,GACZ,IAAK,IAAIrV,EAAI,EAAGA,EAAImD,EAAQnD,IAAK,CAE/B,MAAM4V,EAAM,CAAA,EACZ,IAAK,MAAMjH,KAAOyG,EAAM,CACtB,GAAII,EAAO7G,GAAKxL,SAAWA,EAAQ,MAAM,IAAI7C,MAAM,gCACnDsV,EAAIjH,GAAO6G,EAAO7G,GAAK3O,EACzB,CACIqU,EACFgB,EAAInV,KAAKyV,GAAaC,EAAKvB,EAAQ,IAEnCgB,EAAInV,KAAK0V,EAEb,CACA,OAAOP,CACT,CC/OO,SAASS,GAAkBvW,EAAQU,EAAO+C,GAC/C,MAAM+S,EAAQ/S,aAAkB4B,WAC1BoR,EAAYvO,EAAWlI,GACvB0W,EAAoBxO,EAAWlI,GACrCkI,EAAWlI,GACX,IAAI6E,EAAQmD,EAAiBhI,GACzB2W,EAAc,EAClBlT,EAAOkT,KAAiBH,EAAQrU,OAAO0C,GAASA,EAEhD,MAAM+R,EAAqBH,EAAYC,EAEvC,KAAOC,EAAcjW,GAAO,CAE1B,MAAMmW,EAAW7O,EAAiBhI,GAC5B8W,EAAY,IAAIpT,WAAWgT,GACjC,IAAK,IAAIjW,EAAI,EAAGA,EAAIiW,EAAmBjW,IACrCqW,EAAUrW,GAAKT,EAAOgB,KAAKE,SAASlB,EAAOmB,UAG7C,IAAK,IAAIV,EAAI,EAAGA,EAAIiW,GAAqBC,EAAcjW,EAAOD,IAAK,CAEjE,MAAMuE,EAAWI,OAAO0R,EAAUrW,IAClC,GAAIuE,EAAU,CACZ,IAAI+R,EAAa,GACbC,EAAiBJ,EACrB,MAAMK,GAAQ,IAAMjS,GAAY,GAChC,KAAOgS,GAAkBL,EAAcjW,GAAO,CAC5C,IAAIiF,EAAOP,OAAOpF,EAAOgB,KAAKE,SAASlB,EAAOmB,UAAY4V,EAAaE,EAEvE,IADAF,GAAc/R,EACP+R,GAAc,GACnBA,GAAc,GACd/W,EAAOmB,SACH4V,IACFpR,GAAQP,OAAOpF,EAAOgB,KAAKE,SAASlB,EAAOmB,UAAY6D,EAAW+R,EAAaE,GAInFpS,GADcgS,EAAWlR,EAEzBlC,EAAOkT,KAAiBH,EAAQrU,OAAO0C,GAASA,EAChDmS,GACF,CACIA,IAEFhX,EAAOmB,QAAUG,KAAK4V,MAAMF,EAAiB7U,OAAO6C,GAAY7C,OAAO4U,IAAe,GAE1F,MACE,IAAK,IAAIT,EAAI,EAAGA,EAAIM,GAAsBD,EAAcjW,EAAO4V,IAC7DzR,GAASgS,EACTpT,EAAOkT,KAAiBH,EAAQrU,OAAO0C,GAASA,CAGtD,CACF,CACF,CAOO,SAASsS,GAAqBnX,EAAQU,EAAO+C,GAClD,MAAM2T,EAAU,IAAI/R,WAAW3E,GAC/B6V,GAAkBvW,EAAQU,EAAO0W,GACjC,IAAK,IAAI3W,EAAI,EAAGA,EAAIC,EAAOD,IACzBgD,EAAOhD,GAAK,IAAIiD,WAAW1D,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQiW,EAAQ3W,IAC/FT,EAAOmB,QAAUiW,EAAQ3W,EAE7B,CCnEO,SAASuE,GAASH,GACvB,OAAO,GAAKvD,KAAK+V,MAAMxS,EACzB,CAYO,SAASyS,GAAuBtX,EAAQuX,EAAO9T,EAAQG,QAC7CsM,IAAXtM,IACFA,EAAS5D,EAAOgB,KAAKK,UAAUrB,EAAOmB,QAAQ,GAC9CnB,EAAOmB,QAAU,GAEnB,MAAMqW,EAAcxX,EAAOmB,OAC3B,IAAIsW,EAAO,EACX,KAAOA,EAAOhU,EAAOG,QAAQ,CAC3B,MAAM8T,EAASxP,EAAWlI,GAC1B,GAAa,EAAT0X,EAEFD,EAAOE,GAAc3X,EAAQ0X,EAAQH,EAAO9T,EAAQgU,OAC/C,CAEL,MAAM/W,EAAQgX,IAAW,EACzBE,GAAQ5X,EAAQU,EAAO6W,EAAO9T,EAAQgU,GACtCA,GAAQ/W,CACV,CACF,CACAV,EAAOmB,OAASqW,EAAc5T,CAChC,CAWA,SAASgU,GAAQ5X,EAAQU,EAAOsE,EAAUvB,EAAQgU,GAChD,MAAMF,EAAQvS,EAAW,GAAK,EAC9B,IAAIH,EAAQ,EACZ,IAAK,IAAIpE,EAAI,EAAGA,EAAI8W,EAAO9W,IACzBoE,GAAS7E,EAAOgB,KAAKE,SAASlB,EAAOmB,YAAcV,GAAK,GAK1D,IAAK,IAAIA,EAAI,EAAGA,EAAIC,EAAOD,IACzBgD,EAAOgU,EAAOhX,GAAKoE,CAEvB,CAaA,SAAS8S,GAAc3X,EAAQ0X,EAAQ1S,EAAUvB,EAAQgU,GACvD,IAAI/W,EAAQgX,GAAU,GAAK,EAC3B,MAAMT,GAAQ,GAAKjS,GAAY,EAE/B,IAAI5B,EAAO,EACX,GAAIpD,EAAOmB,OAASnB,EAAOgB,KAAKiC,WAC9BG,EAAOpD,EAAOgB,KAAKE,SAASlB,EAAOmB,eAC9B,GAAI8V,EAET,MAAM,IAAIlW,MAAM,0BAA0Bf,EAAOmB,uBAEnD,IAAI0W,EAAO,EACPC,EAAQ,EAGZ,KAAOpX,GAEDoX,EAAQ,GACVA,GAAS,EACTD,GAAQ,EACRzU,KAAU,GACDyU,EAAOC,EAAQ9S,GAExB5B,GAAQpD,EAAOgB,KAAKE,SAASlB,EAAOmB,SAAW0W,EAC/C7X,EAAOmB,SACP0W,GAAQ,IAEJJ,EAAOhU,EAAOG,SAEhBH,EAAOgU,KAAUrU,GAAQ0U,EAAQb,GAEnCvW,IACAoX,GAAS9S,GAIb,OAAOyS,CACT,CASO,SAASM,GAAgB/X,EAAQU,EAAOP,EAAM6X,GACnD,MAAMT,EA6BR,SAAmBpX,EAAM6X,GACvB,OAAQ7X,GACR,IAAK,QACL,IAAK,QACH,OAAO,EACT,IAAK,QACL,IAAK,SACH,OAAO,EACT,IAAK,uBACH,IAAK6X,EAAY,MAAM,IAAIjX,MAAM,yCACjC,OAAOiX,EACT,QACE,MAAM,IAAIjX,MAAM,6BAA6BZ,KAEjD,CA3CgB8X,CAAU9X,EAAM6X,GACxBrV,EAAQ,IAAIe,WAAWhD,EAAQ6W,GACrC,IAAK,IAAIW,EAAI,EAAGA,EAAIX,EAAOW,IACzB,IAAK,IAAIzX,EAAI,EAAGA,EAAIC,EAAOD,IACzBkC,EAAMlC,EAAI8W,EAAQW,GAAKlY,EAAOgB,KAAKE,SAASlB,EAAOmB,UAIvD,GAAa,UAAThB,EAAkB,OAAO,IAAIgY,aAAaxV,EAAMI,QAC/C,GAAa,WAAT5C,EAAmB,OAAO,IAAIiY,aAAazV,EAAMI,QACrD,GAAa,UAAT5C,EAAkB,OAAO,IAAIkF,WAAW1C,EAAMI,QAClD,GAAa,UAAT5C,EAAkB,OAAO,IAAI+E,cAAcvC,EAAMI,QACrD,GAAa,yBAAT5C,EAAiC,CAExC,MAAMkY,EAAQ,IAAI7T,MAAM9D,GACxB,IAAK,IAAID,EAAI,EAAGA,EAAIC,EAAOD,IACzB4X,EAAM5X,GAAKkC,EAAM2V,SAAS7X,EAAI8W,GAAQ9W,EAAI,GAAK8W,GAEjD,OAAOc,CACT,CACA,MAAM,IAAItX,MAAM,+CAA+CZ,IACjE,CCzIO,SAASoY,GAAUvY,EAAQG,EAAMO,EAAO8X,GAC7C,GAAc,IAAV9X,EAAa,MAAO,GACxB,GAAa,YAATP,EACF,OA4BJ,SAA0BH,EAAQU,GAChC,MAAM6H,EAAS,IAAI/D,MAAM9D,GACzB,IAAK,IAAID,EAAI,EAAGA,EAAIC,EAAOD,IAAK,CAC9B,MAAMuC,EAAahD,EAAOmB,QAAUV,EAAI,EAAI,GACtCgY,EAAYhY,EAAI,EAChBiF,EAAO1F,EAAOgB,KAAKE,SAAS8B,GAClCuF,EAAO9H,MAAMiF,EAAO,GAAK+S,EAC3B,CAEA,OADAzY,EAAOmB,QAAUG,KAAK4V,KAAKxW,EAAQ,GAC5B6H,CACT,CAtCWmQ,CAAiB1Y,EAAQU,GAC3B,GAAa,UAATP,EACT,OA6CJ,SAAwBH,EAAQU,GAC9B,MAAM6H,GAAUvI,EAAOgB,KAAKgC,WAAahD,EAAOmB,QAAU,EACtD,IAAIkE,WAAWsT,GAAM3Y,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAgB,EAART,IACjF,IAAI2E,WAAWrF,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQT,GAE/E,OADAV,EAAOmB,QAAkB,EAART,EACV6H,CACT,CAnDWqQ,CAAe5Y,EAAQU,GACzB,GAAa,UAATP,EACT,OA0DJ,SAAwBH,EAAQU,GAC9B,MAAM6H,GAAUvI,EAAOgB,KAAKgC,WAAahD,EAAOmB,QAAU,EACtD,IAAI+D,cAAcyT,GAAM3Y,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAgB,EAART,IACpF,IAAIwE,cAAclF,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQT,GAElF,OADAV,EAAOmB,QAAkB,EAART,EACV6H,CACT,CAhEWsQ,CAAe7Y,EAAQU,GACzB,GAAa,UAATP,EACT,OAuEJ,SAAwBH,EAAQU,GAC9B,MAAM6H,EAAS,IAAI/D,MAAM9D,GACzB,IAAK,IAAID,EAAI,EAAGA,EAAIC,EAAOD,IAAK,CAC9B,MAAMqY,EAAM9Y,EAAOgB,KAAKqQ,YAAYrR,EAAOmB,OAAa,GAAJV,GAAQ,GACtDsY,EAAO/Y,EAAOgB,KAAKoQ,SAASpR,EAAOmB,OAAa,GAAJV,EAAS,GAAG,GAC9D8H,EAAO9H,GAAK2E,OAAO2T,IAAS,IAAMD,CACpC,CAEA,OADA9Y,EAAOmB,QAAkB,GAART,EACV6H,CACT,CAhFWyQ,CAAehZ,EAAQU,GACzB,GAAa,UAATP,EACT,OAuFJ,SAAwBH,EAAQU,GAC9B,MAAM6H,GAAUvI,EAAOgB,KAAKgC,WAAahD,EAAOmB,QAAU,EACtD,IAAIgX,aAAaQ,GAAM3Y,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAgB,EAART,IACnF,IAAIyX,aAAanY,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQT,GAEjF,OADAV,EAAOmB,QAAkB,EAART,EACV6H,CACT,CA7FW0Q,CAAejZ,EAAQU,GACzB,GAAa,WAATP,EACT,OAoGJ,SAAyBH,EAAQU,GAC/B,MAAM6H,GAAUvI,EAAOgB,KAAKgC,WAAahD,EAAOmB,QAAU,EACtD,IAAIiX,aAAaO,GAAM3Y,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAgB,EAART,IACnF,IAAI0X,aAAapY,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQT,GAEjF,OADAV,EAAOmB,QAAkB,EAART,EACV6H,CACT,CA1GW2Q,CAAgBlZ,EAAQU,GAC1B,GAAa,eAATP,EACT,OAiHJ,SAA4BH,EAAQU,GAClC,MAAM6H,EAAS,IAAI/D,MAAM9D,GACzB,IAAK,IAAID,EAAI,EAAGA,EAAIC,EAAOD,IAAK,CAC9B,MAAMmD,EAAS5D,EAAOgB,KAAKK,UAAUrB,EAAOmB,QAAQ,GACpDnB,EAAOmB,QAAU,EACjBoH,EAAO9H,GAAK,IAAIiD,WAAW1D,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQyC,GACvF5D,EAAOmB,QAAUyC,CACnB,CACA,OAAO2E,CACT,CA1HW4Q,CAAmBnZ,EAAQU,GAC7B,GAAa,yBAATP,EAAiC,CAC1C,IAAKqY,EAAa,MAAM,IAAIzX,MAAM,gCAClC,OAiIJ,SAAiCf,EAAQU,EAAO8X,GAE9C,MAAMjQ,EAAS,IAAI/D,MAAM9D,GACzB,IAAK,IAAID,EAAI,EAAGA,EAAIC,EAAOD,IACzB8H,EAAO9H,GAAK,IAAIiD,WAAW1D,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQqX,GACvFxY,EAAOmB,QAAUqX,EAEnB,OAAOjQ,CACT,CAzIW6Q,CAAwBpZ,EAAQU,EAAO8X,EAChD,CACE,MAAM,IAAIzX,MAAM,2BAA2BZ,IAE/C,CAgJA,SAASwY,GAAM5V,EAAQ5B,EAAQ6R,GAC7B,MAAMqG,EAAU,IAAIxP,YAAYmJ,GAEhC,OADA,IAAItP,WAAW2V,GAAStP,IAAI,IAAIrG,WAAWX,EAAQ5B,EAAQ6R,IACpDqG,CACT,CC7KA,MAAMC,GAAY,CAAC,EAAG,IAAM,MAAQ,SAAU,YAW9C,SAASC,GAAUC,EAAWC,EAASC,EAASC,EAAO/V,GACrD,IAAK,IAAInD,EAAI,EAAGA,EAAImD,EAAQnD,IAC1BiZ,EAAQC,EAAQlZ,GAAK+Y,EAAUC,EAAUhZ,EAE7C,CCPO,SAASmZ,GAAajX,EAAOkX,GAAM1Z,KAAEA,EAAI2D,QAAEA,EAAOkD,WAAEA,IACzD,MAAMhG,EAAO,IAAI8B,SAASH,EAAMI,OAAQJ,EAAMK,WAAYL,EAAMM,YAC1DjD,EAAS,CAAEgB,OAAMG,OAAQ,GAE/B,IAAI2Y,EAGJ,MAAMjG,EAkDR,SAA8B7T,EAAQ6Z,EAAM7S,GAC1C,GAAIA,EAAWpD,OAAS,EAAG,CACzB,MAAMmW,EAAqBhT,EAAsBC,GACjD,GAAI+S,EAAoB,CACtB,MAAMxR,EAAS,IAAI/D,MAAMqV,EAAK3N,YAE9B,OADAoL,GAAuBtX,EAAQgF,GAAS+U,GAAqBxR,GACtDA,CACT,CACF,CACA,MAAO,EACT,CA5D2ByR,CAAqBha,EAAQ6Z,EAAM7S,IAEtD4M,iBAAEA,EAAgBqG,SAAEA,GAkE5B,SAA8Bja,EAAQ6Z,EAAM7S,GAC1C,MAAM+M,EAAqB5M,EAAsBH,GACjD,IAAK+M,EAAoB,MAAO,CAAEH,iBAAkB,GAAIqG,SAAU,GAElE,MAAMrG,EAAmB,IAAIpP,MAAMqV,EAAK3N,YACxCoL,GAAuBtX,EAAQgF,GAAS+O,GAAqBH,GAG7D,IAAIqG,EAAWJ,EAAK3N,WACpB,IAAK,MAAMsI,KAAOZ,EACZY,IAAQT,GAAoBkG,IAEjB,IAAbA,IAAgBrG,EAAiBhQ,OAAS,GAE9C,MAAO,CAAEgQ,mBAAkBqG,WAC7B,CAjFyCC,CAAqBla,EAAQ6Z,EAAM7S,GAIpEmT,EAAUN,EAAK3N,WAAa+N,EAClC,GAAsB,UAAlBJ,EAAKvW,SACPwW,EAAWvB,GAAUvY,EAAQG,EAAMga,EAASrW,EAAQ0G,kBAC/C,GACa,qBAAlBqP,EAAKvW,UACa,mBAAlBuW,EAAKvW,UACa,QAAlBuW,EAAKvW,SACL,CACA,MAAM0B,EAAoB,YAAT7E,EAAqB,EAAIa,EAAKE,SAASlB,EAAOmB,UAC3D6D,GACF8U,EAAW,IAAItV,MAAM2V,GACR,YAATha,GACFmX,GAAuBtX,EAAQgF,EAAU8U,GACzCA,EAAWA,EAASnV,KAAIyV,KAAOA,KAG/B9C,GAAuBtX,EAAQgF,EAAU8U,EAAU9Y,EAAKiC,WAAajD,EAAOmB,SAG9E2Y,EAAW,IAAIpW,WAAWyW,EAE9B,MAAO,GAAsB,sBAAlBN,EAAKvW,SACdwW,EAAW/B,GAAgB/X,EAAQma,EAASha,EAAM2D,EAAQ0G,kBACrD,GAAsB,wBAAlBqP,EAAKvW,SAAoC,CAElDwW,EADuB,UAAT3Z,EACK,IAAIkF,WAAW8U,GAAW,IAAIjV,cAAciV,GAC/D5D,GAAkBvW,EAAQma,EAASL,EACrC,KAAO,IAAsB,4BAAlBD,EAAKvW,SAId,MAAM,IAAIvC,MAAM,iCAAiC8Y,EAAKvW,YAHtDwW,EAAW,IAAItV,MAAM2V,GACrBhD,GAAqBnX,EAAQma,EAASL,EAGxC,CAEA,MAAO,CAAElG,mBAAkBC,mBAAkBiG,WAC/C,CAmDO,SAASO,GAAeC,EAAiBC,EAAwBtO,EAAOuO,GAE7E,IAAIC,EACJ,MAAMC,EAAqBF,IAAcvO,GACzC,GAAc,iBAAVA,EACFwO,EAAOH,OACF,GAAII,EACTD,EAAOC,EAAmBJ,EAAiBC,OACtC,IAAc,WAAVtO,EAIT,MAAM,IAAIlL,MAAM,0CAA0CkL,KAH1DwO,EAAO,IAAI/W,WAAW6W,GD5FnB,SAA0BI,EAAOlX,GACtC,MAAMmX,EAAcD,EAAM1X,WACpB4X,EAAepX,EAAOR,WAC5B,IAAI6X,EAAM,EACNC,EAAS,EAGb,KAAOD,EAAMF,GAAa,CACxB,MAAMI,EAAIL,EAAMG,GAEhB,GADAA,IACIE,EAAI,IACN,KAEJ,CACA,GAAIH,GAAgBC,GAAOF,EACzB,MAAM,IAAI7Z,MAAM,gCAGlB,KAAO+Z,EAAMF,GAAa,CACxB,MAAMI,EAAIL,EAAMG,GAChB,IAAIG,EAAM,EAGV,GAFAH,IAEIA,GAAOF,EACT,MAAM,IAAI7Z,MAAM,sBAIlB,GAAS,EAAJia,EAsBE,CAEL,IAAI7Z,EAAS,EACb,OAAY,EAAJ6Z,GACR,KAAK,EAEHC,EAAwB,GAAjBD,IAAM,EAAI,GACjB7Z,EAASwZ,EAAMG,IAAQE,IAAM,GAAK,GAClCF,IACA,MACF,KAAK,EAEH,GAAIF,GAAeE,EAAM,EACvB,MAAM,IAAI/Z,MAAM,6BAElBka,EAAkB,GAAXD,IAAM,GACb7Z,EAASwZ,EAAMG,IAAQH,EAAMG,EAAM,IAAM,GACzCA,GAAO,EACP,MACF,KAAK,EAEH,GAAIF,GAAeE,EAAM,EACvB,MAAM,IAAI/Z,MAAM,6BAElBka,EAAkB,GAAXD,IAAM,GACb7Z,EAASwZ,EAAMG,IACVH,EAAMG,EAAM,IAAM,IAClBH,EAAMG,EAAM,IAAM,KAClBH,EAAMG,EAAM,IAAM,IACvBA,GAAO,EAKT,GAAe,IAAX3Z,GAAgB+Z,MAAM/Z,GACxB,MAAM,IAAIJ,MAAM,kBAAkBI,SAAc2Z,iBAAmBF,KAErE,GAAIzZ,EAAS4Z,EACX,MAAM,IAAIha,MAAM,2CAElBwY,GAAU9V,EAAQsX,EAAS5Z,EAAQsC,EAAQsX,EAAQE,GACnDF,GAAUE,CACZ,KAhEqB,CAEnB,IAAIA,EAAkB,GAAXD,IAAM,GAEjB,GAAIC,EAAM,GAAI,CACZ,GAAIH,EAAM,GAAKF,EACb,MAAM,IAAI7Z,MAAM,+CAElB,MAAMoa,EAAaF,EAAM,GACzBA,EAAMN,EAAMG,IACPH,EAAMG,EAAM,IAAM,IAClBH,EAAMG,EAAM,IAAM,KAClBH,EAAMG,EAAM,IAAM,IACvBG,EAAsC,GAA/BA,EAAM3B,GAAU6B,IACvBL,GAAOK,CACT,CACA,GAAIL,EAAMG,EAAML,EACd,MAAM,IAAI7Z,MAAM,6CAElBwY,GAAUoB,EAAOG,EAAKrX,EAAQsX,EAAQE,GACtCH,GAAOG,EACPF,GAAUE,CACZ,CA2CF,CAEA,GAAIF,IAAWF,EAAc,MAAM,IAAI9Z,MAAM,yBAC/C,CCHIqa,CAAiBd,EAAiBG,EAGpC,CACA,GAAIA,GAAM7W,SAAW2W,EACnB,MAAM,IAAIxZ,MAAM,oCAAoC0Z,GAAM7W,gCAAgC2W,KAE5F,OAAOE,CACT,CAWO,SAASY,GAAef,EAAiBgB,EAAI/X,GAClD,MACMvD,EAAS,CAAEgB,KADJ,IAAI8B,SAASwX,EAAgBvX,OAAQuX,EAAgBtX,WAAYsX,EAAgBrX,YACvE9B,OAAQ,IACzBhB,KAAEA,EAAI2D,QAAEA,EAAOkD,WAAEA,EAAUiF,MAAEA,EAAKuO,YAAEA,GAAgBjX,EACpDgY,EAAQD,EAAGE,oBACjB,IAAKD,EAAO,MAAM,IAAIxa,MAAM,4CAG5B,MAAM8S,EA2DR,SAAgC7T,EAAQub,EAAOvU,GAC7C,MAAM+S,EAAqBhT,EAAsBC,GACjD,IAAK+S,EAAoB,MAAO,GAEhC,MAAMxR,EAAS,IAAI/D,MAAM+W,EAAMrP,YAE/B,OADAoL,GAAuBtX,EAAQgF,GAAS+U,GAAqBxR,EAAQgT,EAAME,+BACpElT,CACT,CAlE2BmT,CAAuB1b,EAAQub,EAAOvU,GAC/DhH,EAAOmB,OAASoa,EAAME,8BAGtB,MAAM7H,EAsER,SAAgC5T,EAAQub,EAAOvU,GAC7C,MAAM+M,EAAqB5M,EAAsBH,GACjD,GAAI+M,EAAoB,CAEtB,MAAMxL,EAAS,IAAI/D,MAAM+W,EAAMrP,YAE/B,OADAoL,GAAuBtX,EAAQgF,GAAS+O,GAAqBxL,EAAQgT,EAAMI,+BACpEpT,CACT,CACF,CA9E2BqT,CAAuB5b,EAAQub,EAAOvU,GAGzD6U,EAAuBP,EAAGf,uBAAyBgB,EAAMI,8BAAgCJ,EAAME,8BAErG,IAAIhB,EAAOH,EAAgBhC,SAAStY,EAAOmB,SACf,IAAxBoa,EAAMO,gBACRrB,EAAOJ,GAAeI,EAAMoB,EAAsB5P,EAAOuO,IAE3D,MAAMuB,EAAW,IAAIjZ,SAAS2X,EAAK1X,OAAQ0X,EAAKzX,WAAYyX,EAAKxX,YAC3D+Y,EAAa,CAAEhb,KAAM+a,EAAU5a,OAAQ,GAI7C,IAAI2Y,EACJ,MAAMK,EAAUoB,EAAMrP,WAAaqP,EAAMU,UACzC,GAAuB,UAAnBV,EAAMjY,SACRwW,EAAWvB,GAAUyD,EAAY7b,EAAMga,EAASrW,EAAQ0G,kBACnD,GAAuB,QAAnB+Q,EAAMjY,SAEfwW,EAAW,IAAItV,MAAM2V,GACrB7C,GAAuB0E,EAAY,EAAGlC,GACtCA,EAAWA,EAASnV,KAAIyV,KAAOA,SAC1B,GACc,qBAAnBmB,EAAMjY,UACa,mBAAnBiY,EAAMjY,SACN,CACA,MAAM0B,EAAW+W,EAAS7a,SAAS8a,EAAW7a,UAC9C2Y,EAAW,IAAItV,MAAM2V,GACrB7C,GAAuB0E,EAAYhX,EAAU8U,EAAU+B,EAAuB,EAChF,MAAO,GAAuB,wBAAnBN,EAAMjY,SAAoC,CAEnDwW,EADuB,UAAT3Z,EACK,IAAIkF,WAAW8U,GAAW,IAAIjV,cAAciV,GAC/D5D,GAAkByF,EAAY7B,EAASL,EACzC,MAAO,GAAuB,4BAAnByB,EAAMjY,SACfwW,EAAW,IAAItV,MAAM2V,GACrBhD,GAAqB6E,EAAY7B,EAASL,QACrC,GAAuB,qBAAnByB,EAAMjY,SACfwW,EAAW,IAAItV,MAAM2V,GJ9GlB,SAAwBna,EAAQU,EAAO+C,GAC5C,MAAMyY,EAAa,IAAI7W,WAAW3E,GAClC6V,GAAkBvW,EAAQU,EAAOwb,GACjC,MAAMC,EAAa,IAAI9W,WAAW3E,GAClC6V,GAAkBvW,EAAQU,EAAOyb,GAEjC,IAAK,IAAI1b,EAAI,EAAGA,EAAIC,EAAOD,IAAK,CAC9B,MAAM2b,EAAS,IAAI1Y,WAAW1D,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQgb,EAAW1b,IACjGyb,EAAWzb,IAEbgD,EAAOhD,GAAK,IAAIiD,WAAWwY,EAAWzb,GAAK0b,EAAW1b,IACtDgD,EAAOhD,GAAGsJ,IAAItG,EAAOhD,EAAI,GAAG6X,SAAS,EAAG4D,EAAWzb,KACnDgD,EAAOhD,GAAGsJ,IAAIqS,EAAQF,EAAWzb,KAEjCgD,EAAOhD,GAAK2b,EAEdpc,EAAOmB,QAAUgb,EAAW1b,EAC9B,CACF,CI6FI4b,CAAeL,EAAY7B,EAASL,OAC/B,IAAuB,sBAAnByB,EAAMjY,SAGf,MAAM,IAAIvC,MAAM,iCAAiCwa,EAAMjY,YAFvDwW,EAAW/B,GAAgB/X,EAAQma,EAASha,EAAM2D,EAAQ0G,YAG5D,CAEA,MAAO,CAAEoJ,mBAAkBC,mBAAkBiG,WAC/C,CCxLO,SAASwC,GAAWtc,GAAQuc,WAAEA,EAAUC,YAAEA,EAAWC,UAAEA,GAAalZ,EAAemZ,GACxF,MAAMC,WAAEA,EAAU3V,WAAEA,GAAezD,EAC7BqZ,EAASvV,EAAaL,GAEtBqM,EAAS,GAEf,IAAIhQ,EAEAwZ,EACAC,EAAW,EAEf,MAAMC,EAAgBL,SACpBG,GAAaH,EAAO,CAClBC,aACAK,WAAYH,EACZI,SAAUV,EAAaO,EAAWD,EAAUjZ,OAC5CsZ,OAAQX,EAAaO,GAExB,GAED,MAAOF,EAASE,EAAWL,EAAYzc,EAAOmB,OAASnB,EAAOgB,KAAKiC,WAAa,MAC1EjD,EAAOmB,QAAUnB,EAAOgB,KAAKiC,WAAa,IADmC,CAIjF,MAAMyU,EAASyF,GAAcnd,GAC7B,GAAoB,oBAAhB0X,EAAOvX,KAETkD,EAAa+Z,GAASpd,EAAQ0X,EAAQnU,EAAeF,OAAY6M,EAAW,GAC5E7M,EAAaQ,EAAQR,EAAYE,OAC5B,CACL,MAAM8Z,EAAkBR,GAAWjZ,QAAU,EACvC2E,EAAS6U,GAASpd,EAAQ0X,EAAQnU,EAAeF,EAAYwZ,EAAWL,EAAcM,GACxFD,IAActU,EAEhBuU,GAAYvU,EAAO3E,OAASyZ,GAE5BN,MACA1J,EAAO1S,KAAK4H,GACZuU,GAAYvU,EAAO3E,OACnBiZ,EAAYtU,EAEhB,CACF,CAOA,OANAwU,MAEID,EAAWL,GAAaI,IAE1BxJ,EAAOA,EAAOzP,OAAS,GAAKiZ,EAAUzV,MAAM,EAAGqV,GAAaK,EAAWD,EAAUjZ,UAE5EyP,CACT,CAaO,SAAS+J,GAASpd,EAAQ0X,EAAQnU,EAAeF,EAAYia,EAAeC,GACjF,MAAMpd,KAAEA,EAAI2D,QAAEA,EAAOkD,WAAEA,EAAUiF,MAAEA,EAAKuO,YAAEA,GAAgBjX,EAEpD+W,EAAkB,IAAI5W,WAC1B1D,EAAOgB,KAAK+B,OAAQ/C,EAAOgB,KAAKgC,WAAahD,EAAOmB,OAAQuW,EAAO8F,sBAKrE,GAHAxd,EAAOmB,QAAUuW,EAAO8F,qBAGJ,cAAhB9F,EAAOvX,KAAsB,CAC/B,MAAM0Z,EAAOnC,EAAO+F,iBACpB,IAAK5D,EAAM,MAAM,IAAI9Y,MAAM,yCAG3B,GAAIwc,EAAY1D,EAAK3N,YAAc7E,EAAaL,GAC9C,OAAO,IAAIxC,MAAMqV,EAAK3N,YAGxB,MAAMuO,EAAOJ,GAAeC,EAAiBnY,OAAOuV,EAAO6C,wBAAyBtO,EAAOuO,IACrF5G,iBAAEA,EAAgBC,iBAAEA,EAAgBiG,SAAEA,GAAaF,GAAaa,EAAMZ,EAAMtW,GAIlF,IAAIgF,EAASpF,EAAsB2W,EAAUzW,EAAYwW,EAAKvW,SAAUC,GACxE,GAAIsQ,EAAiBjQ,QAAUgQ,GAAkBhQ,OAAQ,CAEvD,OAAO+P,EADQnP,MAAMkZ,QAAQJ,GAAiBA,EAAgB,GACjC1J,EAAkBC,EAAkBtL,EAAQvB,EAC3E,CAEE,IAAK,IAAIvG,EAAI,EAAGA,EAAIuG,EAAWpD,OAAQnD,IACS,aAA1CuG,EAAWvG,GAAGqD,QAAQoD,kBACxBqB,EAAS/D,MAAME,KAAK6D,GAAQ+C,GAAK,CAACA,MAGtC,OAAO/C,CAEX,CAAO,GAAoB,iBAAhBmP,EAAOvX,KAAyB,CACzC,MAAMob,EAAQ7D,EAAO8D,oBACrB,IAAKD,EAAO,MAAM,IAAIxa,MAAM,4CAG5B,GAAIwc,EAAYhC,EAAMhQ,SACpB,OAAO,IAAI/G,MAAM+W,EAAMrP,YAGzB,MAAM0H,iBAAEA,EAAgBC,iBAAEA,EAAgBiG,SAAEA,GAC1CuB,GAAef,EAAiB5C,EAAQnU,GAGpCgF,EAASpF,EAAsB2W,EAAUzW,EAAYkY,EAAMjY,SAAUC,GAE3E,OAAOoQ,EADQnP,MAAMkZ,QAAQJ,GAAiBA,EAAgB,GACjC1J,EAAkBC,EAAkBtL,EAAQvB,EAC3E,CAAO,GAAoB,oBAAhB0Q,EAAOvX,KAA4B,CAC5C,MAAMwd,EAAOjG,EAAOkG,uBACpB,IAAKD,EAAM,MAAM,IAAI5c,MAAM,+CAE3B,MAAM0Z,EAAOJ,GACXC,EAAiBnY,OAAOuV,EAAO6C,wBAAyBtO,EAAOuO,GAIjE,OAAOjC,GADQ,CAAEvX,KAAM,IAAI8B,SAAS2X,EAAK1X,OAAQ0X,EAAKzX,WAAYyX,EAAKxX,YAAa9B,OAAQ,GACnEhB,EAAMwd,EAAKzR,WAAYpI,EAAQ0G,YAC1D,CACE,MAAM,IAAIzJ,MAAM,kCAAkC2W,EAAOvX,OAE7D,CASA,SAASgd,GAAcnd,GACrB,MAAM0X,EAASlQ,EAA4BxH,GAsC3C,MAAO,CACLG,KApCWN,EAAS6X,EAAOrN,SAqC3BkQ,uBApC6B7C,EAAOpN,QAqCpCkT,qBApC2B9F,EAAOjN,QAqClCoT,IApCUnG,EAAOhN,QAqCjB+S,iBApCuB/F,EAAO/M,SAAW,CACzCuB,WAAYwL,EAAO/M,QAAQN,QAC3B/G,SAAU7D,EAASiY,EAAO/M,QAAQL,SAClCwT,0BAA2Bre,EAASiY,EAAO/M,QAAQF,SACnDsT,0BAA2Bte,EAASiY,EAAO/M,QAAQD,SACnDgC,WAAYgL,EAAO/M,QAAQA,SAAW,CACpCrB,IAAKoO,EAAO/M,QAAQA,QAAQN,QAC5BuG,IAAK8G,EAAO/M,QAAQA,QAAQL,QAC5BuG,WAAY6G,EAAO/M,QAAQA,QAAQF,QACnCqG,eAAgB4G,EAAO/M,QAAQA,QAAQD,QACvCqG,UAAW2G,EAAO/M,QAAQA,QAAQA,QAClCqG,UAAW0G,EAAO/M,QAAQA,QAAQC,UA0BpCoT,kBAvBwBtG,EAAO9M,QAwB/BgT,uBAvB6BlG,EAAO7M,SAAW,CAC/CqB,WAAYwL,EAAO7M,QAAQR,QAC3B/G,SAAU7D,EAASiY,EAAO7M,QAAQP,SAClC2T,UAAWvG,EAAO7M,QAAQJ,SAqB1B+Q,oBAnB0B9D,EAAO3M,SAAW,CAC5CmB,WAAYwL,EAAO3M,QAAQV,QAC3B4R,UAAWvE,EAAO3M,QAAQT,QAC1BiB,SAAUmM,EAAO3M,QAAQN,QACzBnH,SAAU7D,EAASiY,EAAO3M,QAAQL,SAClCiR,8BAA+BjE,EAAO3M,QAAQJ,QAC9C8Q,8BAA+B/D,EAAO3M,QAAQH,QAC9CkR,mBAA0C5L,IAA3BwH,EAAO3M,QAAQF,SAA+B6M,EAAO3M,QAAQF,QAC5E6B,WAAYgL,EAAO3M,QAAQA,SAa/B,CCtGOhC,eAAemV,IAAiBC,aAAEA,GAAgB3B,EAAaC,EAAW/Q,EAAS0S,GAGxF,MAAMC,QAAoBC,QAAQC,IAAIJ,EAAaxZ,KAAI,EAAGvB,UAAWA,EAAK4O,KAAKoB,MAGzEoL,EAAsBL,EACzBxZ,KAAI8B,GAASA,EAAMgY,aAAa,KAChCpT,QAAO3E,IAASgF,GAAWA,EAAQgT,SAAShY,KACzCiY,EAAcjT,GAAW8S,EACzBI,EAAgBD,EAAYha,KAAI+B,GAAQyX,EAAaU,WAAUvX,GAAUA,EAAOmX,aAAa,KAAO/X,MAGpGoY,EAAcrC,EAAYD,EAChC,GAAkB,WAAd4B,EAAwB,CAE1B,MAAMW,EAAY,IAAIva,MAAMsa,GAC5B,IAAK,IAAIE,EAAY,EAAGA,EAAYF,EAAaE,IAAa,CAC5D,MAAMC,EAAMzC,EAAcwC,EAGpBE,EAAU,CAAA,EAChB,IAAK,IAAIze,EAAI,EAAGA,EAAI0d,EAAava,OAAQnD,IACvCye,EAAQf,EAAa1d,GAAGge,aAAa,IAAMJ,EAAY5d,GAAGwe,GAE5DF,EAAUC,GAAaE,CACzB,CACA,OAAOH,CACT,CAGA,MAAMA,EAAY,IAAIva,MAAMsa,GAC5B,IAAK,IAAIE,EAAY,EAAGA,EAAYF,EAAaE,IAAa,CAC5D,MAAMC,EAAMzC,EAAcwC,EAEpBE,EAAU,IAAI1a,MAAM2Z,EAAava,QACvC,IAAK,IAAInD,EAAI,EAAGA,EAAIke,EAAY/a,OAAQnD,IAClCme,EAAcne,IAAM,IACtBye,EAAQze,GAAK4d,EAAYO,EAAcne,IAAIwe,IAG/CF,EAAUC,GAAaE,CACzB,CACA,OAAOH,CACT,CC7HOhW,eAAeoW,GAAYC,GAEhCA,EAAQjV,iBAAmBnB,EAAqBoW,EAAQC,KAAMD,GAG9D,MAAME,EAgED,SAA0BF,GAC/B,IAAKA,EAAQjV,SAAU,MAAM,IAAIpJ,MAAM,6BAIvC,MAAMwe,ET/ED,UAAqBpV,SAAEA,EAAQ8S,SAAEA,EAAW,EAACC,OAAEA,EAASjX,IAAQyF,QAAEA,IACvE,IAAKvB,EAAU,MAAM,IAAIpJ,MAAM,iCAE/B,MAAMye,EAAS,GAETC,EAAU,GAGhB,IAAIlD,EAAa,EACjB,IAAK,MAAM9Q,KAAYtB,EAASqB,WAAY,CAC1C,MAAMkU,EAAYvd,OAAOsJ,EAASF,UAC5BoU,EAAWpD,EAAamD,EAE9B,GAAIA,EAAY,GAAKC,GAAY1C,GAAYV,EAAaW,EAAQ,CAEhE,MAAM0C,EAAS,GAEf,IAAK,MAAMhU,UAAEA,EAASE,UAAEA,KAAeL,EAASC,QAAS,CACvD,GAAIE,EAAW,MAAM,IAAI7K,MAAM,mCAC/B,IAAK+K,EAAW,MAAM,IAAI/K,MAAM,wCAE3B2K,IAAWA,EAAQgT,SAAS5S,EAAUE,eAAe,KACxD4T,EAAOjf,KAAK4S,EAAezH,GAE/B,CACA,MAAM0Q,EAAclb,KAAKgI,IAAI2T,EAAWV,EAAY,GAC9CE,EAAYnb,KAAKsP,IAAIsM,EAASX,EAAYmD,GAChDF,EAAO7e,KAAK,CAAEif,SAAQnU,WAAU8Q,aAAYmD,YAAWlD,cAAaC,cAGpE,MAAMoD,EAAYD,EAAOA,EAAOhc,OAAS,IAAI8P,QAAUkM,EAAO,IAAInM,UAClE,IAAK/H,GAAWmU,EA3CS,SA6CvBJ,EAAQ9e,KAAK,CACX8S,UAAWmM,EAAO,GAAGnM,UACrBC,QAASkM,EAAOA,EAAOhc,OAAS,GAAG8P,eAEhC,GAAIkM,EAAOhc,OAChB0N,EAAOmO,EAASG,QACX,GAAIlU,GAAS9H,OAClB,MAAM,IAAI7C,MAAM,8BAA8B2K,EAAQqJ,KAAK,QAE/D,CAEAwH,EAAaoD,CACf,CAGA,OAFKG,SAAS5C,KAASA,EAASX,GAEzB,CAAEpS,WAAU8S,WAAUC,SAAQxR,UAAS+T,UAASD,SACzD,CS8BeO,CAAYX,GAIzB,OAHAA,EAAQC,KTVH,SAA6BA,GAAMI,QAAEA,IAE1C,MAAMO,EAAWP,EAAQ9a,KAAI,EAAG8O,YAAWC,aAAc2L,EAAKjY,MAAMqM,EAAWC,KAC/E,MAAO,CACLzQ,WAAYoc,EAAKpc,WACjB,KAAAmE,CAAMoL,EAAOC,EAAM4M,EAAKpc,YAEtB,MAAMgd,EAAQR,EAAQZ,WAAU,EAAGpL,YAAWC,aAAcD,GAAajB,GAASC,GAAOiB,IACzF,GAAIuM,EAAQ,EAAG,MAAM,IAAIlf,MAAM,0BAA0ByR,MAAUC,MACnE,GAAIgN,EAAQQ,GAAOxM,YAAcjB,GAASiN,EAAQQ,GAAOvM,UAAYjB,EAAK,CAExE,MAAM+E,EAAchF,EAAQiN,EAAQQ,GAAOxM,UACrCyM,EAAYzN,EAAMgN,EAAQQ,GAAOxM,UACvC,OAAIuM,EAASC,aAAkB3B,QACtB0B,EAASC,GAAOjO,MAAKjP,GAAUA,EAAOqE,MAAMoQ,EAAa0I,KAEzDF,EAASC,GAAO7Y,MAAMoQ,EAAa0I,EAE9C,CACE,OAAOF,EAASC,EAEpB,EAEJ,CSbiBE,CAAoBf,EAAQC,KAAME,GAG1CA,EAAKC,OAAO7a,KAAIyb,GDhFlB,SAAsBhB,GAASjV,SAAEA,EAAQuB,QAAEA,GAAW0U,GAC3D,MAAMf,KAAEA,EAAI7E,YAAEA,EAAWxW,KAAEA,GAASob,EAG9BjB,EAAe,GAEfpa,EAAU,IAAKhC,KAAoBqd,EAAQrb,SAGjD,IAAK,MAAM6H,UAAEA,EAASE,UAAEA,KAAesU,EAAU3U,SAASC,QAAS,CACjE,GAAIE,EAAW,MAAM,IAAI7K,MAAM,mCAC/B,IAAK+K,EAAW,MAAM,IAAI/K,MAAM,wCAGhC,MAAM4b,EAAa7Q,EAAUE,eAAe,GAC5C,GAAIN,IAAYA,EAAQgT,SAAS/B,GAAa,SAE9C,MAAMlJ,UAAEA,EAASC,QAAEA,GAAYH,EAAezH,GACxCuU,EAAc3M,EAAUD,EAI9B,GAAI4M,EAAc,GAAK,GAAI,CACzBC,QAAQC,KAAK,iCAAiCzU,EAAUE,mBAAmBqU,WAE3E,QACF,CAIA,MAAMtd,EAASub,QAAQkC,QAAQnB,EAAKjY,MAAMqM,EAAWC,IAGrDyK,EAAaxd,KAAK,CAChB8d,aAAc3S,EAAUE,eACxB5I,KAAML,EAAOiP,MAAK/H,IAChB,MAAMjD,EAAaL,EAAcwD,EAAShE,OAAQ2F,EAAUE,gBACtDhM,EAAS,CAAEgB,KAAM,IAAI8B,SAASmH,GAAc9I,OAAQ,GAEpDoC,EAAgB,CACpBoZ,WAFgB7Q,EAAUE,eAAe+I,KAAK,KAG9C5U,KAAM2L,EAAU3L,KAChB2D,QAASkD,EAAWA,EAAWpD,OAAS,GAAGE,QAC3CkD,aACAiF,MAAOH,EAAUG,MACjBlI,UACAyW,cACAxW,QAEF,OAAOsY,GAAWtc,EAAQogB,EAAW7c,EAAe6b,EAAQ1C,OAAM,KAGxE,CAEA,MAAO,CAAEH,WAAY6D,EAAU7D,WAAYmD,UAAWU,EAAUV,UAAWvB,eAC7E,CCyBsCsC,CAAarB,EAASG,EAAMa,IAClE,CA1EsBM,CAAiBtB,IAE/BnC,SAAEA,EAAW,EAACC,OAAEA,EAAMxR,QAAEA,EAAOiV,QAAEA,EAAOC,WAAEA,EAAUxC,UAAEA,GAAcgB,EAG1E,IAAKwB,IAAeD,EAAS,CAC3B,IAAK,MAAMxC,aAAEA,KAAkBmB,EAC7B,IAAK,MAAMlc,KAAEA,KAAU+a,QAAoB/a,EAE7C,MACF,CAGA,MAAM8C,EZmLD,UAAuBC,OAAEA,IAC9B,OAAOQ,EAAcR,EAAQ,IAAI,EACnC,CYrLqB0a,CAAczB,EAAQjV,UACnC2W,EAAYxB,EAAY3a,KAAIoc,GDmH7B,SAAuBC,EAAe9a,GAC3C,MAAMiY,aAAEA,GAAiB6C,EAEnBF,EAAY,GAClB,IAAK,MAAMra,KAASP,EAAWI,SAC7B,GAAIG,EAAMH,SAAS1C,OAAQ,CACzB,MAAMqd,EAAe9C,EAAa9S,QAAO/D,GAAUA,EAAOmX,aAAa,KAAOhY,EAAM3C,QAAQ4C,OAC5F,IAAKua,EAAard,OAAQ,SAI1B,MAAMsd,EAAW,IAAI5R,IACflM,EAAOkb,QAAQC,IAAI0C,EAAatc,KAAI2C,GACjCA,EAAOlE,KAAK4O,MAAKgL,IACtBkE,EAASnX,IAAIzC,EAAOmX,aAAa1J,KAAK,KAAM3B,EAAQ4J,GAAW,OAE/DhL,MAAK,KAEP4C,GAAesM,EAAUza,GACzB,MAAM0a,EAAaD,EAAS/Q,IAAI1J,EAAMJ,KAAK0O,KAAK,MAChD,IAAKoM,EAAY,MAAM,IAAIpgB,MAAM,qCACjC,MAAO,CAACogB,EAAU,IAGpBL,EAAUngB,KAAK,CAAE8d,aAAchY,EAAMJ,KAAMjD,QAC7C,KAAO,CAEL,MAAMge,EAAcjD,EAAarX,MAAKQ,GAAUA,EAAOmX,aAAa,KAAOhY,EAAM3C,QAAQ4C,OACrF0a,GACFN,EAAUngB,KAAKygB,EAEnB,CAEF,MAAO,IAAKJ,EAAe7C,aAAc2C,EAC3C,CCrJ2CO,CAAcN,EAAK7a,KAG5D,GAAIya,EACF,IAAK,MAAMW,KAAcR,EACvB,IAAK,MAAMM,KAAeE,EAAWnD,aACnCiD,EAAYhe,KAAK4O,MAAKqM,IACpB,IAAIpB,EAAWqE,EAAW/E,WAC1B,IAAK,MAAMS,KAAcqB,EACvBsC,EAAQ,CACNhE,WAAYyE,EAAY3C,aAAa,GACrCzB,aACAC,WACAC,OAAQD,EAAWD,EAAWpZ,SAEhCqZ,GAAYD,EAAWpZ,MACzB,IAOR,GAAIgd,EAAY,CAGd,MAAMW,EAAO,GACb,IAAK,MAAMD,KAAcR,EAAW,CAElC,MAAMtE,EAAclb,KAAKgI,IAAI2T,EAAWqE,EAAW/E,WAAY,GACzDE,EAAYnb,KAAKsP,KAAKsM,GAAUjX,KAAYqb,EAAW/E,WAAY+E,EAAW5B,WAKpFpO,EAAOiQ,EAHyB,WAAdnD,QACVF,GAAiBoD,EAAY9E,EAAaC,EAAW/Q,EAAS,gBAC9DwS,GAAiBoD,EAAY9E,EAAaC,EAAW/Q,EAAS,SAExE,CACAkV,EAAWW,EACb,MAEE,IAAK,MAAMpD,aAAEA,KAAkB2C,EAC7B,IAAK,MAAM1d,KAAEA,KAAU+a,QAAoB/a,CAGjD,CCrEO2F,eAAeyY,IAAUnC,KAAEA,EAAI7E,YAAEA,IACtC,MAAMrQ,QAAiBnB,EAAqBqW,GACtCoC,EAActX,EAASkC,oBAAoBvF,MAAK4a,GAAiB,QAAXA,EAAGtS,MAC/D,IAAKqS,EACH,MAAM,IAAI1gB,MAAM,mDAIlB,MAAM4gB,EAAY7c,KAAKC,MAAM0c,EAAY5c,OAAS,MAG5CzB,QD4G2Bgc,EC5GK,CAAEC,OAAMlV,WAAUnG,MAAM,EAAOwW,eD6G9D,IAAI8D,SAAQ,CAACsC,EAAYgB,KAC9BzC,GAAY,IACPC,EACHhB,UAAW,SACXwC,eACCiB,MAAMD,EAAM,KANZ,IAA4BxC,ECzGjC,MAAM0C,EAAW,GACXC,EAAgBJ,EAAUK,gBAAkB,WAClD,IAAK,MAAM/C,KAAO7b,EAAM,CACtB,MAAM6e,EAAWhD,EAAI8C,GACrB,IAAKE,EAAU,SAIf,MAAMC,EAAa,CAAA,EACnB,IAAK,MAAM9S,KAAOK,OAAOoG,KAAKoJ,GAAM,CAClC,MAAMpa,EAAQoa,EAAI7P,GACdA,IAAQ2S,GAA2B,OAAVld,IAC3Bqd,EAAW9S,GAAOvK,EAEtB,CAGA,MAAMsd,EAAU,CACdhiB,KAAM,UACN8hB,WACAC,cAGFJ,EAASnhB,KAAKwhB,EAChB,CAEA,MAAO,CACLhiB,KAAM,oBACN2hB,WAEJ,ECvDA/Y,iBAEE,MAAMuG,IAAEA,SAAc8S,OAAOC,KAAKC,cAAc,QAG1C3d,EAAM,IAAI2K,EAFsBiT,SAASC,eAAe,OAErC,CACvBC,OAAQ,CAAEC,IAAK,GAAIC,KAAK,IACxBC,KAAM,IAMR,IAEE,MAAMvD,EAAOxM,QACLpB,EAAmB,CAAEC,IALZ,0DAK6BzO,WAAY,SAE1Dqd,QAAQuC,IAAI,mBAAoBxD,GAChC,MAAMyD,QAAgBtB,GAAU,CAAEnC,SAElCiB,QAAQuC,IAAI,WAAYC,GAGxBne,EAAIvB,KAAK2f,WAAWD,EACtB,CAAE,MAAOE,GACP1C,QAAQ0C,MAAM,4CAA6CA,EAC7D,CACF,CACAC","x_google_ignoreList":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]}