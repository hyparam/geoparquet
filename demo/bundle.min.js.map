{"version":3,"file":"bundle.min.js","sources":["../node_modules/hyparquet/src/constants.js","../node_modules/hyparquet/src/convert.js","../node_modules/hyparquet/src/schema.js","../node_modules/hyparquet/src/thrift.js","../node_modules/hyparquet/src/metadata.js","../node_modules/hyparquet/src/assemble.js","../node_modules/hyparquet/src/delta.js","../node_modules/hyparquet/src/encoding.js","../node_modules/hyparquet/src/plain.js","../node_modules/hyparquet/src/snappy.js","../node_modules/hyparquet/src/datapage.js","../node_modules/hyparquet/src/utils.js","../node_modules/hyparquet/src/column.js","../node_modules/hyparquet/src/read.js","../node_modules/hyparquet/src/query.js","../node_modules/hyparquet/src/hyparquet.js","../src/index.js","demo.js"],"sourcesContent":["/** @type {import('../src/types.d.ts').ParquetType[]} */\nexport const ParquetType = [\n  'BOOLEAN',\n  'INT32',\n  'INT64',\n  'INT96', // deprecated\n  'FLOAT',\n  'DOUBLE',\n  'BYTE_ARRAY',\n  'FIXED_LEN_BYTE_ARRAY',\n]\n\nexport const Encoding = [\n  'PLAIN',\n  undefined,\n  'PLAIN_DICTIONARY',\n  'RLE',\n  'BIT_PACKED', // deprecated\n  'DELTA_BINARY_PACKED',\n  'DELTA_LENGTH_BYTE_ARRAY',\n  'DELTA_BYTE_ARRAY',\n  'RLE_DICTIONARY',\n  'BYTE_STREAM_SPLIT',\n]\n\nexport const FieldRepetitionType = [\n  'REQUIRED',\n  'OPTIONAL',\n  'REPEATED',\n]\n\n/** @type {import('../src/types.d.ts').ConvertedType[]} */\nexport const ConvertedType = [\n  'UTF8',\n  'MAP',\n  'MAP_KEY_VALUE',\n  'LIST',\n  'ENUM',\n  'DECIMAL',\n  'DATE',\n  'TIME_MILLIS',\n  'TIME_MICROS',\n  'TIMESTAMP_MILLIS',\n  'TIMESTAMP_MICROS',\n  'UINT_8',\n  'UINT_16',\n  'UINT_32',\n  'UINT_64',\n  'INT_8',\n  'INT_16',\n  'INT_32',\n  'INT_64',\n  'JSON',\n  'BSON',\n  'INTERVAL',\n]\n\n/** @type {import('../src/types.d.ts').LogicalTypeType[]} */\nexport const logicalTypeType = [\n  'NULL',\n  'STRING',\n  'MAP',\n  'LIST',\n  'ENUM',\n  'DECIMAL',\n  'DATE',\n  'TIME',\n  'TIMESTAMP',\n  'INTERVAL',\n  'INTEGER',\n  'NULL',\n  'JSON',\n  'BSON',\n  'UUID',\n]\n\nexport const CompressionCodec = [\n  'UNCOMPRESSED',\n  'SNAPPY',\n  'GZIP',\n  'LZO',\n  'BROTLI',\n  'LZ4',\n  'ZSTD',\n  'LZ4_RAW',\n]\n\n/** @type {import('../src/types.d.ts').PageType[]} */\nexport const PageType = [\n  'DATA_PAGE',\n  'INDEX_PAGE',\n  'DICTIONARY_PAGE',\n  'DATA_PAGE_V2',\n]\n\n/** @type {import('../src/types.d.ts').BoundaryOrder[]} */\nexport const BoundaryOrder = [\n  'UNORDERED',\n  'ASCENDING',\n  'DESCENDING',\n]\n","const dayMillis = 86400000 // 1 day in milliseconds\n\n/**\n * Convert known types from primitive to rich, and dereference dictionary.\n *\n * @import {DecodedArray, Encoding, SchemaElement} from '../src/types.d.ts'\n * @param {DecodedArray} data series of primitive types\n * @param {DecodedArray | undefined} dictionary\n * @param {SchemaElement} schemaElement\n * @param {Encoding} encoding\n * @param {boolean | undefined} utf8 decode bytes as utf8?\n * @returns {DecodedArray} series of rich types\n */\nexport function convertWithDictionary(data, dictionary, schemaElement, encoding, utf8 = true) {\n  if (dictionary && encoding.endsWith('_DICTIONARY')) {\n    // convert dictionary\n    dictionary = convert(dictionary, schemaElement, utf8)\n    let output = data\n    if (data instanceof Uint8Array && !(dictionary instanceof Uint8Array)) {\n      // @ts-expect-error upgrade data to match dictionary type with fancy constructor\n      output = new dictionary.constructor(data.length)\n    }\n    for (let i = 0; i < data.length; i++) {\n      output[i] = dictionary[data[i]]\n    }\n    return output\n  } else {\n    return convert(data, schemaElement, utf8)\n  }\n}\n\n/**\n * Convert known types from primitive to rich.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {SchemaElement} schemaElement\n * @param {boolean | undefined} utf8 decode bytes as utf8?\n * @returns {DecodedArray} series of rich types\n */\nexport function convert(data, schemaElement, utf8 = true) {\n  const ctype = schemaElement.converted_type\n  if (ctype === 'DECIMAL') {\n    const scale = schemaElement.scale || 0\n    const factor = Math.pow(10, -scale)\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      if (data[0] instanceof Uint8Array) {\n        arr[i] = parseDecimal(data[i]) * factor\n      } else {\n        arr[i] = Number(data[i]) * factor\n      }\n    }\n    return arr\n  }\n  if (ctype === undefined && schemaElement.type === 'INT96') {\n    return Array.from(data).map(parseInt96Date)\n  }\n  if (ctype === 'DATE') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = new Date(data[i] * dayMillis)\n    }\n    return arr\n  }\n  if (ctype === 'TIMESTAMP_MILLIS') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = new Date(Number(data[i]))\n    }\n    return arr\n  }\n  if (ctype === 'TIMESTAMP_MICROS') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = new Date(Number(data[i] / 1000n))\n    }\n    return arr\n  }\n  if (ctype === 'JSON') {\n    const decoder = new TextDecoder()\n    return data.map(v => JSON.parse(decoder.decode(v)))\n  }\n  if (ctype === 'BSON') {\n    throw new Error('parquet bson not supported')\n  }\n  if (ctype === 'INTERVAL') {\n    throw new Error('parquet interval not supported')\n  }\n  if (ctype === 'UTF8' || utf8 && schemaElement.type === 'BYTE_ARRAY') {\n    const decoder = new TextDecoder()\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = data[i] && decoder.decode(data[i])\n    }\n    return arr\n  }\n  if (ctype === 'UINT_64') {\n    const arr = new BigUint64Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = BigInt(data[i])\n    }\n    return arr\n  }\n  if (schemaElement.logical_type?.type === 'FLOAT16') {\n    return Array.from(data).map(parseFloat16)\n  }\n  if (schemaElement.logical_type?.type === 'TIMESTAMP') {\n    const { unit } = schemaElement.logical_type\n    let factor = 1n\n    if (unit === 'MICROS') factor = 1000n\n    if (unit === 'NANOS') factor = 1000000n\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = new Date(Number(data[i] / factor))\n    }\n    return arr\n  }\n  return data\n}\n\n/**\n * @param {Uint8Array} bytes\n * @returns {number}\n */\nexport function parseDecimal(bytes) {\n  // TODO: handle signed\n  let value = 0\n  for (const byte of bytes) {\n    value = value << 8 | byte\n  }\n  return value\n}\n\n/**\n * @param {bigint} value\n * @returns {Date}\n */\nfunction parseInt96Date(value) {\n  const days = Number((value >> 64n) - 2440588n)\n  const nano = Number((value & 0xffffffffffffffffn) / 1000000n)\n  const millis = days * dayMillis + nano\n  return new Date(millis)\n}\n\n/**\n * @param {Uint8Array | undefined} bytes\n * @returns {number | undefined}\n */\nexport function parseFloat16(bytes) {\n  if (!bytes) return undefined\n  const int16 = bytes[1] << 8 | bytes[0]\n  const sign = int16 >> 15 ? -1 : 1\n  const exp = int16 >> 10 & 0x1f\n  const frac = int16 & 0x3ff\n  if (exp === 0) return sign * Math.pow(2, -14) * (frac / 1024) // subnormals\n  if (exp === 0x1f) return frac ? NaN : sign * Infinity\n  return sign * Math.pow(2, exp - 15) * (1 + frac / 1024)\n}\n","/**\n * Build a tree from the schema elements.\n *\n * @import {SchemaElement, SchemaTree} from '../src/types.d.ts'\n * @param {SchemaElement[]} schema\n * @param {number} rootIndex index of the root element\n * @param {string[]} path path to the element\n * @returns {SchemaTree} tree of schema elements\n */\nfunction schemaTree(schema, rootIndex, path) {\n  const element = schema[rootIndex]\n  const children = []\n  let count = 1\n\n  // Read the specified number of children\n  if (element.num_children) {\n    while (children.length < element.num_children) {\n      const childElement = schema[rootIndex + count]\n      const child = schemaTree(schema, rootIndex + count, [...path, childElement.name])\n      count += child.count\n      children.push(child)\n    }\n  }\n\n  return { count, element, children, path }\n}\n\n/**\n * Get schema elements from the root to the given element name.\n *\n * @param {SchemaElement[]} schema\n * @param {string[]} name path to the element\n * @returns {SchemaTree[]} list of schema elements\n */\nexport function getSchemaPath(schema, name) {\n  let tree = schemaTree(schema, 0, [])\n  const path = [tree]\n  for (const part of name) {\n    const child = tree.children.find(child => child.element.name === part)\n    if (!child) throw new Error(`parquet schema element not found: ${name}`)\n    path.push(child)\n    tree = child\n  }\n  return path\n}\n\n/**\n * Get the max repetition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max repetition level\n */\nexport function getMaxRepetitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath) {\n    if (element.repetition_type === 'REPEATED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Get the max definition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max definition level\n */\nexport function getMaxDefinitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath.slice(1)) {\n    if (element.repetition_type !== 'REQUIRED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Check if a column is list-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if list-like\n */\nexport function isListLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'LIST') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length > 1) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Check if a column is map-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if map-like\n */\nexport function isMapLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'MAP') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length !== 2) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  const keyChild = firstChild.children.find(child => child.element.name === 'key')\n  if (keyChild?.element.repetition_type === 'REPEATED') return false\n\n  const valueChild = firstChild.children.find(child => child.element.name === 'value')\n  if (valueChild?.element.repetition_type === 'REPEATED') return false\n\n  return true\n}\n","// TCompactProtocol types\nconst CompactType = {\n  STOP: 0,\n  TRUE: 1,\n  FALSE: 2,\n  BYTE: 3,\n  I16: 4,\n  I32: 5,\n  I64: 6,\n  DOUBLE: 7,\n  BINARY: 8,\n  LIST: 9,\n  SET: 10,\n  MAP: 11,\n  STRUCT: 12,\n  UUID: 13,\n}\n\n/**\n * Parse TCompactProtocol\n *\n * @import {DataReader} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @returns {Record<string, any>}\n */\nexport function deserializeTCompactProtocol(reader) {\n  let lastFid = 0\n  /** @type {Record<string, any>} */\n  const value = {}\n\n  while (reader.offset < reader.view.byteLength) {\n    // Parse each field based on its type and add to the result object\n    const [type, fid, newLastFid] = readFieldBegin(reader, lastFid)\n    lastFid = newLastFid\n\n    if (type === CompactType.STOP) {\n      break\n    }\n\n    // Handle the field based on its type\n    value[`field_${fid}`] = readElement(reader, type)\n  }\n\n  return value\n}\n\n/**\n * Read a single element based on its type\n *\n * @param {DataReader} reader\n * @param {number} type\n * @returns {any} value\n */\nfunction readElement(reader, type) {\n  switch (type) {\n  case CompactType.TRUE:\n    return true\n  case CompactType.FALSE:\n    return false\n  case CompactType.BYTE:\n    // read byte directly\n    return reader.view.getInt8(reader.offset++)\n  case CompactType.I16:\n  case CompactType.I32:\n    return readZigZag(reader)\n  case CompactType.I64:\n    return readZigZagBigInt(reader)\n  case CompactType.DOUBLE: {\n    const value = reader.view.getFloat64(reader.offset, true)\n    reader.offset += 8\n    return value\n  }\n  case CompactType.BINARY: {\n    const stringLength = readVarInt(reader)\n    const strBytes = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, stringLength)\n    reader.offset += stringLength\n    return strBytes\n  }\n  case CompactType.LIST: {\n    const [elemType, listSize] = readCollectionBegin(reader)\n    const boolType = elemType === CompactType.TRUE || elemType === CompactType.FALSE\n    const values = new Array(listSize)\n    for (let i = 0; i < listSize; i++) {\n      values[i] = boolType ? readElement(reader, CompactType.BYTE) === 1 : readElement(reader, elemType)\n    }\n    return values\n  }\n  case CompactType.STRUCT: {\n    /** @type {Record<string, any>} */\n    const structValues = {}\n    let structLastFid = 0\n    while (true) {\n      let structFieldType, structFid\n      [structFieldType, structFid, structLastFid] = readFieldBegin(reader, structLastFid)\n      if (structFieldType === CompactType.STOP) {\n        break\n      }\n      structValues[`field_${structFid}`] = readElement(reader, structFieldType)\n    }\n    return structValues\n  }\n  // TODO: MAP and SET\n  case CompactType.UUID: {\n    // Read 16 bytes to uuid string\n    let uuid = ''\n    for (let i = 0; i < 16; i++) {\n      uuid += reader.view.getUint8(reader.offset++).toString(16).padStart(2, '0')\n    }\n    return uuid\n  }\n  default:\n    throw new Error(`thrift unhandled type: ${type}`)\n  }\n}\n\n/**\n * Var int, also known as Unsigned LEB128.\n * Var ints take 1 to 5 bytes (int32) or 1 to 10 bytes (int64).\n * Reads groups of 7 low bits until high bit is 0.\n *\n * @param {DataReader} reader\n * @returns {number} value\n */\nexport function readVarInt(reader) {\n  let result = 0\n  let shift = 0\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= (byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7\n  }\n}\n\n/**\n * Read a varint as a bigint.\n *\n * @param {DataReader} reader\n * @returns {bigint} value\n */\nfunction readVarBigInt(reader) {\n  let result = 0n\n  let shift = 0n\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= BigInt(byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7n\n  }\n}\n\n/**\n * Values of type int32 and int64 are transformed to a zigzag int.\n * A zigzag int folds positive and negative numbers into the positive number space.\n *\n * @param {DataReader} reader\n * @returns {number} value\n */\nfunction readZigZag(reader) {\n  const zigzag = readVarInt(reader)\n  // convert zigzag to int\n  return zigzag >>> 1 ^ -(zigzag & 1)\n}\n\n/**\n * A zigzag int folds positive and negative numbers into the positive number space.\n * This version returns a BigInt.\n *\n * @param {DataReader} reader\n * @returns {bigint} value\n */\nexport function readZigZagBigInt(reader) {\n  const zigzag = readVarBigInt(reader)\n  // convert zigzag to int\n  return zigzag >> BigInt(1) ^ -(zigzag & BigInt(1))\n}\n\n/**\n * Get thrift type from half a byte\n *\n * @param {number} byte\n * @returns {number}\n */\nfunction getCompactType(byte) {\n  return byte & 0x0f\n}\n\n/**\n * Read field type and field id\n *\n * @param {DataReader} reader\n * @param {number} lastFid\n * @returns {[number, number, number]} [type, fid, newLastFid]\n */\nfunction readFieldBegin(reader, lastFid) {\n  const type = reader.view.getUint8(reader.offset++)\n  if ((type & 0x0f) === CompactType.STOP) {\n    // STOP also ends a struct\n    return [0, 0, lastFid]\n  }\n  const delta = type >> 4\n  let fid // field id\n  if (delta) {\n    // add delta to last field id\n    fid = lastFid + delta\n  } else {\n    throw new Error('non-delta field id not supported')\n  }\n  return [getCompactType(type), fid, fid]\n}\n\n/**\n * Read collection type and size\n *\n * @param {DataReader} reader\n * @returns {[number, number]} [type, size]\n */\nfunction readCollectionBegin(reader) {\n  const sizeType = reader.view.getUint8(reader.offset++)\n  const size = sizeType >> 4\n  const type = getCompactType(sizeType)\n  if (size === 15) {\n    const newSize = readVarInt(reader)\n    return [type, newSize]\n  }\n  return [type, size]\n}\n\n/**\n * Convert int to varint. Outputs 1-5 bytes for int32.\n *\n * @param {number} n\n * @returns {number[]}\n */\nexport function toVarInt(n) {\n  let idx = 0\n  const varInt = []\n  while (true) {\n    if ((n & ~0x7f) === 0) {\n      varInt[idx++] = n\n      break\n    } else {\n      varInt[idx++] = n & 0x7f | 0x80\n      n >>>= 7\n    }\n  }\n  return varInt\n}\n","import { CompressionCodec, ConvertedType, Encoding, FieldRepetitionType, PageType, ParquetType } from './constants.js'\nimport { parseDecimal, parseFloat16 } from './convert.js'\nimport { getSchemaPath } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\n\n/**\n * Read parquet metadata from an async buffer.\n *\n * An AsyncBuffer is like an ArrayBuffer, but the slices are loaded\n * asynchronously, possibly over the network.\n *\n * You must provide the byteLength of the buffer, typically from a HEAD request.\n *\n * In theory, you could use suffix-range requests to fetch the end of the file,\n * and save a round trip. But in practice, this doesn't work because chrome\n * deems suffix-range requests as a not-safe-listed header, and will require\n * a pre-flight. So the byteLength is required.\n *\n * To make this efficient, we initially request the last 512kb of the file,\n * which is likely to contain the metadata. If the metadata length exceeds the\n * initial fetch, 512kb, we request the rest of the metadata from the AsyncBuffer.\n *\n * This ensures that we either make one 512kb initial request for the metadata,\n * or a second request for up to the metadata size.\n *\n * @param {AsyncBuffer} asyncBuffer parquet file contents\n * @param {number} initialFetchSize initial fetch size in bytes (default 512kb)\n * @returns {Promise<FileMetaData>} parquet metadata object\n */\nexport async function parquetMetadataAsync(asyncBuffer, initialFetchSize = 1 << 19 /* 512kb */) {\n  if (!asyncBuffer) throw new Error('parquet file is required')\n  if (!(asyncBuffer.byteLength >= 0)) throw new Error('parquet file byteLength is required')\n\n  // fetch last bytes (footer) of the file\n  const footerOffset = Math.max(0, asyncBuffer.byteLength - initialFetchSize)\n  const footerBuffer = await asyncBuffer.slice(footerOffset, asyncBuffer.byteLength)\n\n  // Check for parquet magic number \"PAR1\"\n  const footerView = new DataView(footerBuffer)\n  if (footerView.getUint32(footerBuffer.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLength = footerView.getUint32(footerBuffer.byteLength - 8, true)\n  if (metadataLength > asyncBuffer.byteLength - 8) {\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${asyncBuffer.byteLength - 8}`)\n  }\n\n  // check if metadata size fits inside the initial fetch\n  if (metadataLength + 8 > initialFetchSize) {\n    // fetch the rest of the metadata\n    const metadataOffset = asyncBuffer.byteLength - metadataLength - 8\n    const metadataBuffer = await asyncBuffer.slice(metadataOffset, footerOffset)\n    // combine initial fetch with the new slice\n    const combinedBuffer = new ArrayBuffer(metadataLength + 8)\n    const combinedView = new Uint8Array(combinedBuffer)\n    combinedView.set(new Uint8Array(metadataBuffer))\n    combinedView.set(new Uint8Array(footerBuffer), footerOffset - metadataOffset)\n    return parquetMetadata(combinedBuffer)\n  } else {\n    // parse metadata from the footer\n    return parquetMetadata(footerBuffer)\n  }\n}\n\n/**\n * Read parquet metadata from a buffer synchronously.\n *\n * @param {ArrayBuffer} arrayBuffer parquet file contents\n * @returns {FileMetaData} parquet metadata object\n */\nexport function parquetMetadata(arrayBuffer) {\n  if (!arrayBuffer) throw new Error('parquet file is required')\n  const view = new DataView(arrayBuffer)\n\n  // Validate footer magic number \"PAR1\"\n  if (view.byteLength < 8) {\n    throw new Error('parquet file is too short')\n  }\n  if (view.getUint32(view.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLengthOffset = view.byteLength - 8\n  const metadataLength = view.getUint32(metadataLengthOffset, true)\n  if (metadataLength > view.byteLength - 8) {\n    // {metadata}, metadata_length, PAR1\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${view.byteLength - 8}`)\n  }\n\n  const metadataOffset = metadataLengthOffset - metadataLength\n  const reader = { view, offset: metadataOffset }\n  const metadata = deserializeTCompactProtocol(reader)\n  const decoder = new TextDecoder()\n  function decode(/** @type {Uint8Array} */ value) {\n    return value && decoder.decode(value)\n  }\n\n  // Parse metadata from thrift data\n  const version = metadata.field_1\n  /** @type {SchemaElement[]} */\n  const schema = metadata.field_2.map((/** @type {any} */ field) => ({\n    type: ParquetType[field.field_1],\n    type_length: field.field_2,\n    repetition_type: FieldRepetitionType[field.field_3],\n    name: decode(field.field_4),\n    num_children: field.field_5,\n    converted_type: ConvertedType[field.field_6],\n    scale: field.field_7,\n    precision: field.field_8,\n    field_id: field.field_9,\n    logical_type: logicalType(field.field_10),\n  }))\n  // schema element per column index\n  const columnSchema = schema.filter(e => e.type)\n  const num_rows = metadata.field_3\n  const row_groups = metadata.field_4.map((/** @type {any} */ rowGroup) => ({\n    columns: rowGroup.field_1.map((/** @type {any} */ column, /** @type {number} */ columnIndex) => ({\n      file_path: decode(column.field_1),\n      file_offset: column.field_2,\n      meta_data: column.field_3 && {\n        type: ParquetType[column.field_3.field_1],\n        encodings: column.field_3.field_2?.map((/** @type {number} */ e) => Encoding[e]),\n        path_in_schema: column.field_3.field_3.map(decode),\n        codec: CompressionCodec[column.field_3.field_4],\n        num_values: column.field_3.field_5,\n        total_uncompressed_size: column.field_3.field_6,\n        total_compressed_size: column.field_3.field_7,\n        key_value_metadata: column.field_3.field_8,\n        data_page_offset: column.field_3.field_9,\n        index_page_offset: column.field_3.field_10,\n        dictionary_page_offset: column.field_3.field_11,\n        statistics: convertStats(column.field_3.field_12, columnSchema[columnIndex]),\n        encoding_stats: column.field_3.field_13?.map((/** @type {any} */ encodingStat) => ({\n          page_type: PageType[encodingStat.field_1],\n          encoding: Encoding[encodingStat.field_2],\n          count: encodingStat.field_3,\n        })),\n        bloom_filter_offset: column.field_3.field_14,\n        bloom_filter_length: column.field_3.field_15,\n        size_statistics: column.field_3.field_16 && {\n          unencoded_byte_array_data_bytes: column.field_3.field_16.field_1,\n          repetition_level_histogram: column.field_3.field_16.field_2,\n          definition_level_histogram: column.field_3.field_16.field_3,\n        },\n      },\n      offset_index_offset: column.field_4,\n      offset_index_length: column.field_5,\n      column_index_offset: column.field_6,\n      column_index_length: column.field_7,\n      crypto_metadata: column.field_7,\n      encrypted_column_metadata: column.field_8,\n    })),\n    total_byte_size: rowGroup.field_2,\n    num_rows: rowGroup.field_3,\n    sorting_columns: rowGroup.field_4?.map((/** @type {any} */ sortingColumn) => ({\n      column_idx: sortingColumn.field_1,\n      descending: sortingColumn.field_2,\n      nulls_first: sortingColumn.field_3,\n    })),\n    file_offset: rowGroup.field_5,\n    total_compressed_size: rowGroup.field_6,\n    ordinal: rowGroup.field_7,\n  }))\n  const key_value_metadata = metadata.field_5?.map((/** @type {any} */ keyValue) => ({\n    key: decode(keyValue.field_1),\n    value: decode(keyValue.field_2),\n  }))\n  const created_by = decode(metadata.field_6)\n\n  return {\n    version,\n    schema,\n    num_rows,\n    row_groups,\n    key_value_metadata,\n    created_by,\n    metadata_length: metadataLength,\n  }\n}\n\n/**\n * Return a tree of schema elements from parquet metadata.\n *\n * @param {FileMetaData} metadata parquet metadata object\n * @returns {SchemaTree} tree of schema elements\n */\nexport function parquetSchema(metadata) {\n  return getSchemaPath(metadata.schema, [])[0]\n}\n\n/**\n * @param {any} logicalType\n * @returns {LogicalType | undefined}\n */\nfunction logicalType(logicalType) {\n  if (logicalType?.field_1) return { type: 'STRING' }\n  if (logicalType?.field_2) return { type: 'MAP' }\n  if (logicalType?.field_3) return { type: 'LIST' }\n  if (logicalType?.field_4) return { type: 'ENUM' }\n  if (logicalType?.field_5) return {\n    type: 'DECIMAL',\n    scale: logicalType.field_5.field_1,\n    precision: logicalType.field_5.field_2,\n  }\n  if (logicalType?.field_6) return { type: 'DATE' }\n  if (logicalType?.field_7) return {\n    type: 'TIME',\n    isAdjustedToUTC: logicalType.field_7.field_1,\n    unit: timeUnit(logicalType.field_7.field_2),\n  }\n  if (logicalType?.field_8) return {\n    type: 'TIMESTAMP',\n    isAdjustedToUTC: logicalType.field_8.field_1,\n    unit: timeUnit(logicalType.field_8.field_2),\n  }\n  if (logicalType?.field_10) return {\n    type: 'INTEGER',\n    bitWidth: logicalType.field_10.field_1,\n    isSigned: logicalType.field_10.field_2,\n  }\n  if (logicalType?.field_11) return { type: 'NULL' }\n  if (logicalType?.field_12) return { type: 'JSON' }\n  if (logicalType?.field_13) return { type: 'BSON' }\n  if (logicalType?.field_14) return { type: 'UUID' }\n  if (logicalType?.field_15) return { type: 'FLOAT16' }\n  return logicalType\n}\n\n/**\n * @param {any} unit\n * @returns {TimeUnit}\n */\nfunction timeUnit(unit) {\n  if (unit.field_1) return 'MILLIS'\n  if (unit.field_2) return 'MICROS'\n  if (unit.field_3) return 'NANOS'\n  throw new Error('parquet time unit required')\n}\n\n/**\n * Convert column statistics based on column type.\n *\n * @import {AsyncBuffer, FileMetaData, LogicalType, MinMaxType, SchemaElement, SchemaTree, Statistics, TimeUnit} from '../src/types.d.ts'\n * @param {any} stats\n * @param {SchemaElement} schema\n * @returns {Statistics}\n */\nfunction convertStats(stats, schema) {\n  return stats && {\n    max: convertMetadata(stats.field_1, schema),\n    min: convertMetadata(stats.field_2, schema),\n    null_count: stats.field_3,\n    distinct_count: stats.field_4,\n    max_value: convertMetadata(stats.field_5, schema),\n    min_value: convertMetadata(stats.field_6, schema),\n    is_max_value_exact: stats.field_7,\n    is_min_value_exact: stats.field_8,\n  }\n}\n\n/**\n * @param {Uint8Array | undefined} value\n * @param {SchemaElement} schema\n * @returns {MinMaxType | undefined}\n */\nexport function convertMetadata(value, schema) {\n  const { type, converted_type, logical_type } = schema\n  if (value === undefined) return value\n  if (type === 'BOOLEAN') return value[0] === 1\n  if (type === 'BYTE_ARRAY') return new TextDecoder().decode(value)\n  const view = new DataView(value.buffer, value.byteOffset, value.byteLength)\n  if (type === 'FLOAT' && view.byteLength === 4) return view.getFloat32(0, true)\n  if (type === 'DOUBLE' && view.byteLength === 8) return view.getFloat64(0, true)\n  if (type === 'INT32' && converted_type === 'DATE') return new Date(view.getInt32(0, true) * 86400000)\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MICROS') return new Date(Number(view.getBigInt64(0, true) / 1000n))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MILLIS') return new Date(Number(view.getBigInt64(0, true)))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'NANOS') return new Date(Number(view.getBigInt64(0, true) / 1000000n))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'MICROS') return new Date(Number(view.getBigInt64(0, true) / 1000n))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP') return new Date(Number(view.getBigInt64(0, true)))\n  if (type === 'INT32' && view.byteLength === 4) return view.getInt32(0, true)\n  if (type === 'INT64' && view.byteLength === 8) return view.getBigInt64(0, true)\n  if (converted_type === 'DECIMAL') return parseDecimal(value) * Math.pow(10, -(schema.scale || 0))\n  if (logical_type?.type === 'FLOAT16') return parseFloat16(value)\n  if (type === 'FIXED_LEN_BYTE_ARRAY') return value\n  // assert(false)\n  return value\n}\n","import { isListLike, isMapLike } from './schema.js'\n\n/**\n * Dremel-assembly of arrays of values into lists\n *\n * Reconstructs a complex nested structure from flat arrays of definition and repetition levels,\n * according to Dremel encoding.\n *\n * @import {DecodedArray, FieldRepetitionType} from '../src/types.d.ts'\n * @param {any[]} output\n * @param {number[] | undefined} definitionLevels\n * @param {number[]} repetitionLevels\n * @param {DecodedArray} values\n * @param {(FieldRepetitionType | undefined)[]} repetitionPath\n * @param {number} maxDefinitionLevel definition level that corresponds to non-null\n * @returns {any[]}\n */\nexport function assembleLists(\n  output, definitionLevels, repetitionLevels, values, repetitionPath, maxDefinitionLevel\n) {\n  const n = definitionLevels?.length || repetitionLevels.length\n  let valueIndex = 0\n\n  // Track state of nested structures\n  const containerStack = [output]\n  let currentContainer = output\n  let currentDepth = 0 // schema depth\n  let currentDefLevel = 0 // list depth\n  let currentRepLevel = 0\n\n  if (repetitionLevels[0]) {\n    // continue previous row\n    while (currentDepth < repetitionPath.length - 2 && currentRepLevel < repetitionLevels[0]) {\n      // go into last list\n      currentContainer = currentContainer.at(-1)\n      containerStack.push(currentContainer)\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') currentDefLevel++\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n  }\n\n  for (let i = 0; i < n; i++) {\n    // assert(currentDefLevel === containerStack.length - 1)\n    const def = definitionLevels?.length ? definitionLevels[i] : maxDefinitionLevel\n    const rep = repetitionLevels[i]\n\n    // Pop up to start of rep level\n    while (currentDepth && (rep < currentRepLevel || repetitionPath[currentDepth] !== 'REPEATED')) {\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        containerStack.pop()\n        currentDefLevel--\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel--\n      currentDepth--\n    }\n    // @ts-expect-error won't be empty\n    currentContainer = containerStack.at(-1)\n\n    // Go deeper to end of definition level\n    while (\n      (currentDepth < repetitionPath.length - 2 || repetitionPath[currentDepth + 1] === 'REPEATED') &&\n      (currentDefLevel < def || repetitionPath[currentDepth + 1] === 'REQUIRED')\n    ) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        /** @type {any[]} */\n        const newList = []\n        currentContainer.push(newList)\n        currentContainer = newList\n        containerStack.push(newList)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n\n    // Add value or null based on definition level\n    if (def === maxDefinitionLevel) {\n      // assert(currentDepth === maxDefinitionLevel || currentDepth === repetitionPath.length - 2)\n      currentContainer.push(values[valueIndex++])\n    } else if (currentDepth === repetitionPath.length - 2) {\n      currentContainer.push(null)\n    } else {\n      currentContainer.push([])\n    }\n  }\n\n  // Handle edge cases for empty inputs or single-level data\n  if (!output.length) {\n    // return max definition level of nested lists\n    for (let i = 0; i < maxDefinitionLevel; i++) {\n      /** @type {any[]} */\n      const newList = []\n      currentContainer.push(newList)\n      currentContainer = newList\n    }\n  }\n\n  return output\n}\n\n/**\n * Assemble a nested structure from subcolumn data.\n * https://github.com/apache/parquet-format/blob/apache-parquet-format-2.10.0/LogicalTypes.md#nested-types\n *\n * @import {SchemaTree} from '../src/types.d.ts'\n * @param {Map<string, any[]>} subcolumnData\n * @param {SchemaTree} schema top-level schema element\n * @param {number} [depth] depth of nested structure\n */\nexport function assembleNested(subcolumnData, schema, depth = 0) {\n  const path = schema.path.join('.')\n  const optional = schema.element.repetition_type === 'OPTIONAL'\n  const nextDepth = optional ? depth + 1 : depth\n\n  if (isListLike(schema)) {\n    let sublist = schema.children[0]\n    let subDepth = nextDepth\n    if (sublist.children.length === 1) {\n      sublist = sublist.children[0]\n      subDepth++\n    }\n    assembleNested(subcolumnData, sublist, subDepth)\n\n    const subcolumn = sublist.path.join('.')\n    const values = subcolumnData.get(subcolumn)\n    if (!values) throw new Error('parquet list column missing values')\n    if (optional) flattenAtDepth(values, depth)\n    subcolumnData.set(path, values)\n    subcolumnData.delete(subcolumn)\n    return\n  }\n\n  if (isMapLike(schema)) {\n    const mapName = schema.children[0].element.name\n\n    // Assemble keys and values\n    assembleNested(subcolumnData, schema.children[0].children[0], nextDepth + 1)\n    assembleNested(subcolumnData, schema.children[0].children[1], nextDepth + 1)\n\n    const keys = subcolumnData.get(`${path}.${mapName}.key`)\n    const values = subcolumnData.get(`${path}.${mapName}.value`)\n\n    if (!keys) throw new Error('parquet map column missing keys')\n    if (!values) throw new Error('parquet map column missing values')\n    if (keys.length !== values.length) {\n      throw new Error('parquet map column key/value length mismatch')\n    }\n\n    const out = assembleMaps(keys, values, nextDepth)\n    if (optional) flattenAtDepth(out, depth)\n\n    subcolumnData.delete(`${path}.${mapName}.key`)\n    subcolumnData.delete(`${path}.${mapName}.value`)\n    subcolumnData.set(path, out)\n    return\n  }\n\n  // Struct-like column\n  if (schema.children.length) {\n    // construct a meta struct and then invert\n    const invertDepth = schema.element.repetition_type === 'REQUIRED' ? depth : depth + 1\n    /** @type {Record<string, any>} */\n    const struct = {}\n    for (const child of schema.children) {\n      assembleNested(subcolumnData, child, invertDepth)\n      const childData = subcolumnData.get(child.path.join('.'))\n      if (!childData) throw new Error('parquet struct missing child data')\n      struct[child.element.name] = childData\n    }\n    // remove children\n    for (const child of schema.children) {\n      subcolumnData.delete(child.path.join('.'))\n    }\n    // invert struct by depth\n    const inverted = invertStruct(struct, invertDepth)\n    if (optional) flattenAtDepth(inverted, depth)\n    subcolumnData.set(path, inverted)\n  }\n}\n\n/**\n * @param {any[]} arr\n * @param {number} depth\n */\nfunction flattenAtDepth(arr, depth) {\n  for (let i = 0; i < arr.length; i++) {\n    if (depth) {\n      flattenAtDepth(arr[i], depth - 1)\n    } else {\n      arr[i] = arr[i][0]\n    }\n  }\n}\n\n/**\n * @param {any[]} keys\n * @param {any[]} values\n * @param {number} depth\n * @returns {any[]}\n */\nfunction assembleMaps(keys, values, depth) {\n  const out = []\n  for (let i = 0; i < keys.length; i++) {\n    if (depth) {\n      out.push(assembleMaps(keys[i], values[i], depth - 1)) // go deeper\n    } else {\n      if (keys[i]) {\n        /** @type {Record<string, any>} */\n        const obj = {}\n        for (let j = 0; j < keys[i].length; j++) {\n          const value = values[i][j]\n          obj[keys[i][j]] = value === undefined ? null : value\n        }\n        out.push(obj)\n      } else {\n        out.push(undefined)\n      }\n    }\n  }\n  return out\n}\n\n/**\n * Invert a struct-like object by depth.\n *\n * @param {Record<string, any[]>} struct\n * @param {number} depth\n * @returns {any[]}\n */\nfunction invertStruct(struct, depth) {\n  const keys = Object.keys(struct)\n  const length = struct[keys[0]]?.length\n  const out = []\n  for (let i = 0; i < length; i++) {\n    /** @type {Record<string, any>} */\n    const obj = {}\n    for (const key of keys) {\n      if (struct[key].length !== length) throw new Error('parquet struct parsing error')\n      obj[key] = struct[key][i]\n    }\n    if (depth) {\n      out.push(invertStruct(obj, depth - 1)) // deeper\n    } else {\n      out.push(obj)\n    }\n  }\n  return out\n}\n","import { readVarInt, readZigZagBigInt } from './thrift.js'\n\n/**\n * @import {DataReader} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} count number of values to read\n * @param {Int32Array | BigInt64Array} output\n */\nexport function deltaBinaryUnpack(reader, count, output) {\n  const int32 = output instanceof Int32Array\n  const blockSize = readVarInt(reader)\n  const miniblockPerBlock = readVarInt(reader)\n  readVarInt(reader) // assert(=== count)\n  let value = readZigZagBigInt(reader) // first value\n  let outputIndex = 0\n  output[outputIndex++] = int32 ? Number(value) : value\n\n  const valuesPerMiniblock = blockSize / miniblockPerBlock\n\n  while (outputIndex < count) {\n    // new block\n    const minDelta = readZigZagBigInt(reader)\n    const bitWidths = new Uint8Array(miniblockPerBlock)\n    for (let i = 0; i < miniblockPerBlock; i++) {\n      bitWidths[i] = reader.view.getUint8(reader.offset++)\n    }\n\n    for (let i = 0; i < miniblockPerBlock && outputIndex < count; i++) {\n      // new miniblock\n      const bitWidth = BigInt(bitWidths[i])\n      if (bitWidth) {\n        let bitpackPos = 0n\n        let miniblockCount = valuesPerMiniblock\n        const mask = (1n << bitWidth) - 1n\n        while (miniblockCount && outputIndex < count) {\n          let bits = BigInt(reader.view.getUint8(reader.offset)) >> bitpackPos & mask // TODO: don't re-read value every time\n          bitpackPos += bitWidth\n          while (bitpackPos >= 8) {\n            bitpackPos -= 8n\n            reader.offset++\n            if (bitpackPos) {\n              bits |= BigInt(reader.view.getUint8(reader.offset)) << bitWidth - bitpackPos & mask\n            }\n          }\n          const delta = minDelta + bits\n          value += delta\n          output[outputIndex++] = int32 ? Number(value) : value\n          miniblockCount--\n        }\n        if (miniblockCount) {\n          // consume leftover miniblock\n          reader.offset += Math.ceil((miniblockCount * Number(bitWidth) + Number(bitpackPos)) / 8)\n        }\n      } else {\n        for (let j = 0; j < valuesPerMiniblock && outputIndex < count; j++) {\n          value += minDelta\n          output[outputIndex++] = int32 ? Number(value) : value\n        }\n      }\n    }\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaLengthByteArray(reader, count, output) {\n  const lengths = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, lengths)\n  for (let i = 0; i < count; i++) {\n    output[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, lengths[i])\n    reader.offset += lengths[i]\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaByteArray(reader, count, output) {\n  const prefixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, prefixData)\n  const suffixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, suffixData)\n\n  for (let i = 0; i < count; i++) {\n    const suffix = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, suffixData[i])\n    if (prefixData[i]) {\n      // copy from previous value\n      output[i] = new Uint8Array(prefixData[i] + suffixData[i])\n      output[i].set(output[i - 1].subarray(0, prefixData[i]))\n      output[i].set(suffix, prefixData[i])\n    } else {\n      output[i] = suffix\n    }\n    reader.offset += suffixData[i]\n  }\n}\n","import { readVarInt } from './thrift.js'\n\n/**\n * Minimum bits needed to store value.\n *\n * @param {number} value\n * @returns {number}\n */\nexport function bitWidth(value) {\n  return 32 - Math.clz32(value)\n}\n\n/**\n * Read values from a run-length encoded/bit-packed hybrid encoding.\n *\n * If length is zero, then read int32 length at the start.\n *\n * @param {DataReader} reader\n * @param {number} width - width of each bit-packed group\n * @param {number} length - length of the encoded data\n * @param {DecodedArray} output\n */\nexport function readRleBitPackedHybrid(reader, width, length, output) {\n  if (!length) {\n    // length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n  }\n  let seen = 0\n  while (seen < output.length) {\n    const header = readVarInt(reader)\n    if (header & 1) {\n      // bit-packed\n      seen = readBitPacked(reader, header, width, output, seen)\n    } else {\n      // rle\n      const count = header >>> 1\n      readRle(reader, count, width, output, seen)\n      seen += count\n    }\n  }\n  // assert(reader.offset - startOffset === length)\n}\n\n/**\n * Run-length encoding: read value with bitWidth and repeat it count times.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n */\nfunction readRle(reader, count, bitWidth, output, seen) {\n  const width = bitWidth + 7 >> 3\n  let value = 0\n  for (let i = 0; i < width; i++) {\n    value |= reader.view.getUint8(reader.offset++) << (i << 3)\n  }\n  // assert(value < 1 << bitWidth)\n\n  // repeat value count times\n  for (let i = 0; i < count; i++) {\n    output[seen + i] = value\n  }\n}\n\n/**\n * Read a bit-packed run of the rle/bitpack hybrid.\n * Supports width > 8 (crossing bytes).\n *\n * @param {DataReader} reader\n * @param {number} header - bit-pack header\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n * @returns {number} total output values so far\n */\nfunction readBitPacked(reader, header, bitWidth, output, seen) {\n  let count = header >> 1 << 3 // values to read\n  const mask = (1 << bitWidth) - 1\n\n  let data = 0\n  if (reader.offset < reader.view.byteLength) {\n    data = reader.view.getUint8(reader.offset++)\n  } else if (mask) {\n    // sometimes out-of-bounds reads are masked out\n    throw new Error(`parquet bitpack offset ${reader.offset} out of range`)\n  }\n  let left = 8\n  let right = 0\n\n  // read values\n  while (count) {\n    // if we have crossed a byte boundary, shift the data\n    if (right > 8) {\n      right -= 8\n      left -= 8\n      data >>>= 8\n    } else if (left - right < bitWidth) {\n      // if we don't have bitWidth number of bits to read, read next byte\n      data |= reader.view.getUint8(reader.offset) << left\n      reader.offset++\n      left += 8\n    } else {\n      if (seen < output.length) {\n        // emit value\n        output[seen++] = data >> right & mask\n      }\n      count--\n      right += bitWidth\n    }\n  }\n\n  return seen\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {DecodedArray}\n */\nexport function byteStreamSplit(reader, count, type, typeLength) {\n  const width = byteWidth(type, typeLength)\n  const bytes = new Uint8Array(count * width)\n  for (let b = 0; b < width; b++) {\n    for (let i = 0; i < count; i++) {\n      bytes[i * width + b] = reader.view.getUint8(reader.offset++)\n    }\n  }\n  // interpret bytes as typed array\n  if (type === 'FLOAT') return new Float32Array(bytes.buffer)\n  else if (type === 'DOUBLE') return new Float64Array(bytes.buffer)\n  else if (type === 'INT32') return new Int32Array(bytes.buffer)\n  else if (type === 'INT64') return new BigInt64Array(bytes.buffer)\n  else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    // split into arrays of typeLength\n    const split = new Array(count)\n    for (let i = 0; i < count; i++) {\n      split[i] = bytes.subarray(i * width, (i + 1) * width)\n    }\n    return split\n  }\n  throw new Error(`parquet byte_stream_split unsupported type: ${type}`)\n}\n\n/**\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {number}\n */\nfunction byteWidth(type, typeLength) {\n  switch (type) {\n  case 'INT32':\n  case 'FLOAT':\n    return 4\n  case 'INT64':\n  case 'DOUBLE':\n    return 8\n  case 'FIXED_LEN_BYTE_ARRAY':\n    if (!typeLength) throw new Error('parquet byteWidth missing type_length')\n    return typeLength\n  default:\n    throw new Error(`parquet unsupported type: ${type}`)\n  }\n}\n","/**\n * Read `count` values of the given type from the reader.view.\n *\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {DataReader} reader - buffer to read data from\n * @param {ParquetType} type - parquet type of the data\n * @param {number} count - number of values to read\n * @param {number | undefined} fixedLength - length of each fixed length byte array\n * @returns {DecodedArray} array of values\n */\nexport function readPlain(reader, type, count, fixedLength) {\n  if (count === 0) return []\n  if (type === 'BOOLEAN') {\n    return readPlainBoolean(reader, count)\n  } else if (type === 'INT32') {\n    return readPlainInt32(reader, count)\n  } else if (type === 'INT64') {\n    return readPlainInt64(reader, count)\n  } else if (type === 'INT96') {\n    return readPlainInt96(reader, count)\n  } else if (type === 'FLOAT') {\n    return readPlainFloat(reader, count)\n  } else if (type === 'DOUBLE') {\n    return readPlainDouble(reader, count)\n  } else if (type === 'BYTE_ARRAY') {\n    return readPlainByteArray(reader, count)\n  } else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    if (!fixedLength) throw new Error('parquet missing fixed length')\n    return readPlainByteArrayFixed(reader, count, fixedLength)\n  } else {\n    throw new Error(`parquet unhandled type: ${type}`)\n  }\n}\n\n/**\n * Read `count` boolean values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {boolean[]}\n */\nfunction readPlainBoolean(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const byteOffset = reader.offset + (i / 8 | 0)\n    const bitOffset = i % 8\n    const byte = reader.view.getUint8(byteOffset)\n    values[i] = (byte & 1 << bitOffset) !== 0\n  }\n  reader.offset += Math.ceil(count / 8)\n  return values\n}\n\n/**\n * Read `count` int32 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Int32Array}\n */\nfunction readPlainInt32(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Int32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Int32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` int64 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {BigInt64Array}\n */\nfunction readPlainInt64(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new BigInt64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new BigInt64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` int96 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {bigint[]}\n */\nfunction readPlainInt96(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const low = reader.view.getBigInt64(reader.offset + i * 12, true)\n    const high = reader.view.getInt32(reader.offset + i * 12 + 8, true)\n    values[i] = BigInt(high) << 64n | low\n  }\n  reader.offset += count * 12\n  return values\n}\n\n/**\n * Read `count` float values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float32Array}\n */\nfunction readPlainFloat(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Float32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Float32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` double values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float64Array}\n */\nfunction readPlainDouble(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new Float64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new Float64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` byte array values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArray(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const length = reader.view.getInt32(reader.offset, true)\n    reader.offset += 4\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, length)\n    reader.offset += length\n  }\n  return values\n}\n\n/**\n * Read a fixed length byte array.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} fixedLength\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArrayFixed(reader, count, fixedLength) {\n  // assert(reader.view.byteLength - reader.offset >= count * fixedLength)\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, fixedLength)\n    reader.offset += fixedLength\n  }\n  return values\n}\n\n/**\n * Create a new buffer with the offset and size.\n *\n * @param {ArrayBufferLike} buffer\n * @param {number} offset\n * @param {number} size\n * @returns {ArrayBuffer}\n */\nfunction align(buffer, offset, size) {\n  const aligned = new ArrayBuffer(size)\n  new Uint8Array(aligned).set(new Uint8Array(buffer, offset, size))\n  return aligned\n}\n","/**\n * The MIT License (MIT)\n * Copyright (c) 2016 Zhipeng Jia\n * https://github.com/zhipeng-jia/snappyjs\n */\n\nconst WORD_MASK = [0, 0xff, 0xffff, 0xffffff, 0xffffffff]\n\n/**\n * Copy bytes from one array to another\n *\n * @param {Uint8Array} fromArray source array\n * @param {number} fromPos source position\n * @param {Uint8Array} toArray destination array\n * @param {number} toPos destination position\n * @param {number} length number of bytes to copy\n * @returns {void}\n */\nfunction copyBytes(fromArray, fromPos, toArray, toPos, length) {\n  for (let i = 0; i < length; i++) {\n    toArray[toPos + i] = fromArray[fromPos + i]\n  }\n}\n\n/**\n * Copy bytes within an array\n *\n * @param {Uint8Array} array source and destination array\n * @param {number} pos source position\n * @param {number} offset offset back from current position to read\n * @param {number} length number of bytes to copy\n * @returns {void}\n */\nfunction selfCopyBytes(array, pos, offset, length) {\n  for (let i = 0; i < length; i++) {\n    array[pos + i] = array[pos - offset + i]\n  }\n}\n\n/**\n * Decompress snappy data.\n * Accepts an output buffer to avoid allocating a new buffer for each call.\n *\n * @param {Uint8Array} input compressed data\n * @param {Uint8Array} output output buffer\n * @returns {void}\n */\nexport function snappyUncompress(input, output) {\n  const inputLength = input.byteLength\n  const outputLength = output.byteLength\n  let pos = 0\n  let outPos = 0\n\n  // skip preamble (contains uncompressed length as varint)\n  while (pos < inputLength) {\n    const c = input[pos]\n    pos++\n    if (c < 128) {\n      break\n    }\n  }\n  if (outputLength && pos >= inputLength) {\n    throw new Error('invalid snappy length header')\n  }\n\n  while (pos < inputLength) {\n    const c = input[pos]\n    let len = 0\n    pos++\n\n    if (pos >= inputLength) {\n      throw new Error('missing eof marker')\n    }\n\n    // There are two types of elements, literals and copies (back references)\n    if ((c & 0x3) === 0) {\n      // Literals are uncompressed data stored directly in the byte stream\n      let len = (c >>> 2) + 1\n      // Longer literal length is encoded in multiple bytes\n      if (len > 60) {\n        if (pos + 3 >= inputLength) {\n          throw new Error('snappy error literal pos + 3 >= inputLength')\n        }\n        const lengthSize = len - 60 // length bytes - 1\n        len = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        len = (len & WORD_MASK[lengthSize]) + 1\n        pos += lengthSize\n      }\n      if (pos + len > inputLength) {\n        throw new Error('snappy error literal exceeds input length')\n      }\n      copyBytes(input, pos, output, outPos, len)\n      pos += len\n      outPos += len\n    } else {\n      // Copy elements\n      let offset = 0 // offset back from current position to read\n      switch (c & 0x3) {\n      case 1:\n        // Copy with 1-byte offset\n        len = (c >>> 2 & 0x7) + 4\n        offset = input[pos] + (c >>> 5 << 8)\n        pos++\n        break\n      case 2:\n        // Copy with 2-byte offset\n        if (inputLength <= pos + 1) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos] + (input[pos + 1] << 8)\n        pos += 2\n        break\n      case 3:\n        // Copy with 4-byte offset\n        if (inputLength <= pos + 3) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        pos += 4\n        break\n      default:\n        break\n      }\n      if (offset === 0 || isNaN(offset)) {\n        throw new Error(`invalid offset ${offset} pos ${pos} inputLength ${inputLength}`)\n      }\n      if (offset > outPos) {\n        throw new Error('cannot copy from before start of buffer')\n      }\n      selfCopyBytes(output, outPos, offset, len)\n      outPos += len\n    }\n  }\n\n  if (outPos !== outputLength) throw new Error('premature end of input')\n}\n","import { deltaBinaryUnpack, deltaByteArray, deltaLengthByteArray } from './delta.js'\nimport { bitWidth, byteStreamSplit, readRleBitPackedHybrid } from './encoding.js'\nimport { readPlain } from './plain.js'\nimport { getMaxDefinitionLevel, getMaxRepetitionLevel } from './schema.js'\nimport { snappyUncompress } from './snappy.js'\n\n/**\n * Read a data page from uncompressed reader.\n *\n * @param {Uint8Array} bytes raw page data (should already be decompressed)\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @param {ColumnMetaData} columnMetadata\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPage(bytes, daph, schemaPath, { type }) {\n  const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength)\n  const reader = { view, offset: 0 }\n  /** @type {DecodedArray} */\n  let dataPage\n\n  // repetition and definition levels\n  const repetitionLevels = readRepetitionLevels(reader, daph, schemaPath)\n  // assert(!repetitionLevels.length || repetitionLevels.length === daph.num_values)\n  const { definitionLevels, numNulls } = readDefinitionLevels(reader, daph, schemaPath)\n  // assert(!definitionLevels.length || definitionLevels.length === daph.num_values)\n\n  // read values based on encoding\n  const nValues = daph.num_values - numNulls\n  if (daph.encoding === 'PLAIN') {\n    const { type_length } = schemaPath[schemaPath.length - 1].element\n    dataPage = readPlain(reader, type, nValues, type_length)\n  } else if (\n    daph.encoding === 'PLAIN_DICTIONARY' ||\n    daph.encoding === 'RLE_DICTIONARY' ||\n    daph.encoding === 'RLE'\n  ) {\n    const bitWidth = type === 'BOOLEAN' ? 1 : view.getUint8(reader.offset++)\n    if (bitWidth) {\n      dataPage = new Array(nValues)\n      readRleBitPackedHybrid(reader, bitWidth, view.byteLength - reader.offset, dataPage)\n    } else {\n      dataPage = new Uint8Array(nValues) // nValue zeroes\n    }\n  } else if (daph.encoding === 'BYTE_STREAM_SPLIT') {\n    const { type_length } = schemaPath[schemaPath.length - 1].element\n    dataPage = byteStreamSplit(reader, nValues, type, type_length)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @param {Uint8Array} bytes raw page data\n * @param {DictionaryPageHeader} diph dictionary page header\n * @param {ColumnMetaData} columnMetadata\n * @param {number | undefined} typeLength - type_length from schema\n * @returns {DecodedArray}\n */\nexport function readDictionaryPage(bytes, diph, columnMetadata, typeLength) {\n  const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength)\n  const reader = { view, offset: 0 }\n  return readPlain(reader, columnMetadata.type, diph.num_values, typeLength)\n}\n\n/**\n * @import {ColumnMetaData, CompressionCodec, Compressors, DataPage, DataPageHeader, DataPageHeaderV2, DataReader, DecodedArray, DictionaryPageHeader, PageHeader, SchemaTree} from '../src/types.d.ts'\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels and number of bytes read\n */\nfunction readRepetitionLevels(reader, daph, schemaPath) {\n  if (schemaPath.length > 1) {\n    const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n    if (maxRepetitionLevel) {\n      const values = new Array(daph.num_values)\n      readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), 0, values)\n      return values\n    }\n  }\n  return []\n}\n\n/**\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {{ definitionLevels: number[], numNulls: number }} definition levels\n */\nfunction readDefinitionLevels(reader, daph, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (!maxDefinitionLevel) return { definitionLevels: [], numNulls: 0 }\n\n  const definitionLevels = new Array(daph.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), 0, definitionLevels)\n\n  // count nulls\n  let numNulls = daph.num_values\n  for (const def of definitionLevels) {\n    if (def === maxDefinitionLevel) numNulls--\n  }\n  if (numNulls === 0) definitionLevels.length = 0\n\n  return { definitionLevels, numNulls }\n}\n\n/**\n * @param {Uint8Array} compressedBytes\n * @param {number} uncompressed_page_size\n * @param {CompressionCodec} codec\n * @param {Compressors | undefined} compressors\n * @returns {Uint8Array}\n */\nexport function decompressPage(compressedBytes, uncompressed_page_size, codec, compressors) {\n  /** @type {Uint8Array} */\n  let page\n  const customDecompressor = compressors?.[codec]\n  if (codec === 'UNCOMPRESSED') {\n    page = compressedBytes\n  } else if (customDecompressor) {\n    page = customDecompressor(compressedBytes, uncompressed_page_size)\n  } else if (codec === 'SNAPPY') {\n    page = new Uint8Array(uncompressed_page_size)\n    snappyUncompress(compressedBytes, page)\n  } else {\n    throw new Error(`parquet unsupported compression codec: ${codec}`)\n  }\n  if (page?.length !== uncompressed_page_size) {\n    throw new Error(`parquet decompressed page length ${page?.length} does not match header ${uncompressed_page_size}`)\n  }\n  return page\n}\n\n\n/**\n * Read a data page from the given Uint8Array.\n *\n * @param {Uint8Array} compressedBytes raw page data\n * @param {PageHeader} ph page header\n * @param {SchemaTree[]} schemaPath\n * @param {ColumnMetaData} columnMetadata\n * @param {Compressors | undefined} compressors\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPageV2(compressedBytes, ph, schemaPath, columnMetadata, compressors) {\n  const view = new DataView(compressedBytes.buffer, compressedBytes.byteOffset, compressedBytes.byteLength)\n  const reader = { view, offset: 0 }\n  const { codec, type } = columnMetadata\n  const daph2 = ph.data_page_header_v2\n  if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n  // repetition levels\n  const repetitionLevels = readRepetitionLevelsV2(reader, daph2, schemaPath)\n  reader.offset = daph2.repetition_levels_byte_length // readVarInt() => len for boolean v2?\n\n  // definition levels\n  const definitionLevels = readDefinitionLevelsV2(reader, daph2, schemaPath)\n  // assert(reader.offset === daph2.repetition_levels_byte_length + daph2.definition_levels_byte_length)\n\n  const uncompressedPageSize = ph.uncompressed_page_size - daph2.definition_levels_byte_length - daph2.repetition_levels_byte_length\n\n  let page = compressedBytes.subarray(reader.offset)\n  if (daph2.is_compressed !== false) {\n    page = decompressPage(page, uncompressedPageSize, codec, compressors)\n  }\n  const pageView = new DataView(page.buffer, page.byteOffset, page.byteLength)\n  const pageReader = { view: pageView, offset: 0 }\n\n  // read values based on encoding\n  /** @type {DecodedArray} */\n  let dataPage\n  const nValues = daph2.num_values - daph2.num_nulls\n  if (daph2.encoding === 'PLAIN') {\n    const { type_length } = schemaPath[schemaPath.length - 1].element\n    dataPage = readPlain(pageReader, type, nValues, type_length)\n  } else if (daph2.encoding === 'RLE') {\n    // assert(columnMetadata.type === 'BOOLEAN')\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, 1, 0, dataPage)\n    dataPage = dataPage.map(x => !!x)\n  } else if (\n    daph2.encoding === 'PLAIN_DICTIONARY' ||\n    daph2.encoding === 'RLE_DICTIONARY'\n  ) {\n    const bitWidth = pageView.getUint8(pageReader.offset++)\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, bitWidth, uncompressedPageSize - 1, dataPage)\n  } else if (daph2.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'BYTE_STREAM_SPLIT') {\n    const { type_length } = schemaPath[schemaPath.length - 1].element\n    dataPage = byteStreamSplit(reader, nValues, type, type_length)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph2.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels\n */\nfunction readRepetitionLevelsV2(reader, daph2, schemaPath) {\n  const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n  if (!maxRepetitionLevel) return []\n\n  const values = new Array(daph2.num_values)\n  readRleBitPackedHybrid(\n    reader, bitWidth(maxRepetitionLevel), daph2.repetition_levels_byte_length, values\n  )\n  return values\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {number[] | undefined} definition levels\n */\nfunction readDefinitionLevelsV2(reader, daph2, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (maxDefinitionLevel) {\n    // V2 we know the length\n    const values = new Array(daph2.num_values)\n    readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), daph2.definition_levels_byte_length, values)\n    return values\n  }\n}\n","/**\n * Replace bigint, date, etc with legal JSON types.\n * When parsing parquet files, bigints are used to represent 64-bit integers.\n * However, JSON does not support bigints, so it's helpful to convert to numbers.\n *\n * @param {any} obj object to convert\n * @returns {unknown} converted object\n */\nexport function toJson(obj) {\n  if (obj === undefined) return null\n  if (typeof obj === 'bigint') return Number(obj)\n  if (Array.isArray(obj)) return obj.map(toJson)\n  if (obj instanceof Uint8Array) return Array.from(obj)\n  if (obj instanceof Date) return obj.toISOString()\n  if (obj instanceof Object) {\n    /** @type {Record<string, unknown>} */\n    const newObj = {}\n    for (const key of Object.keys(obj)) {\n      if (obj[key] === undefined) continue\n      newObj[key] = toJson(obj[key])\n    }\n    return newObj\n  }\n  return obj\n}\n\n/**\n * Concatenate two arrays fast.\n *\n * @param {any[]} aaa first array\n * @param {DecodedArray} bbb second array\n */\nexport function concat(aaa, bbb) {\n  const chunk = 10000\n  for (let i = 0; i < bbb.length; i += chunk) {\n    aaa.push(...bbb.slice(i, i + chunk))\n  }\n}\n\n/**\n * Get the byte length of a URL using a HEAD request.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {string} url\n * @param {RequestInit} [requestInit] fetch options\n * @returns {Promise<number>}\n */\nexport async function byteLengthFromUrl(url, requestInit) {\n  return await fetch(url, { ...requestInit, method: 'HEAD' })\n    .then(res => {\n      if (!res.ok) throw new Error(`fetch head failed ${res.status}`)\n      const length = res.headers.get('Content-Length')\n      if (!length) throw new Error('missing content length')\n      return parseInt(length)\n    })\n}\n\n/**\n * Construct an AsyncBuffer for a URL.\n * If byteLength is not provided, will make a HEAD request to get the file size.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {object} options\n * @param {string} options.url\n * @param {number} [options.byteLength]\n * @param {RequestInit} [options.requestInit]\n * @returns {Promise<AsyncBuffer>}\n */\nexport async function asyncBufferFromUrl({ url, byteLength, requestInit }) {\n  // byte length from HEAD request\n  byteLength ||= await byteLengthFromUrl(url, requestInit)\n  const init = requestInit || {}\n  return {\n    byteLength,\n    async slice(start, end) {\n      // fetch byte range from url\n      const headers = new Headers(init.headers)\n      const endStr = end === undefined ? '' : end - 1\n      headers.set('Range', `bytes=${start}-${endStr}`)\n      const res = await fetch(url, { ...init, headers })\n      if (!res.ok || !res.body) throw new Error(`fetch failed ${res.status}`)\n      return res.arrayBuffer()\n    },\n  }\n}\n\n/**\n * Construct an AsyncBuffer for a local file using node fs package.\n *\n * @param {string} filename\n * @returns {Promise<AsyncBuffer>}\n */\nexport async function asyncBufferFromFile(filename) {\n  const fsPackage = 'fs' // webpack no include\n  const fs = await import(fsPackage)\n  const stat = await fs.promises.stat(filename)\n  return {\n    byteLength: stat.size,\n    async slice(start, end) {\n      // read file slice\n      const readStream = fs.createReadStream(filename, { start, end })\n      return await readStreamToArrayBuffer(readStream)\n    },\n  }\n}\n\n/**\n * Convert a node ReadStream to ArrayBuffer.\n *\n * @param {import('stream').Readable} input\n * @returns {Promise<ArrayBuffer>}\n */\nfunction readStreamToArrayBuffer(input) {\n  return new Promise((resolve, reject) => {\n    /** @type {Buffer[]} */\n    const chunks = []\n    input.on('data', chunk => chunks.push(chunk))\n    input.on('end', () => {\n      const buffer = Buffer.concat(chunks)\n      resolve(buffer.buffer.slice(buffer.byteOffset, buffer.byteOffset + buffer.byteLength))\n    })\n    input.on('error', reject)\n  })\n}\n\n/**\n * Returns a cached layer on top of an AsyncBuffer. For caching slices of a file\n * that are read multiple times, possibly over a network.\n *\n * @param {AsyncBuffer} file file-like object to cache\n * @returns {AsyncBuffer} cached file-like object\n */\nexport function cachedAsyncBuffer({ byteLength, slice }) {\n  const cache = new Map()\n  return {\n    byteLength,\n    /**\n     * @param {number} start\n     * @param {number} [end]\n     * @returns {Awaitable<ArrayBuffer>}\n     */\n    slice(start, end) {\n      const key = cacheKey(start, end, byteLength)\n      const cached = cache.get(key)\n      if (cached) return cached\n      // cache miss, read from file\n      const promise = slice(start, end)\n      cache.set(key, promise)\n      return promise\n    },\n  }\n}\n\n\n/**\n * Returns canonical cache key for a byte range 'start,end'.\n * Normalize int-range and suffix-range requests to the same key.\n *\n * @import {AsyncBuffer, Awaitable, DecodedArray} from '../src/types.d.ts'\n * @param {number} start start byte of range\n * @param {number} [end] end byte of range, or undefined for suffix range\n * @param {number} [size] size of file, or undefined for suffix range\n * @returns {string}\n */\nfunction cacheKey(start, end, size) {\n  if (start < 0) {\n    if (end !== undefined) throw new Error(`invalid suffix range [${start}, ${end}]`)\n    if (size === undefined) return `${start},`\n    return `${size + start},${size}`\n  } else if (end !== undefined) {\n    if (start > end) throw new Error(`invalid empty range [${start}, ${end}]`)\n    return `${start},${end}`\n  } else if (size === undefined) {\n    return `${start},`\n  } else {\n    return `${start},${size}`\n  }\n}\n","import { assembleLists } from './assemble.js'\nimport { Encoding, PageType } from './constants.js'\nimport { convertWithDictionary } from './convert.js'\nimport { decompressPage, readDataPage, readDataPageV2, readDictionaryPage } from './datapage.js'\nimport { getMaxDefinitionLevel } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\nimport { concat } from './utils.js'\n\n/**\n * Parse column data from a buffer.\n *\n * @param {DataReader} reader\n * @param {number} rowLimit maximum number of rows to read\n * @param {ColumnMetaData} columnMetadata column metadata\n * @param {SchemaTree[]} schemaPath schema path for the column\n * @param {ParquetReadOptions} options read options\n * @returns {any[]} array of values\n */\nexport function readColumn(reader, rowLimit, columnMetadata, schemaPath, { compressors, utf8 }) {\n  const { element } = schemaPath[schemaPath.length - 1]\n  /** @type {DecodedArray | undefined} */\n  let dictionary = undefined\n  /** @type {any[]} */\n  const rowData = []\n\n  while (rowData.length < rowLimit) {\n    // parse column header\n    const header = parquetHeader(reader)\n    // assert(header.compressed_page_size !== undefined)\n\n    // read compressed_page_size bytes starting at offset\n    const compressedBytes = new Uint8Array(\n      reader.view.buffer, reader.view.byteOffset + reader.offset, header.compressed_page_size\n    )\n\n    // parse page data by type\n    /** @type {DecodedArray} */\n    let values\n    if (header.type === 'DATA_PAGE') {\n      const daph = header.data_page_header\n      if (!daph) throw new Error('parquet data page header is undefined')\n\n      const page = decompressPage(compressedBytes, Number(header.uncompressed_page_size), columnMetadata.codec, compressors)\n      const { definitionLevels, repetitionLevels, dataPage } = readDataPage(page, daph, schemaPath, columnMetadata)\n      // assert(!daph.statistics?.null_count || daph.statistics.null_count === BigInt(daph.num_values - dataPage.length))\n\n      // convert types, dereference dictionary, and assemble lists\n      values = convertWithDictionary(dataPage, dictionary, element, daph.encoding, utf8)\n      if (repetitionLevels.length || definitionLevels?.length) {\n        const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n        const repetitionPath = schemaPath.map(({ element }) => element.repetition_type)\n        assembleLists(\n          rowData, definitionLevels, repetitionLevels, values, repetitionPath, maxDefinitionLevel\n        )\n      } else {\n        // wrap nested flat data by depth\n        for (let i = 2; i < schemaPath.length; i++) {\n          if (schemaPath[i].element.repetition_type !== 'REQUIRED') {\n            values = Array.from(values, e => [e])\n          }\n        }\n        concat(rowData, values)\n      }\n    } else if (header.type === 'DATA_PAGE_V2') {\n      const daph2 = header.data_page_header_v2\n      if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n      const { definitionLevels, repetitionLevels, dataPage } = readDataPageV2(\n        compressedBytes, header, schemaPath, columnMetadata, compressors\n      )\n\n      // convert types, dereference dictionary, and assemble lists\n      values = convertWithDictionary(dataPage, dictionary, element, daph2.encoding, utf8)\n      if (repetitionLevels.length || definitionLevels?.length) {\n        const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n        const repetitionPath = schemaPath.map(({ element }) => element.repetition_type)\n        assembleLists(\n          rowData, definitionLevels, repetitionLevels, values, repetitionPath, maxDefinitionLevel\n        )\n      } else {\n        concat(rowData, values)\n      }\n    } else if (header.type === 'DICTIONARY_PAGE') {\n      const diph = header.dictionary_page_header\n      if (!diph) throw new Error('parquet dictionary page header is undefined')\n\n      const page = decompressPage(\n        compressedBytes, Number(header.uncompressed_page_size), columnMetadata.codec, compressors\n      )\n      dictionary = readDictionaryPage(page, diph, columnMetadata, element.type_length)\n    } else {\n      throw new Error(`parquet unsupported page type: ${header.type}`)\n    }\n    reader.offset += header.compressed_page_size\n  }\n  if (rowData.length < rowLimit) {\n    throw new Error(`parquet row data length ${rowData.length} does not match row group limit ${rowLimit}}`)\n  }\n  if (rowData.length > rowLimit) {\n    rowData.length = rowLimit // truncate to row limit\n  }\n  return rowData\n}\n\n/**\n * Find the start byte offset for a column chunk.\n *\n * @param {ColumnMetaData} columnMetadata\n * @returns {[bigint, bigint]} byte offset range\n */\nexport function getColumnRange({ dictionary_page_offset, data_page_offset, total_compressed_size }) {\n  let columnOffset = dictionary_page_offset\n  if (!columnOffset || data_page_offset < columnOffset) {\n    columnOffset = data_page_offset\n  }\n  return [columnOffset, columnOffset + total_compressed_size]\n}\n\n/**\n * Read parquet header from a buffer.\n *\n * @import {ColumnMetaData, DecodedArray, DataReader, PageHeader, ParquetReadOptions, SchemaTree} from '../src/types.d.ts'\n * @param {DataReader} reader - parquet file reader\n * @returns {PageHeader} metadata object and bytes read\n */\nfunction parquetHeader(reader) {\n  const header = deserializeTCompactProtocol(reader)\n\n  // Parse parquet header from thrift data\n  const type = PageType[header.field_1]\n  const uncompressed_page_size = header.field_2\n  const compressed_page_size = header.field_3\n  const crc = header.field_4\n  const data_page_header = header.field_5 && {\n    num_values: header.field_5.field_1,\n    encoding: Encoding[header.field_5.field_2],\n    definition_level_encoding: Encoding[header.field_5.field_3],\n    repetition_level_encoding: Encoding[header.field_5.field_4],\n    statistics: header.field_5.field_5 && {\n      max: header.field_5.field_5.field_1,\n      min: header.field_5.field_5.field_2,\n      null_count: header.field_5.field_5.field_3,\n      distinct_count: header.field_5.field_5.field_4,\n      max_value: header.field_5.field_5.field_5,\n      min_value: header.field_5.field_5.field_6,\n    },\n  }\n  const index_page_header = header.field_6\n  const dictionary_page_header = header.field_7 && {\n    num_values: header.field_7.field_1,\n    encoding: Encoding[header.field_7.field_2],\n    is_sorted: header.field_7.field_3,\n  }\n  const data_page_header_v2 = header.field_8 && {\n    num_values: header.field_8.field_1,\n    num_nulls: header.field_8.field_2,\n    num_rows: header.field_8.field_3,\n    encoding: Encoding[header.field_8.field_4],\n    definition_levels_byte_length: header.field_8.field_5,\n    repetition_levels_byte_length: header.field_8.field_6,\n    is_compressed: header.field_8.field_7 === undefined ? true : header.field_8.field_7, // default true\n    statistics: header.field_8.field_8,\n  }\n\n  return {\n    type,\n    uncompressed_page_size,\n    compressed_page_size,\n    crc,\n    data_page_header,\n    index_page_header,\n    dictionary_page_header,\n    data_page_header_v2,\n  }\n}\n","import { assembleNested } from './assemble.js'\nimport { getColumnRange, readColumn } from './column.js'\nimport { parquetMetadataAsync } from './metadata.js'\nimport { getSchemaPath } from './schema.js'\nimport { concat } from './utils.js'\n\n/**\n * Read parquet data rows from a file-like object.\n * Reads the minimal number of row groups and columns to satisfy the request.\n *\n * Returns a void promise when complete, and to throw errors.\n * Data is returned in onComplete, not the return promise, because\n * if onComplete is undefined, we parse the data, and emit chunks, but skip\n * computing the row view directly. This saves on allocation if the caller\n * wants to cache the full chunks, and make their own view of the data from\n * the chunks.\n *\n * @param {ParquetReadOptions} options read options\n * @returns {Promise<void>} resolves when all requested rows and columns are parsed\n */\nexport async function parquetRead(options) {\n  if (!options.file) throw new Error('parquet file is required')\n\n  // load metadata if not provided\n  options.metadata ||= await parquetMetadataAsync(options.file)\n  if (!options.metadata) throw new Error('parquet metadata not found')\n\n  const { metadata, onComplete, rowEnd } = options\n  const rowStart = options.rowStart || 0\n  /** @type {any[][]} */\n  const rowData = []\n\n  // find which row groups to read\n  let groupStart = 0 // first row index of the current group\n  for (const rowGroup of metadata.row_groups) {\n    // number of rows in this row group\n    const groupRows = Number(rowGroup.num_rows)\n    // if row group overlaps with row range, read it\n    if (groupStart + groupRows >= rowStart && (rowEnd === undefined || groupStart < rowEnd)) {\n      // read row group\n      const rowLimit = rowEnd && rowEnd - groupStart\n      const groupData = await readRowGroup(options, rowGroup, groupStart, rowLimit)\n      if (onComplete) {\n        // filter to rows in range\n        const start = Math.max(rowStart - groupStart, 0)\n        const end = rowEnd === undefined ? undefined : rowEnd - groupStart\n        concat(rowData, groupData.slice(start, end))\n      }\n    }\n    groupStart += groupRows\n  }\n\n  if (onComplete) onComplete(rowData)\n}\n\n/**\n * Read a row group from a file-like object.\n *\n * @param {ParquetReadOptions} options read options\n * @param {RowGroup} rowGroup row group to read\n * @param {number} groupStart row index of the first row in the group\n * @param {number} [rowLimit] max rows to read from this group\n * @returns {Promise<any[][]>} resolves to row data\n */\nexport async function readRowGroup(options, rowGroup, groupStart, rowLimit) {\n  const { file, metadata, columns } = options\n  if (!metadata) throw new Error('parquet metadata not found')\n  if (rowLimit === undefined || rowLimit > rowGroup.num_rows) rowLimit = Number(rowGroup.num_rows)\n\n  // loop through metadata to find min/max bytes to read\n  let [groupStartByte, groupEndByte] = [file.byteLength, 0]\n  rowGroup.columns.forEach(({ meta_data: columnMetadata }) => {\n    if (!columnMetadata) throw new Error('parquet column metadata is undefined')\n    // skip columns that are not requested\n    if (columns && !columns.includes(columnMetadata.path_in_schema[0])) return\n\n    const [columnStartByte, columnEndByte] = getColumnRange(columnMetadata).map(Number)\n    groupStartByte = Math.min(groupStartByte, columnStartByte)\n    groupEndByte = Math.max(groupEndByte, columnEndByte)\n  })\n  if (groupStartByte >= groupEndByte && columns?.length) {\n    // TODO: should throw if any column is missing\n    throw new Error(`parquet columns not found: ${columns.join(', ')}`)\n  }\n  // if row group size is less than 32mb, pre-load in one read\n  let groupBuffer\n  if (groupEndByte - groupStartByte <= 1 << 25) {\n    // pre-load row group byte data in one big read,\n    // otherwise read column data individually\n    groupBuffer = await file.slice(groupStartByte, groupEndByte)\n  }\n\n  const promises = []\n  // Top-level columns to assemble\n  const { children } = getSchemaPath(metadata.schema, [])[0]\n  const subcolumnNames = new Map(children.map(child => [child.element.name, getSubcolumns(child)]))\n  const subcolumnData = new Map() // columns to assemble as maps\n  // read column data\n  for (let columnIndex = 0; columnIndex < rowGroup.columns.length; columnIndex++) {\n    const columnMetadata = rowGroup.columns[columnIndex].meta_data\n    if (!columnMetadata) throw new Error('parquet column metadata is undefined')\n\n    // skip columns that are not requested\n    const columnName = columnMetadata.path_in_schema[0]\n    if (columns && !columns.includes(columnName)) continue\n\n    const [columnStartByte, columnEndByte] = getColumnRange(columnMetadata).map(Number)\n    const columnBytes = columnEndByte - columnStartByte\n\n    // skip columns larger than 1gb\n    // TODO: stream process the data, returning only the requested rows\n    if (columnBytes > 1 << 30) {\n      console.warn(`parquet skipping huge column \"${columnMetadata.path_in_schema}\" ${columnBytes.toLocaleString()} bytes`)\n      // TODO: set column to new Error('parquet column too large')\n      continue\n    }\n\n    // use pre-loaded row group byte data if available, else read column data\n    /** @type {Promise<ArrayBuffer>} */\n    let buffer\n    let bufferOffset = 0\n    if (groupBuffer) {\n      buffer = Promise.resolve(groupBuffer)\n      bufferOffset = columnStartByte - groupStartByte\n    } else {\n      // wrap awaitable to ensure it's a promise\n      buffer = Promise.resolve(file.slice(columnStartByte, columnEndByte))\n    }\n\n    // read column data async\n    promises.push(buffer.then(arrayBuffer => {\n      const schemaPath = getSchemaPath(metadata.schema, columnMetadata.path_in_schema)\n      const reader = { view: new DataView(arrayBuffer), offset: bufferOffset }\n      /** @type {any[] | undefined} */\n      let columnData = readColumn(reader, rowLimit, columnMetadata, schemaPath, options)\n      // assert(columnData.length === Number(rowGroup.num_rows)\n\n      // TODO: fast path for non-nested columns\n      // Save column data for assembly\n      const subcolumn = columnMetadata.path_in_schema.join('.')\n      subcolumnData.set(subcolumn, columnData)\n      columnData = undefined\n\n      const subcolumns = subcolumnNames.get(columnName)\n      if (subcolumns?.every(name => subcolumnData.has(name))) {\n        // We have all data needed to assemble a top level column\n        assembleNested(subcolumnData, schemaPath[1])\n        columnData = subcolumnData.get(columnName)\n        if (!columnData) {\n          throw new Error(`parquet column data not assembled: ${columnName}`)\n        }\n      }\n\n      // do not emit column data until structs are fully parsed\n      if (!columnData) return\n      // notify caller of column data\n      options.onChunk?.({\n        columnName,\n        columnData,\n        rowStart: groupStart,\n        rowEnd: groupStart + columnData.length,\n      })\n    }))\n  }\n  await Promise.all(promises)\n  if (options.onComplete) {\n    // transpose columns into rows\n    const groupData = new Array(rowLimit)\n    const includedColumnNames = children\n      .map(child => child.element.name)\n      .filter(name => !columns || columns.includes(name))\n    const columnOrder = columns || includedColumnNames\n    const includedColumns = columnOrder\n      .map(name => includedColumnNames.includes(name) ? subcolumnData.get(name) : undefined)\n\n    for (let row = 0; row < rowLimit; row++) {\n      if (options.rowFormat === 'object') {\n        // return each row as an object\n        /** @type {Record<string, any>} */\n        const rowData = {}\n        columnOrder.forEach((name, index) => {\n          rowData[name] = includedColumns[index]?.[row]\n        })\n        groupData[row] = rowData\n      } else {\n        // return each row as an array\n        groupData[row] = includedColumns.map(column => column?.[row])\n      }\n    }\n    return groupData\n  }\n  return []\n}\n\n\n/**\n * Return a list of sub-columns needed to construct a top-level column.\n *\n * @import {ParquetReadOptions, RowGroup, SchemaTree} from '../src/types.d.ts'\n * @param {SchemaTree} schema\n * @param {string[]} output\n * @returns {string[]}\n */\nfunction getSubcolumns(schema, output = []) {\n  if (schema.children.length) {\n    for (const child of schema.children) {\n      getSubcolumns(child, output)\n    }\n  } else {\n    output.push(schema.path.join('.'))\n  }\n  return output\n}\n","import { parquetReadObjects } from './hyparquet.js'\nimport { parquetMetadataAsync } from './metadata.js'\n\n/**\n * Wraps parquetRead with orderBy support.\n * This is a parquet-aware query engine that can read a subset of rows and columns.\n * Accepts an optional orderBy column name to sort the results.\n * Note that using orderBy may SIGNIFICANTLY increase the query time.\n *\n * @param {ParquetReadOptions & { orderBy?: string }} options\n * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed\n */\nexport async function parquetQuery(options) {\n  const { file, rowStart, rowEnd, orderBy } = options\n  options.metadata ||= await parquetMetadataAsync(file)\n\n  // TODO: Faster path for: no orderBy, no rowStart/rowEnd, one row group\n\n  if (typeof orderBy === 'string') {\n    // Fetch orderBy column first\n    const orderColumn = await parquetReadObjects({ ...options, rowStart: undefined, rowEnd: undefined, columns: [orderBy] })\n\n    // Compute row groups to fetch\n    const sortedIndices = Array.from(orderColumn, (_, index) => index)\n      .sort((a, b) => compare(orderColumn[a][orderBy], orderColumn[b][orderBy]))\n      .slice(rowStart, rowEnd)\n\n    const sparseData = await parquetReadRows({ ...options, rows: sortedIndices })\n    const data = sortedIndices.map(index => sparseData[index])\n    return data\n  } else {\n    return await parquetReadObjects(options)\n  }\n}\n\n/**\n * Reads a list rows from a parquet file, reading only the row groups that contain the rows.\n * Returns a sparse array of rows.\n * @import {ParquetReadOptions} from '../src/types.d.ts'\n * @param {ParquetReadOptions & { rows: number[] }} options\n * @returns {Promise<Record<string, any>[]>}\n */\nasync function parquetReadRows(options) {\n  const { file, rows } = options\n  options.metadata ||= await parquetMetadataAsync(file)\n  const { row_groups: rowGroups } = options.metadata\n  // Compute row groups to fetch\n  const groupIncluded = Array(rowGroups.length).fill(false)\n  let groupStart = 0\n  const groupEnds = rowGroups.map(group => groupStart += Number(group.num_rows))\n  for (const index of rows) {\n    const groupIndex = groupEnds.findIndex(end => index < end)\n    groupIncluded[groupIndex] = true\n  }\n\n  // Compute row ranges to fetch\n  const rowRanges = []\n  let rangeStart\n  groupStart = 0\n  for (let i = 0; i < groupIncluded.length; i++) {\n    const groupEnd = groupStart + Number(rowGroups[i].num_rows)\n    if (groupIncluded[i]) {\n      if (rangeStart === undefined) {\n        rangeStart = groupStart\n      }\n    } else {\n      if (rangeStart !== undefined) {\n        rowRanges.push([rangeStart, groupEnd])\n        rangeStart = undefined\n      }\n    }\n    groupStart = groupEnd\n  }\n  if (rangeStart !== undefined) {\n    rowRanges.push([rangeStart, groupStart])\n  }\n\n  // Fetch by row group and map to rows\n  const sparseData = new Array(Number(options.metadata.num_rows))\n  for (const [rangeStart, rangeEnd] of rowRanges) {\n    // TODO: fetch in parallel\n    const groupData = await parquetReadObjects({ ...options, rowStart: rangeStart, rowEnd: rangeEnd })\n    for (let i = rangeStart; i < rangeEnd; i++) {\n      sparseData[i] = groupData[i - rangeStart]\n      sparseData[i].__index__ = i\n    }\n  }\n  return sparseData\n}\n\n/**\n * @param {any} a\n * @param {any} b\n * @returns {number}\n */\nfunction compare(a, b) {\n  if (a < b) return -1\n  if (a > b) return 1\n  return 1 // TODO: how to handle nulls?\n}\n","export { parquetMetadata, parquetMetadataAsync, parquetSchema } from './metadata.js'\n\nimport { parquetRead } from './read.js'\nexport { parquetRead }\n\nexport { parquetQuery } from './query.js'\n\nexport { snappyUncompress } from './snappy.js'\n\nexport { asyncBufferFromFile, asyncBufferFromUrl, byteLengthFromUrl, cachedAsyncBuffer, toJson } from './utils.js'\n\n/**\n * @param {ParquetReadOptions} options\n * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed\n*/\nexport function parquetReadObjects(options) {\n  return new Promise((onComplete, reject) => {\n    parquetRead({\n      rowFormat: 'object',\n      ...options,\n      onComplete,\n    }).catch(reject)\n  })\n}\n/**\n * Explicitly export types for use in downstream typescript projects through\n * `import { ParquetReadOptions } from 'hyparquet'` for example.\n *\n * @template {any} T\n * @typedef {import('../src/types.d.ts').Awaitable<T>} Awaitable<T>\n */\n\n/**\n * @typedef {import('../src/types.d.ts').AsyncBuffer} AsyncBuffer\n * @typedef {import('../src/types.d.ts').DataReader} DataReader\n * @typedef {import('../src/types.d.ts').FileMetaData} FileMetaData\n * @typedef {import('../src/types.d.ts').SchemaTree} SchemaTree\n * @typedef {import('../src/types.d.ts').SchemaElement} SchemaElement\n * @typedef {import('../src/types.d.ts').ParquetType} ParquetType\n * @typedef {import('../src/types.d.ts').FieldRepetitionType} FieldRepetitionType\n * @typedef {import('../src/types.d.ts').ConvertedType} ConvertedType\n * @typedef {import('../src/types.d.ts').TimeUnit} TimeUnit\n * @typedef {import('../src/types.d.ts').LogicalType} LogicalType\n * @typedef {import('../src/types.d.ts').LogicalTypeType} LogicalTypeType\n * @typedef {import('../src/types.d.ts').RowGroup} RowGroup\n * @typedef {import('../src/types.d.ts').ColumnChunk} ColumnChunk\n * @typedef {import('../src/types.d.ts').ColumnMetaData} ColumnMetaData\n * @typedef {import('../src/types.d.ts').Encoding} Encoding\n * @typedef {import('../src/types.d.ts').CompressionCodec} CompressionCodec\n * @typedef {import('../src/types.d.ts').Compressors} Compressors\n * @typedef {import('../src/types.d.ts').Statistics} Statistics\n * @typedef {import('../src/types.d.ts').PageType} PageType\n * @typedef {import('../src/types.d.ts').PageHeader} PageHeader\n * @typedef {import('../src/types.d.ts').DataPageHeader} DataPageHeader\n * @typedef {import('../src/types.d.ts').DictionaryPageHeader} DictionaryPageHeader\n * @typedef {import('../src/types.d.ts').DecodedArray} DecodedArray\n * @typedef {import('../src/types.d.ts').OffsetIndex} OffsetIndex\n * @typedef {import('../src/types.d.ts').ColumnIndex} ColumnIndex\n * @typedef {import('../src/types.d.ts').BoundaryOrder} BoundaryOrder\n * @typedef {import('../src/types.d.ts').ColumnData} ColumnData\n * @typedef {import('../src/types.d.ts').ParquetReadOptions} ParquetReadOptions\n */\n","import { parquetMetadataAsync, parquetQuery, toJson } from 'hyparquet'\n// import { Geometry as wkx } from 'wkx'\n\n/**\n * @import { AsyncBuffer } from 'hyparquet'\n * @import { GeoJSON, Geometry } from './geojson.js'\n * @param {AsyncBuffer} asyncBuffer \n * @returns {Promise<GeoJSON>}\n */\nexport async function geoparquet2geojson(asyncBuffer) {\n  const metadata = await parquetMetadataAsync(asyncBuffer)\n  const geoMetadata = metadata.key_value_metadata?.find(kv => kv.key === 'geo')\n  if (!geoMetadata) {\n    throw new Error('Invalid GeoParquet file: missing \"geo\" metadata')\n  }\n  const geoSchema = JSON.parse(geoMetadata.value || '{}')\n  console.log('Geoparquet schema:', geoSchema)\n\n  // Read all parquet data\n  const data = await parquetQuery({ file: asyncBuffer })\n  console.log('Geoparquet data:', toJson(data))\n\n  // Convert parquet data to GeoJSON\n  /**\n   * @import { Feature } from './geojson.js'\n   * @type {Feature[]}\n   */\n  const features = []\n\n  // According to the schema, the primary geometry column is 'geometry'\n  // We'll assume WKB encoding, and other columns are properties\n  const primaryColumn = geoSchema.primary_column || 'geometry'\n\n  for (const row of data) {\n    const wkbStr = row[primaryColumn]\n    if (!wkbStr) {\n      // No geometry\n      continue\n    }\n\n    // Convert the UTF-8 string with weird chars back to binary\n    // The parquetQuery returns strings. We'll treat as binary data with char codes.\n    const binary = new Uint8Array(wkbStr.length)\n    for (let i=0; i<wkbStr.length; i++) {\n      binary[i] = wkbStr.charCodeAt(i)\n    }\n\n    // const geom2 = wkx.parse(binary.buffer)\n    // console.log('WKB:', binary, 'WKX:', geom2)\n\n    const geometry = decodeWKB(binary)\n\n    // Extract properties (all fields except geometry)\n    /** @type {Record<string, any>} */\n    const properties = {}\n    for (const key of Object.keys(row)) {\n      if (key !== primaryColumn) {\n        properties[key] = row[key]\n      }\n    }\n\n    /** @type {Feature} */\n    const feature = {\n      type: 'Feature',\n      geometry,\n      properties\n    }\n\n    features.push(feature)\n  }\n\n  return {\n    type: 'FeatureCollection',\n    features,\n  }\n}\n\n/**\n * Minimal WKB (Well Known Binary) decoder supporting Polygon and MultiPolygon.\n * Supports both big-endian (byteOrder=0) and little-endian (byteOrder=1).\n * @param {Uint8Array} wkb \n * @returns {Geometry} GeoJSON geometry object\n */\nfunction decodeWKB(wkb) {\n  let offset = 0\n\n  // Byte order: 0 = big-endian, 1 = little-endian\n  const byteOrder = wkb[offset]; offset += 1\n  const isLittleEndian = (byteOrder === 1)\n\n  // Helper functions\n  /**\n   * Read a 32-bit unsigned integer from buffer at given offset\n   * @param {Uint8Array} buf\n   * @param {number} off\n   */\n  function readUInt32(buf, off) {\n    const dv = new DataView(buf.buffer, buf.byteOffset, buf.byteLength)\n    return dv.getUint32(off, isLittleEndian)\n  }\n\n  /**\n   * Read a 64-bit double from buffer at given offset\n   * @param {Uint8Array} buf\n   * @param {number} off\n   */\n  function readDouble(buf, off) {\n    const dv = new DataView(buf.buffer, buf.byteOffset, buf.byteLength)\n    return dv.getFloat64(off, isLittleEndian)\n  }\n\n  // Read geometry type\n  const geometryType = readUInt32(wkb, offset)\n  offset += 4\n\n  // WKB geometry types (OGC):\n  // 1=Point, 2=LineString, 3=Polygon, 4=MultiPoint, 5=MultiLineString, 6=MultiPolygon\n  // We handle Polygon(3) and MultiPolygon(6)\n  if (geometryType === 3) {\n    // Polygon\n    const numRings = readUInt32(wkb, offset); offset += 4\n    const coords = []\n    for (let r = 0; r < numRings; r++) {\n      const numPoints = readUInt32(wkb, offset); offset += 4\n      const ring = []\n      for (let p = 0; p < numPoints; p++) {\n        const x = readDouble(wkb, offset); offset += 8\n        const y = readDouble(wkb, offset); offset += 8\n        ring.push([x,y])\n      }\n      coords.push(ring)\n    }\n    return { type: 'Polygon', coordinates: coords }\n\n  } else if (geometryType === 6) {\n    // MultiPolygon\n    const numPolygons = readUInt32(wkb, offset); offset += 4\n    const polygons = []\n    for (let i = 0; i < numPolygons; i++) {\n      // Each polygon has its own byte order & geometry type\n      const pgByteOrder = wkb[offset]; offset += 1\n      const pgIsLittleEndian = (pgByteOrder === 1)\n      const pgType = (function() {\n        const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength)\n        const val = dv.getUint32(offset, pgIsLittleEndian)\n        offset += 4\n        return val\n      })()\n\n      if (pgType !== 3) throw new Error(`Expected Polygon in MultiPolygon, got ${pgType}`)\n\n      const numRings = (function() {\n        const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength)\n        const val = dv.getUint32(offset, pgIsLittleEndian)\n        offset += 4\n        return val\n      })()\n\n      const pgCoords = []\n      for (let r = 0; r < numRings; r++) {\n        const numPoints = (function() {\n          const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength)\n          const val = dv.getUint32(offset, pgIsLittleEndian)\n          offset += 4\n          return val\n        })()\n        const ring = []\n        for (let p = 0; p < numPoints; p++) {\n          const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength)\n          const x = dv.getFloat64(offset, pgIsLittleEndian); offset += 8\n          const y = dv.getFloat64(offset, pgIsLittleEndian); offset += 8\n          ring.push([x,y])\n        }\n        pgCoords.push(ring)\n      }\n      polygons.push(pgCoords)\n    }\n    return { type: 'MultiPolygon', coordinates: polygons }\n  } else {\n    throw new Error(\"Unsupported geometry type in this example: \" + geometryType)\n  }\n}\n","import { asyncBufferFromUrl } from 'hyparquet'\nimport { geoparquet2geojson } from '../src/index.js'\n\nlet map\n\nwindow.initMap = async function loadGeoParquet() {\n  // Create a new map centered on a default location\n  const div = document.getElementById('map')\n  map = new google.maps.Map(div, {\n    center: { lat: 40.7128, lng: -74.0060 },\n    zoom: 5,\n  })\n\n  // URL or path to your GeoParquet file\n  const parquetUrl = 'examples/example.geo.parquet'\n\n  try {\n    // Read the GeoParquet file and convert to GeoJSON\n    const asyncBuffer = await asyncBufferFromUrl({ url: parquetUrl })\n    const geojson = await geoparquet2geojson(asyncBuffer)\n\n    // Add the GeoJSON data to the map\n    map.data.addGeoJson(geojson)\n  } catch (error) {\n    console.error('Error loading or parsing GeoParquet file:', error)\n  }\n}\n"],"names":["ParquetType","Encoding","undefined","FieldRepetitionType","ConvertedType","CompressionCodec","PageType","dayMillis","convertWithDictionary","data","dictionary","schemaElement","encoding","utf8","endsWith","convert","output","Uint8Array","constructor","length","i","ctype","converted_type","scale","factor","Math","pow","arr","Array","parseDecimal","Number","type","from","map","parseInt96Date","Date","decoder","TextDecoder","v","JSON","parse","decode","Error","BigUint64Array","BigInt","logical_type","parseFloat16","unit","bytes","value","byte","days","nano","int16","sign","exp","frac","NaN","Infinity","schemaTree","schema","rootIndex","path","element","children","count","num_children","childElement","child","name","push","getSchemaPath","tree","part","find","getMaxRepetitionLevel","schemaPath","maxLevel","repetition_type","getMaxDefinitionLevel","slice","CompactType","deserializeTCompactProtocol","reader","lastFid","offset","view","byteLength","fid","newLastFid","readFieldBegin","readElement","getInt8","zigzag","readVarInt","readZigZag","readZigZagBigInt","getFloat64","stringLength","strBytes","buffer","byteOffset","elemType","listSize","sizeType","getUint8","size","getCompactType","readCollectionBegin","boolType","values","structValues","structLastFid","structFieldType","structFid","uuid","toString","padStart","result","shift","readVarBigInt","delta","async","parquetMetadataAsync","asyncBuffer","initialFetchSize","footerOffset","max","footerBuffer","footerView","DataView","getUint32","metadataLength","metadataOffset","metadataBuffer","combinedBuffer","ArrayBuffer","combinedView","set","parquetMetadata","arrayBuffer","metadataLengthOffset","metadata","version","field_1","field_2","field","type_length","field_3","field_4","field_5","field_6","field_7","precision","field_8","field_id","field_9","logicalType","field_10","columnSchema","filter","e","num_rows","row_groups","rowGroup","columns","column","columnIndex","file_path","file_offset","meta_data","encodings","path_in_schema","codec","num_values","total_uncompressed_size","total_compressed_size","key_value_metadata","data_page_offset","index_page_offset","dictionary_page_offset","field_11","statistics","convertStats","field_12","encoding_stats","field_13","encodingStat","page_type","bloom_filter_offset","field_14","bloom_filter_length","field_15","size_statistics","field_16","unencoded_byte_array_data_bytes","repetition_level_histogram","definition_level_histogram","offset_index_offset","offset_index_length","column_index_offset","column_index_length","crypto_metadata","encrypted_column_metadata","total_byte_size","sorting_columns","sortingColumn","column_idx","descending","nulls_first","ordinal","keyValue","key","created_by","metadata_length","isAdjustedToUTC","timeUnit","bitWidth","isSigned","stats","convertMetadata","min","null_count","distinct_count","max_value","min_value","is_max_value_exact","is_min_value_exact","getFloat32","getInt32","getBigInt64","assembleLists","definitionLevels","repetitionLevels","repetitionPath","maxDefinitionLevel","n","valueIndex","containerStack","currentContainer","currentDepth","currentDefLevel","currentRepLevel","at","def","rep","pop","newList","assembleNested","subcolumnData","depth","join","optional","nextDepth","firstChild","isListLike","sublist","subDepth","subcolumn","get","flattenAtDepth","delete","keyChild","valueChild","isMapLike","mapName","keys","out","assembleMaps","invertDepth","struct","childData","inverted","invertStruct","obj","j","Object","deltaBinaryUnpack","int32","Int32Array","blockSize","miniblockPerBlock","outputIndex","valuesPerMiniblock","minDelta","bitWidths","bitpackPos","miniblockCount","mask","bits","ceil","clz32","readRleBitPackedHybrid","width","seen","header","readBitPacked","readRle","left","right","byteStreamSplit","typeLength","byteWidth","b","Float32Array","Float64Array","BigInt64Array","split","subarray","readPlain","fixedLength","bitOffset","readPlainBoolean","align","readPlainInt32","readPlainInt64","low","high","readPlainInt96","readPlainFloat","readPlainDouble","readPlainByteArray","readPlainByteArrayFixed","aligned","WORD_MASK","copyBytes","fromArray","fromPos","toArray","toPos","selfCopyBytes","array","pos","readDataPage","daph","dataPage","maxRepetitionLevel","readRepetitionLevels","numNulls","readDefinitionLevels","nValues","readDictionaryPage","diph","columnMetadata","decompressPage","compressedBytes","uncompressed_page_size","compressors","page","customDecompressor","input","inputLength","outputLength","outPos","c","len","isNaN","lengthSize","snappyUncompress","readDataPageV2","ph","daph2","data_page_header_v2","repetition_levels_byte_length","readRepetitionLevelsV2","definition_levels_byte_length","readDefinitionLevelsV2","uncompressedPageSize","is_compressed","pageView","pageReader","num_nulls","x","lengths","deltaLengthByteArray","prefixData","suffixData","suffix","deltaByteArray","toJson","isArray","toISOString","newObj","concat","aaa","bbb","asyncBufferFromUrl","url","requestInit","fetch","method","then","res","ok","status","headers","parseInt","byteLengthFromUrl","init","start","end","Headers","endStr","body","readColumn","rowLimit","rowData","parquetHeader","compressed_page_size","data_page_header","dictionary_page_header","getColumnRange","columnOffset","crc","definition_level_encoding","repetition_level_encoding","index_page_header","is_sorted","readRowGroup","options","groupStart","file","groupBuffer","groupStartByte","groupEndByte","forEach","includes","columnStartByte","columnEndByte","promises","subcolumnNames","Map","getSubcolumns","columnName","columnBytes","console","warn","toLocaleString","bufferOffset","Promise","resolve","columnData","subcolumns","every","has","onChunk","rowStart","rowEnd","all","onComplete","groupData","includedColumnNames","columnOrder","includedColumns","row","rowFormat","index","parquetQuery","orderBy","orderColumn","parquetReadObjects","sortedIndices","_","sort","a","compare","sparseData","rows","rowGroups","groupIncluded","fill","groupEnds","group","findIndex","rowRanges","rangeStart","groupEnd","rangeEnd","__index__","parquetReadRows","reject","groupRows","parquetRead","catch","decodeWKB","wkb","byteOrder","isLittleEndian","readUInt32","buf","off","readDouble","geometryType","numRings","coords","r","numPoints","ring","p","y","coordinates","numPolygons","polygons","pgByteOrder","pgIsLittleEndian","pgType","val","pgCoords","dv","window","initMap","div","document","getElementById","google","maps","center","lat","lng","zoom","geojson","geoMetadata","kv","geoSchema","log","features","primaryColumn","primary_column","wkbStr","binary","charCodeAt","geometry","properties","feature","geoparquet2geojson","addGeoJson","error"],"mappings":"AACO,MAAMA,EAAc,CACzB,UACA,QACA,QACA,QACA,QACA,SACA,aACA,wBAGWC,EAAW,CACtB,aACAC,EACA,mBACA,MACA,aACA,sBACA,0BACA,mBACA,iBACA,qBAGWC,EAAsB,CACjC,WACA,WACA,YAIWC,EAAgB,CAC3B,OACA,MACA,gBACA,OACA,OACA,UACA,OACA,cACA,cACA,mBACA,mBACA,SACA,UACA,UACA,UACA,QACA,SACA,SACA,SACA,OACA,OACA,YAsBWC,EAAmB,CAC9B,eACA,SACA,OACA,MACA,SACA,MACA,OACA,WAIWC,EAAW,CACtB,YACA,aACA,kBACA,gBC5FIC,EAAY,MAaX,SAASC,EAAsBC,EAAMC,EAAYC,EAAeC,EAAUC,GAAO,GACtF,GAAIH,GAAcE,EAASE,SAAS,eAAgB,CAElDJ,EAAaK,EAAQL,EAAYC,EAAeE,GAChD,IAAIG,EAASP,EACTA,aAAgBQ,cAAgBP,aAAsBO,cAExDD,EAAS,IAAIN,EAAWQ,YAAYT,EAAKU,SAE3C,IAAK,IAAIC,EAAI,EAAGA,EAAIX,EAAKU,OAAQC,IAC/BJ,EAAOI,GAAKV,EAAWD,EAAKW,IAE9B,OAAOJ,CACX,CACI,OAAOD,EAAQN,EAAME,EAAeE,EAExC,CAUO,SAASE,EAAQN,EAAME,EAAeE,GAAO,GAClD,MAAMQ,EAAQV,EAAcW,eAC5B,GAAc,YAAVD,EAAqB,CACvB,MAAME,EAAQZ,EAAcY,OAAS,EAC/BC,EAASC,KAAKC,IAAI,IAAKH,GACvBI,EAAM,IAAIC,MAAMnB,EAAKU,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIO,EAAIR,OAAQC,IAC1BX,EAAK,aAAcQ,WACrBU,EAAIP,GAAKS,EAAapB,EAAKW,IAAMI,EAEjCG,EAAIP,GAAKU,OAAOrB,EAAKW,IAAMI,EAG/B,OAAOG,CACX,CACE,QAAczB,IAAVmB,GAA8C,UAAvBV,EAAcoB,KACvC,OAAOH,MAAMI,KAAKvB,GAAMwB,IAAIC,GAE9B,GAAc,SAAVb,EAAkB,CACpB,MAAMM,EAAM,IAAIC,MAAMnB,EAAKU,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIO,EAAIR,OAAQC,IAC9BO,EAAIP,GAAK,IAAIe,KAAK1B,EAAKW,GAAKb,GAE9B,OAAOoB,CACX,CACE,GAAc,qBAAVN,EAA8B,CAChC,MAAMM,EAAM,IAAIC,MAAMnB,EAAKU,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIO,EAAIR,OAAQC,IAC9BO,EAAIP,GAAK,IAAIe,KAAKL,OAAOrB,EAAKW,KAEhC,OAAOO,CACX,CACE,GAAc,qBAAVN,EAA8B,CAChC,MAAMM,EAAM,IAAIC,MAAMnB,EAAKU,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIO,EAAIR,OAAQC,IAC9BO,EAAIP,GAAK,IAAIe,KAAKL,OAAOrB,EAAKW,GAAK,QAErC,OAAOO,CACX,CACE,GAAc,SAAVN,EAAkB,CACpB,MAAMe,EAAU,IAAIC,YACpB,OAAO5B,EAAKwB,KAAIK,GAAKC,KAAKC,MAAMJ,EAAQK,OAAOH,KACnD,CACE,GAAc,SAAVjB,EACF,MAAM,IAAIqB,MAAM,8BAElB,GAAc,aAAVrB,EACF,MAAM,IAAIqB,MAAM,kCAElB,GAAc,SAAVrB,GAAoBR,GAA+B,eAAvBF,EAAcoB,KAAuB,CACnE,MAAMK,EAAU,IAAIC,YACdV,EAAM,IAAIC,MAAMnB,EAAKU,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIO,EAAIR,OAAQC,IAC9BO,EAAIP,GAAKX,EAAKW,IAAMgB,EAAQK,OAAOhC,EAAKW,IAE1C,OAAOO,CACX,CACE,GAAc,YAAVN,EAAqB,CACvB,MAAMM,EAAM,IAAIgB,eAAelC,EAAKU,QACpC,IAAK,IAAIC,EAAI,EAAGA,EAAIO,EAAIR,OAAQC,IAC9BO,EAAIP,GAAKwB,OAAOnC,EAAKW,IAEvB,OAAOO,CACX,CACE,GAAyC,YAArChB,EAAckC,cAAcd,KAC9B,OAAOH,MAAMI,KAAKvB,GAAMwB,IAAIa,GAE9B,GAAyC,cAArCnC,EAAckC,cAAcd,KAAsB,CACpD,MAAMgB,KAAEA,GAASpC,EAAckC,aAC/B,IAAIrB,EAAS,GACA,WAATuB,IAAmBvB,EAAS,OACnB,UAATuB,IAAkBvB,EAAS,UAC/B,MAAMG,EAAM,IAAIC,MAAMnB,EAAKU,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIO,EAAIR,OAAQC,IAC9BO,EAAIP,GAAK,IAAIe,KAAKL,OAAOrB,EAAKW,GAAKI,IAErC,OAAOG,CACX,CACE,OAAOlB,CACT,CAMO,SAASoB,EAAamB,GAE3B,IAAIC,EAAQ,EACZ,IAAK,MAAMC,KAAQF,EACjBC,EAAQA,GAAS,EAAIC,EAEvB,OAAOD,CACT,CAMA,SAASf,EAAee,GACtB,MAAME,EAAOrB,QAAQmB,GAAS,KAAO,UAC/BG,EAAOtB,QAAgB,oBAARmB,GAA+B,UAEpD,OAAO,IAAId,KADIgB,EAAO5C,EAAY6C,EAEpC,CAMO,SAASN,EAAaE,GAC3B,IAAKA,EAAO,OACZ,MAAMK,EAAQL,EAAM,IAAM,EAAIA,EAAM,GAC9BM,EAAOD,GAAS,IAAM,EAAI,EAC1BE,EAAMF,GAAS,GAAK,GACpBG,EAAe,KAARH,EACb,OAAY,IAARE,EAAkBD,EAAO7B,KAAKC,IAAI,GAAI,KAAO8B,EAAO,MAC5C,KAARD,EAAqBC,EAAOC,IAAMH,GAAOI,KACtCJ,EAAO7B,KAAKC,IAAI,EAAG6B,EAAM,KAAO,EAAIC,EAAO,KACpD,CCpJA,SAASG,EAAWC,EAAQC,EAAWC,GACrC,MAAMC,EAAUH,EAAOC,GACjBG,EAAW,GACjB,IAAIC,EAAQ,EAGZ,GAAIF,EAAQG,aACV,KAAOF,EAAS7C,OAAS4C,EAAQG,cAAc,CAC7C,MAAMC,EAAeP,EAAOC,EAAYI,GAClCG,EAAQT,EAAWC,EAAQC,EAAYI,EAAO,IAAIH,EAAMK,EAAaE,OAC3EJ,GAASG,EAAMH,MACfD,EAASM,KAAKF,EACpB,CAGE,MAAO,CAAEH,QAAOF,UAASC,WAAUF,OACrC,CASO,SAASS,EAAcX,EAAQS,GACpC,IAAIG,EAAOb,EAAWC,EAAQ,EAAG,IACjC,MAAME,EAAO,CAACU,GACd,IAAK,MAAMC,KAAQJ,EAAM,CACvB,MAAMD,EAAQI,EAAKR,SAASU,MAAKN,GAASA,EAAML,QAAQM,OAASI,IACjE,IAAKL,EAAO,MAAM,IAAI1B,MAAM,qCAAqC2B,KACjEP,EAAKQ,KAAKF,GACVI,EAAOJ,CACX,CACE,OAAON,CACT,CAQO,SAASa,EAAsBC,GACpC,IAAIC,EAAW,EACf,IAAK,MAAMd,QAAEA,KAAaa,EACQ,aAA5Bb,EAAQe,iBACVD,IAGJ,OAAOA,CACT,CAQO,SAASE,EAAsBH,GACpC,IAAIC,EAAW,EACf,IAAK,MAAMd,QAAEA,KAAaa,EAAWI,MAAM,GACT,aAA5BjB,EAAQe,iBACVD,IAGJ,OAAOA,CACT,CC3EA,MAAMI,EACE,EADFA,EAEE,EAFFA,EAGG,EAHHA,EAIE,EAJFA,EAKC,EALDA,EAMC,EANDA,EAOC,EAPDA,EAQI,EARJA,EASI,EATJA,EAUE,EAVFA,EAaI,GAbJA,EAcE,GAUD,SAASC,EAA4BC,GAC1C,IAAIC,EAAU,EAEd,MAAMnC,EAAQ,CAAA,EAEd,KAAOkC,EAAOE,OAASF,EAAOG,KAAKC,YAAY,CAE7C,MAAOxD,EAAMyD,EAAKC,GAAcC,EAAeP,EAAQC,GAGvD,GAFAA,EAAUK,EAEN1D,IAASkD,EACX,MAIFhC,EAAM,SAASuC,KAASG,EAAYR,EAAQpD,EAChD,CAEE,OAAOkB,CACT,CASA,SAAS0C,EAAYR,EAAQpD,GAC3B,OAAQA,GACR,KAAKkD,EACH,OAAO,EACT,KAAKA,EACH,OAAO,EACT,KAAKA,EAEH,OAAOE,EAAOG,KAAKM,QAAQT,EAAOE,UACpC,KAAKJ,EACL,KAAKA,EACH,OAkGJ,SAAoBE,GAClB,MAAMU,EAASC,EAAWX,GAE1B,OAAOU,IAAW,IAAe,EAATA,EAC1B,CAtGWE,CAAWZ,GACpB,KAAKF,EACH,OAAOe,EAAiBb,GAC1B,KAAKF,EAAoB,CACvB,MAAMhC,EAAQkC,EAAOG,KAAKW,WAAWd,EAAOE,QAAQ,GAEpD,OADAF,EAAOE,QAAU,EACVpC,CACX,CACE,KAAKgC,EAAoB,CACvB,MAAMiB,EAAeJ,EAAWX,GAC1BgB,EAAW,IAAIlF,WAAWkE,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQa,GAE5F,OADAf,EAAOE,QAAUa,EACVC,CACX,CACE,KAAKlB,EAAkB,CACrB,MAAOqB,EAAUC,GA8IrB,SAA6BpB,GAC3B,MAAMqB,EAAWrB,EAAOG,KAAKmB,SAAStB,EAAOE,UACvCqB,EAAOF,GAAY,EACnBzE,EAAO4E,EAAeH,GAC5B,GAAa,KAATE,EAAa,CAEf,MAAO,CAAC3E,EADQ+D,EAAWX,GAE/B,CACE,MAAO,CAACpD,EAAM2E,EAChB,CAvJiCE,CAAoBzB,GAC3C0B,EAAWP,IAAarB,GAAoBqB,IAAarB,EACzD6B,EAAS,IAAIlF,MAAM2E,GACzB,IAAK,IAAInF,EAAI,EAAGA,EAAImF,EAAUnF,IAC5B0F,EAAO1F,GAAKyF,EAAqD,IAA1ClB,EAAYR,EAAQF,GAA0BU,EAAYR,EAAQmB,GAE3F,OAAOQ,CACX,CACE,KAAK7B,EAAoB,CAEvB,MAAM8B,EAAe,CAAA,EACrB,IAAIC,EAAgB,EACpB,OAAa,CACX,IAAIC,EAAiBC,EAErB,IADCD,EAAiBC,EAAWF,GAAiBtB,EAAeP,EAAQ6B,GACjEC,IAAoBhC,EACtB,MAEF8B,EAAa,SAASG,KAAevB,EAAYR,EAAQ8B,EAC/D,CACI,OAAOF,CACX,CAEE,KAAK9B,EAAkB,CAErB,IAAIkC,EAAO,GACX,IAAK,IAAI/F,EAAI,EAAGA,EAAI,GAAIA,IACtB+F,GAAQhC,EAAOG,KAAKmB,SAAStB,EAAOE,UAAU+B,SAAS,IAAIC,SAAS,EAAG,KAEzE,OAAOF,CACX,CACE,QACE,MAAM,IAAIzE,MAAM,0BAA0BX,KAE9C,CAUO,SAAS+D,EAAWX,GACzB,IAAImC,EAAS,EACTC,EAAQ,EACZ,OAAa,CACX,MAAMrE,EAAOiC,EAAOG,KAAKmB,SAAStB,EAAOE,UAEzC,GADAiC,IAAkB,IAAPpE,IAAgBqE,IACd,IAAPrE,GACJ,OAAOoE,EAETC,GAAS,CACb,CACA,CAyCO,SAASvB,EAAiBb,GAC/B,MAAMU,EAlCR,SAAuBV,GACrB,IAAImC,EAAS,GACTC,EAAQ,GACZ,OAAa,CACX,MAAMrE,EAAOiC,EAAOG,KAAKmB,SAAStB,EAAOE,UAEzC,GADAiC,GAAU1E,OAAc,IAAPM,IAAgBqE,IACpB,IAAPrE,GACJ,OAAOoE,EAETC,GAAS,EACb,CACA,CAuBiBC,CAAcrC,GAE7B,OAAOU,GAAUjD,OAAO,KAAOiD,EAASjD,OAAO,GACjD,CAQA,SAAS+D,EAAezD,GACtB,OAAc,GAAPA,CACT,CASA,SAASwC,EAAeP,EAAQC,GAC9B,MAAMrD,EAAOoD,EAAOG,KAAKmB,SAAStB,EAAOE,UACzC,IAAY,GAAPtD,KAAiBkD,EAEpB,MAAO,CAAC,EAAG,EAAGG,GAEhB,MAAMqC,EAAQ1F,GAAQ,EACtB,IAAIyD,EACJ,IAAIiC,EAIF,MAAM,IAAI/E,MAAM,oCAElB,OAJE8C,EAAMJ,EAAUqC,EAIX,CAACd,EAAe5E,GAAOyD,EAAKA,EACrC,CCxLOkC,eAAeC,EAAqBC,EAAaC,EAAmB,GAAK,IAC9E,IAAKD,EAAa,MAAM,IAAIlF,MAAM,4BAClC,KAAMkF,EAAYrC,YAAc,GAAI,MAAM,IAAI7C,MAAM,uCAGpD,MAAMoF,EAAerG,KAAKsG,IAAI,EAAGH,EAAYrC,WAAasC,GACpDG,QAAqBJ,EAAY5C,MAAM8C,EAAcF,EAAYrC,YAGjE0C,EAAa,IAAIC,SAASF,GAChC,GAAgE,YAA5DC,EAAWE,UAAUH,EAAazC,WAAa,GAAG,GACpD,MAAM,IAAI7C,MAAM,yCAKlB,MAAM0F,EAAiBH,EAAWE,UAAUH,EAAazC,WAAa,GAAG,GACzE,GAAI6C,EAAiBR,EAAYrC,WAAa,EAC5C,MAAM,IAAI7C,MAAM,2BAA2B0F,8BAA2CR,EAAYrC,WAAa,KAIjH,GAAI6C,EAAiB,EAAIP,EAAkB,CAEzC,MAAMQ,EAAiBT,EAAYrC,WAAa6C,EAAiB,EAC3DE,QAAuBV,EAAY5C,MAAMqD,EAAgBP,GAEzDS,EAAiB,IAAIC,YAAYJ,EAAiB,GAClDK,EAAe,IAAIxH,WAAWsH,GAGpC,OAFAE,EAAaC,IAAI,IAAIzH,WAAWqH,IAChCG,EAAaC,IAAI,IAAIzH,WAAW+G,GAAeF,EAAeO,GACvDM,EAAgBJ,EAC3B,CAEI,OAAOI,EAAgBX,EAE3B,CAQO,SAASW,EAAgBC,GAC9B,IAAKA,EAAa,MAAM,IAAIlG,MAAM,4BAClC,MAAM4C,EAAO,IAAI4C,SAASU,GAG1B,GAAItD,EAAKC,WAAa,EACpB,MAAM,IAAI7C,MAAM,6BAElB,GAAkD,YAA9C4C,EAAK6C,UAAU7C,EAAKC,WAAa,GAAG,GACtC,MAAM,IAAI7C,MAAM,yCAKlB,MAAMmG,EAAuBvD,EAAKC,WAAa,EACzC6C,EAAiB9C,EAAK6C,UAAUU,GAAsB,GAC5D,GAAIT,EAAiB9C,EAAKC,WAAa,EAErC,MAAM,IAAI7C,MAAM,2BAA2B0F,8BAA2C9C,EAAKC,WAAa,KAG1G,MAEMuD,EAAW5D,EADF,CAAEI,OAAMD,OADAwD,EAAuBT,IAGxChG,EAAU,IAAIC,YACpB,SAASI,EAAiCQ,GACxC,OAAOA,GAASb,EAAQK,OAAOQ,EACnC,CAGE,MAAM8F,EAAUD,EAASE,QAEnBpF,EAASkF,EAASG,QAAQhH,KAAwBiH,IAAW,CACjEnH,KAAM/B,EAAYkJ,EAAMF,SACxBG,YAAaD,EAAMD,QACnBnE,gBAAiB3E,EAAoB+I,EAAME,SAC3C/E,KAAM5B,EAAOyG,EAAMG,SACnBnF,aAAcgF,EAAMI,QACpBhI,eAAgBlB,EAAc8I,EAAMK,SACpChI,MAAO2H,EAAMM,QACbC,UAAWP,EAAMQ,QACjBC,SAAUT,EAAMU,QAChB/G,aAAcgH,EAAYX,EAAMY,cAG5BC,EAAenG,EAAOoG,QAAOC,GAAKA,EAAElI,OACpCmI,EAAWpB,EAASM,QACpBe,EAAarB,EAASO,QAAQpH,KAAwBmI,IAAc,CACxEC,QAASD,EAASpB,QAAQ/G,KAAI,CAAoBqI,EAA8BC,KAAiB,CAC/FC,UAAW/H,EAAO6H,EAAOtB,SACzByB,YAAaH,EAAOrB,QACpByB,UAAWJ,EAAOlB,SAAW,CAC3BrH,KAAM/B,EAAYsK,EAAOlB,QAAQJ,SACjC2B,UAAWL,EAAOlB,QAAQH,SAAShH,KAA2BgI,GAAMhK,EAASgK,KAC7EW,eAAgBN,EAAOlB,QAAQA,QAAQnH,IAAIQ,GAC3CoI,MAAOxK,EAAiBiK,EAAOlB,QAAQC,SACvCyB,WAAYR,EAAOlB,QAAQE,QAC3ByB,wBAAyBT,EAAOlB,QAAQG,QACxCyB,sBAAuBV,EAAOlB,QAAQI,QACtCyB,mBAAoBX,EAAOlB,QAAQM,QACnCwB,iBAAkBZ,EAAOlB,QAAQQ,QACjCuB,kBAAmBb,EAAOlB,QAAQU,SAClCsB,uBAAwBd,EAAOlB,QAAQiC,SACvCC,WAAYC,EAAajB,EAAOlB,QAAQoC,SAAUzB,EAAaQ,IAC/DkB,eAAgBnB,EAAOlB,QAAQsC,UAAUzJ,KAAwB0J,IAAkB,CACjFC,UAAWtL,EAASqL,EAAa3C,SACjCpI,SAAUX,EAAS0L,EAAa1C,SAChChF,MAAO0H,EAAavC,YAEtByC,oBAAqBvB,EAAOlB,QAAQ0C,SACpCC,oBAAqBzB,EAAOlB,QAAQ4C,SACpCC,gBAAiB3B,EAAOlB,QAAQ8C,UAAY,CAC1CC,gCAAiC7B,EAAOlB,QAAQ8C,SAASlD,QACzDoD,2BAA4B9B,EAAOlB,QAAQ8C,SAASjD,QACpDoD,2BAA4B/B,EAAOlB,QAAQ8C,SAAS9C,UAGxDkD,oBAAqBhC,EAAOjB,QAC5BkD,oBAAqBjC,EAAOhB,QAC5BkD,oBAAqBlC,EAAOf,QAC5BkD,oBAAqBnC,EAAOd,QAC5BkD,gBAAiBpC,EAAOd,QACxBmD,0BAA2BrC,EAAOZ,YAEpCkD,gBAAiBxC,EAASnB,QAC1BiB,SAAUE,EAAShB,QACnByD,gBAAiBzC,EAASf,SAASpH,KAAwB6K,IAAmB,CAC5EC,WAAYD,EAAc9D,QAC1BgE,WAAYF,EAAc7D,QAC1BgE,YAAaH,EAAc1D,YAE7BqB,YAAaL,EAASd,QACtB0B,sBAAuBZ,EAASb,QAChC2D,QAAS9C,EAASZ,YAEdyB,EAAqBnC,EAASQ,SAASrH,KAAwBkL,IAAc,CACjFC,IAAK3K,EAAO0K,EAASnE,SACrB/F,MAAOR,EAAO0K,EAASlE,aAIzB,MAAO,CACLF,UACAnF,SACAsG,WACAC,aACAc,qBACAoC,WARiB5K,EAAOqG,EAASS,SASjC+D,gBAAiBlF,EAErB,CAgBA,SAASyB,EAAYA,GACnB,OAAIA,GAAab,QAAgB,CAAEjH,KAAM,UACrC8H,GAAaZ,QAAgB,CAAElH,KAAM,OACrC8H,GAAaT,QAAgB,CAAErH,KAAM,QACrC8H,GAAaR,QAAgB,CAAEtH,KAAM,QACrC8H,GAAaP,QAAgB,CAC/BvH,KAAM,UACNR,MAAOsI,EAAYP,QAAQN,QAC3BS,UAAWI,EAAYP,QAAQL,SAE7BY,GAAaN,QAAgB,CAAExH,KAAM,QACrC8H,GAAaL,QAAgB,CAC/BzH,KAAM,OACNwL,gBAAiB1D,EAAYL,QAAQR,QACrCjG,KAAMyK,EAAS3D,EAAYL,QAAQP,UAEjCY,GAAaH,QAAgB,CAC/B3H,KAAM,YACNwL,gBAAiB1D,EAAYH,QAAQV,QACrCjG,KAAMyK,EAAS3D,EAAYH,QAAQT,UAEjCY,GAAaC,SAAiB,CAChC/H,KAAM,UACN0L,SAAU5D,EAAYC,SAASd,QAC/B0E,SAAU7D,EAAYC,SAASb,SAE7BY,GAAawB,SAAiB,CAAEtJ,KAAM,QACtC8H,GAAa2B,SAAiB,CAAEzJ,KAAM,QACtC8H,GAAa6B,SAAiB,CAAE3J,KAAM,QACtC8H,GAAaiC,SAAiB,CAAE/J,KAAM,QACtC8H,GAAamC,SAAiB,CAAEjK,KAAM,WACnC8H,CACT,CAMA,SAAS2D,EAASzK,GAChB,GAAIA,EAAKiG,QAAS,MAAO,SACzB,GAAIjG,EAAKkG,QAAS,MAAO,SACzB,GAAIlG,EAAKqG,QAAS,MAAO,QACzB,MAAM,IAAI1G,MAAM,6BAClB,CAUA,SAAS6I,EAAaoC,EAAO/J,GAC3B,OAAO+J,GAAS,CACd5F,IAAK6F,EAAgBD,EAAM3E,QAASpF,GACpCiK,IAAKD,EAAgBD,EAAM1E,QAASrF,GACpCkK,WAAYH,EAAMvE,QAClB2E,eAAgBJ,EAAMtE,QACtB2E,UAAWJ,EAAgBD,EAAMrE,QAAS1F,GAC1CqK,UAAWL,EAAgBD,EAAMpE,QAAS3F,GAC1CsK,mBAAoBP,EAAMnE,QAC1B2E,mBAAoBR,EAAMjE,QAE9B,CAOO,SAASkE,EAAgB3K,EAAOW,GACrC,MAAM7B,KAAEA,EAAIT,eAAEA,EAAcuB,aAAEA,GAAiBe,EAC/C,QAAc1D,IAAV+C,EAAqB,OAAOA,EAChC,GAAa,YAATlB,EAAoB,OAAoB,IAAbkB,EAAM,GACrC,GAAa,eAATlB,EAAuB,OAAO,IAAIM,aAAcI,OAAOQ,GAC3D,MAAMqC,EAAO,IAAI4C,SAASjF,EAAMmD,OAAQnD,EAAMoD,WAAYpD,EAAMsC,YAChE,MAAa,UAATxD,GAAwC,IAApBuD,EAAKC,WAAyBD,EAAK8I,WAAW,GAAG,GAC5D,WAATrM,GAAyC,IAApBuD,EAAKC,WAAyBD,EAAKW,WAAW,GAAG,GAC7D,UAATlE,GAAuC,SAAnBT,EAAkC,IAAIa,KAA8B,MAAzBmD,EAAK+I,SAAS,GAAG,IACvE,UAATtM,GAAuC,qBAAnBT,EAA8C,IAAIa,KAAKL,OAAOwD,EAAKgJ,YAAY,GAAG,GAAQ,QACrG,UAATvM,GAAuC,qBAAnBT,EAA8C,IAAIa,KAAKL,OAAOwD,EAAKgJ,YAAY,GAAG,KAC7F,UAATvM,GAA2C,cAAvBc,GAAcd,MAA+C,UAAvBc,GAAcE,KAAyB,IAAIZ,KAAKL,OAAOwD,EAAKgJ,YAAY,GAAG,GAAQ,WACpI,UAATvM,GAA2C,cAAvBc,GAAcd,MAA+C,WAAvBc,GAAcE,KAA0B,IAAIZ,KAAKL,OAAOwD,EAAKgJ,YAAY,GAAG,GAAQ,QACrI,UAATvM,GAA2C,cAAvBc,GAAcd,KAA6B,IAAII,KAAKL,OAAOwD,EAAKgJ,YAAY,GAAG,KAC1F,UAATvM,GAAwC,IAApBuD,EAAKC,WAAyBD,EAAK+I,SAAS,GAAG,GAC1D,UAATtM,GAAwC,IAApBuD,EAAKC,WAAyBD,EAAKgJ,YAAY,GAAG,GACnD,YAAnBhN,EAAqCO,EAAaoB,GAASxB,KAAKC,IAAI,KAAMkC,EAAOrC,OAAS,IACnE,YAAvBsB,GAAcd,KAA2Be,EAAaG,GACdA,CAG9C,CClRO,SAASsL,EACdvN,EAAQwN,EAAkBC,EAAkB3H,EAAQ4H,EAAgBC,GAEpE,MAAMC,EAAIJ,GAAkBrN,QAAUsN,EAAiBtN,OACvD,IAAI0N,EAAa,EAGjB,MAAMC,EAAiB,CAAC9N,GACxB,IAAI+N,EAAmB/N,EACnBgO,EAAe,EACfC,EAAkB,EAClBC,EAAkB,EAEtB,GAAIT,EAAiB,GAEnB,KAAOO,EAAeN,EAAevN,OAAS,GAAK+N,EAAkBT,EAAiB,IAEpFM,EAAmBA,EAAiBI,IAAI,GACxCL,EAAexK,KAAKyK,GACpBC,IACqC,aAAjCN,EAAeM,IAA8BC,IACZ,aAAjCP,EAAeM,IAA8BE,IAIrD,IAAK,IAAI9N,EAAI,EAAGA,EAAIwN,EAAGxN,IAAK,CAE1B,MAAMgO,EAAMZ,GAAkBrN,OAASqN,EAAiBpN,GAAKuN,EACvDU,EAAMZ,EAAiBrN,GAG7B,KAAO4N,IAAiBK,EAAMH,GAAoD,aAAjCR,EAAeM,KACzB,aAAjCN,EAAeM,KACjBF,EAAeQ,MACfL,KAEmC,aAAjCP,EAAeM,IAA8BE,IACjDF,IAMF,IAHAD,EAAmBD,EAAeK,IAAI,IAInCH,EAAeN,EAAevN,OAAS,GAA0C,aAArCuN,EAAeM,EAAe,MAC1EC,EAAkBG,GAA4C,aAArCV,EAAeM,EAAe,KACxD,CAEA,GADAA,IACqC,aAAjCN,EAAeM,GAA8B,CAE/C,MAAMO,EAAU,GAChBR,EAAiBzK,KAAKiL,GACtBR,EAAmBQ,EACnBT,EAAexK,KAAKiL,GACpBN,GACR,CAC2C,aAAjCP,EAAeM,IAA8BE,GACvD,CAGQE,IAAQT,EAEVI,EAAiBzK,KAAKwC,EAAO+H,MACpBG,IAAiBN,EAAevN,OAAS,EAClD4N,EAAiBzK,KAAK,MAEtByK,EAAiBzK,KAAK,GAE5B,CAGE,IAAKtD,EAAOG,OAEV,IAAK,IAAIC,EAAI,EAAGA,EAAIuN,EAAoBvN,IAAK,CAE3C,MAAMmO,EAAU,GAChBR,EAAiBzK,KAAKiL,GACtBR,EAAmBQ,CACzB,CAGE,OAAOvO,CACT,CAWO,SAASwO,EAAeC,EAAe7L,EAAQ8L,EAAQ,GAC5D,MAAM5L,EAAOF,EAAOE,KAAK6L,KAAK,KACxBC,EAA8C,aAAnChM,EAAOG,QAAQe,gBAC1B+K,EAAYD,EAAWF,EAAQ,EAAIA,EAEzC,GH/BK,SAAoB9L,GACzB,IAAKA,EAAQ,OAAO,EACpB,GAAsC,SAAlCA,EAAOG,QAAQzC,eAA2B,OAAO,EACrD,GAAIsC,EAAOI,SAAS7C,OAAS,EAAG,OAAO,EAEvC,MAAM2O,EAAalM,EAAOI,SAAS,GACnC,QAAI8L,EAAW9L,SAAS7C,OAAS,IACU,aAAvC2O,EAAW/L,QAAQe,eAGzB,CGqBMiL,CAAWnM,GAAS,CACtB,IAAIoM,EAAUpM,EAAOI,SAAS,GAC1BiM,EAAWJ,EACiB,IAA5BG,EAAQhM,SAAS7C,SACnB6O,EAAUA,EAAQhM,SAAS,GAC3BiM,KAEFT,EAAeC,EAAeO,EAASC,GAEvC,MAAMC,EAAYF,EAAQlM,KAAK6L,KAAK,KAC9B7I,EAAS2I,EAAcU,IAAID,GACjC,IAAKpJ,EAAQ,MAAM,IAAIpE,MAAM,sCAI7B,OAHIkN,GAAUQ,EAAetJ,EAAQ4I,GACrCD,EAAc/G,IAAI5E,EAAMgD,QACxB2I,EAAcY,OAAOH,EAEzB,CAEE,GH/BK,SAAmBtM,GACxB,IAAKA,EAAQ,OAAO,EACpB,GAAsC,QAAlCA,EAAOG,QAAQzC,eAA0B,OAAO,EACpD,GAAIsC,EAAOI,SAAS7C,OAAS,EAAG,OAAO,EAEvC,MAAM2O,EAAalM,EAAOI,SAAS,GACnC,GAAmC,IAA/B8L,EAAW9L,SAAS7C,OAAc,OAAO,EAC7C,GAA2C,aAAvC2O,EAAW/L,QAAQe,gBAAgC,OAAO,EAE9D,MAAMwL,EAAWR,EAAW9L,SAASU,MAAKN,GAAgC,QAAvBA,EAAML,QAAQM,OACjE,GAA0C,aAAtCiM,GAAUvM,QAAQe,gBAAgC,OAAO,EAE7D,MAAMyL,EAAaT,EAAW9L,SAASU,MAAKN,GAAgC,UAAvBA,EAAML,QAAQM,OACnE,MAA4C,aAAxCkM,GAAYxM,QAAQe,eAG1B,CGeM0L,CAAU5M,GAAS,CACrB,MAAM6M,EAAU7M,EAAOI,SAAS,GAAGD,QAAQM,KAG3CmL,EAAeC,EAAe7L,EAAOI,SAAS,GAAGA,SAAS,GAAI6L,EAAY,GAC1EL,EAAeC,EAAe7L,EAAOI,SAAS,GAAGA,SAAS,GAAI6L,EAAY,GAE1E,MAAMa,EAAOjB,EAAcU,IAAI,GAAGrM,KAAQ2M,SACpC3J,EAAS2I,EAAcU,IAAI,GAAGrM,KAAQ2M,WAE5C,IAAKC,EAAM,MAAM,IAAIhO,MAAM,mCAC3B,IAAKoE,EAAQ,MAAM,IAAIpE,MAAM,qCAC7B,GAAIgO,EAAKvP,SAAW2F,EAAO3F,OACzB,MAAM,IAAIuB,MAAM,gDAGlB,MAAMiO,EAAMC,EAAaF,EAAM5J,EAAQ+I,GAMvC,OALID,GAAUQ,EAAeO,EAAKjB,GAElCD,EAAcY,OAAO,GAAGvM,KAAQ2M,SAChChB,EAAcY,OAAO,GAAGvM,KAAQ2M,gBAChChB,EAAc/G,IAAI5E,EAAM6M,EAE5B,CAGE,GAAI/M,EAAOI,SAAS7C,OAAQ,CAE1B,MAAM0P,EAAiD,aAAnCjN,EAAOG,QAAQe,gBAAiC4K,EAAQA,EAAQ,EAE9EoB,EAAS,CAAA,EACf,IAAK,MAAM1M,KAASR,EAAOI,SAAU,CACnCwL,EAAeC,EAAerL,EAAOyM,GACrC,MAAME,EAAYtB,EAAcU,IAAI/L,EAAMN,KAAK6L,KAAK,MACpD,IAAKoB,EAAW,MAAM,IAAIrO,MAAM,qCAChCoO,EAAO1M,EAAML,QAAQM,MAAQ0M,CACnC,CAEI,IAAK,MAAM3M,KAASR,EAAOI,SACzByL,EAAcY,OAAOjM,EAAMN,KAAK6L,KAAK,MAGvC,MAAMqB,EAAWC,EAAaH,EAAQD,GAClCjB,GAAUQ,EAAeY,EAAUtB,GACvCD,EAAc/G,IAAI5E,EAAMkN,EAC5B,CACA,CAMA,SAASZ,EAAezO,EAAK+N,GAC3B,IAAK,IAAItO,EAAI,EAAGA,EAAIO,EAAIR,OAAQC,IAC1BsO,EACFU,EAAezO,EAAIP,GAAIsO,EAAQ,GAE/B/N,EAAIP,GAAKO,EAAIP,GAAG,EAGtB,CAQA,SAASwP,EAAaF,EAAM5J,EAAQ4I,GAClC,MAAMiB,EAAM,GACZ,IAAK,IAAIvP,EAAI,EAAGA,EAAIsP,EAAKvP,OAAQC,IAC/B,GAAIsO,EACFiB,EAAIrM,KAAKsM,EAAaF,EAAKtP,GAAI0F,EAAO1F,GAAIsO,EAAQ,SAElD,GAAIgB,EAAKtP,GAAI,CAEX,MAAM8P,EAAM,CAAA,EACZ,IAAK,IAAIC,EAAI,EAAGA,EAAIT,EAAKtP,GAAGD,OAAQgQ,IAAK,CACvC,MAAMlO,EAAQ6D,EAAO1F,GAAG+P,GACxBD,EAAIR,EAAKtP,GAAG+P,SAAgBjR,IAAV+C,EAAsB,KAAOA,CACzD,CACQ0N,EAAIrM,KAAK4M,EACjB,MACQP,EAAIrM,UAAKpE,GAIf,OAAOyQ,CACT,CASA,SAASM,EAAaH,EAAQpB,GAC5B,MAAMgB,EAAOU,OAAOV,KAAKI,GACnB3P,EAAS2P,EAAOJ,EAAK,KAAKvP,OAC1BwP,EAAM,GACZ,IAAK,IAAIvP,EAAI,EAAGA,EAAID,EAAQC,IAAK,CAE/B,MAAM8P,EAAM,CAAA,EACZ,IAAK,MAAM9D,KAAOsD,EAAM,CACtB,GAAII,EAAO1D,GAAKjM,SAAWA,EAAQ,MAAM,IAAIuB,MAAM,gCACnDwO,EAAI9D,GAAO0D,EAAO1D,GAAKhM,EAC7B,CACQsO,EACFiB,EAAIrM,KAAK2M,EAAaC,EAAKxB,EAAQ,IAEnCiB,EAAIrM,KAAK4M,EAEf,CACE,OAAOP,CACT,CChPO,SAASU,EAAkBlM,EAAQlB,EAAOjD,GAC/C,MAAMsQ,EAAQtQ,aAAkBuQ,WAC1BC,EAAY1L,EAAWX,GACvBsM,EAAoB3L,EAAWX,GACrCW,EAAWX,GACX,IAAIlC,EAAQ+C,EAAiBb,GACzBuM,EAAc,EAClB1Q,EAAO0Q,KAAiBJ,EAAQxP,OAAOmB,GAASA,EAEhD,MAAM0O,EAAqBH,EAAYC,EAEvC,KAAOC,EAAczN,GAAO,CAE1B,MAAM2N,EAAW5L,EAAiBb,GAC5B0M,EAAY,IAAI5Q,WAAWwQ,GACjC,IAAK,IAAIrQ,EAAI,EAAGA,EAAIqQ,EAAmBrQ,IACrCyQ,EAAUzQ,GAAK+D,EAAOG,KAAKmB,SAAStB,EAAOE,UAG7C,IAAK,IAAIjE,EAAI,EAAGA,EAAIqQ,GAAqBC,EAAczN,EAAO7C,IAAK,CAEjE,MAAMqM,EAAW7K,OAAOiP,EAAUzQ,IAClC,GAAIqM,EAAU,CACZ,IAAIqE,EAAa,GACbC,EAAiBJ,EACrB,MAAMK,GAAQ,IAAMvE,GAAY,GAChC,KAAOsE,GAAkBL,EAAczN,GAAO,CAC5C,IAAIgO,EAAOrP,OAAOuC,EAAOG,KAAKmB,SAAStB,EAAOE,UAAYyM,EAAaE,EAEvE,IADAF,GAAcrE,EACPqE,GAAc,GACnBA,GAAc,GACd3M,EAAOE,SACHyM,IACFG,GAAQrP,OAAOuC,EAAOG,KAAKmB,SAAStB,EAAOE,UAAYoI,EAAWqE,EAAaE,GAInF/O,GADc2O,EAAWK,EAEzBjR,EAAO0Q,KAAiBJ,EAAQxP,OAAOmB,GAASA,EAChD8O,GACV,CACYA,IAEF5M,EAAOE,QAAU5D,KAAKyQ,MAAMH,EAAiBjQ,OAAO2L,GAAY3L,OAAOgQ,IAAe,GAEhG,MACQ,IAAK,IAAIX,EAAI,EAAGA,EAAIQ,GAAsBD,EAAczN,EAAOkN,IAC7DlO,GAAS2O,EACT5Q,EAAO0Q,KAAiBJ,EAAQxP,OAAOmB,GAASA,CAG1D,CACA,CACA,CCrDO,SAASwK,EAASxK,GACvB,OAAO,GAAKxB,KAAK0Q,MAAMlP,EACzB,CAYO,SAASmP,EAAuBjN,EAAQkN,EAAOlR,EAAQH,GACvDG,IAEHgE,EAAOE,QAAU,GAEnB,IAAIiN,EAAO,EACX,KAAOA,EAAOtR,EAAOG,QAAQ,CAC3B,MAAMoR,EAASzM,EAAWX,GAC1B,GAAa,EAAToN,EAEFD,EAAOE,EAAcrN,EAAQoN,EAAQF,EAAOrR,EAAQsR,OAC/C,CAEL,MAAMrO,EAAQsO,IAAW,EACzBE,EAAQtN,EAAQlB,EAAOoO,EAAOrR,EAAQsR,GACtCA,GAAQrO,CACd,CACA,CAEA,CAWA,SAASwO,EAAQtN,EAAQlB,EAAOwJ,EAAUzM,EAAQsR,GAChD,MAAMD,EAAQ5E,EAAW,GAAK,EAC9B,IAAIxK,EAAQ,EACZ,IAAK,IAAI7B,EAAI,EAAGA,EAAIiR,EAAOjR,IACzB6B,GAASkC,EAAOG,KAAKmB,SAAStB,EAAOE,YAAcjE,GAAK,GAK1D,IAAK,IAAIA,EAAI,EAAGA,EAAI6C,EAAO7C,IACzBJ,EAAOsR,EAAOlR,GAAK6B,CAEvB,CAaA,SAASuP,EAAcrN,EAAQoN,EAAQ9E,EAAUzM,EAAQsR,GACvD,IAAIrO,EAAQsO,GAAU,GAAK,EAC3B,MAAMP,GAAQ,GAAKvE,GAAY,EAE/B,IAAIhN,EAAO,EACX,GAAI0E,EAAOE,OAASF,EAAOG,KAAKC,WAC9B9E,EAAO0E,EAAOG,KAAKmB,SAAStB,EAAOE,eAC9B,GAAI2M,EAET,MAAM,IAAItP,MAAM,0BAA0ByC,EAAOE,uBAEnD,IAAIqN,EAAO,EACPC,EAAQ,EAGZ,KAAO1O,GAED0O,EAAQ,GACVA,GAAS,EACTD,GAAQ,EACRjS,KAAU,GACDiS,EAAOC,EAAQlF,GAExBhN,GAAQ0E,EAAOG,KAAKmB,SAAStB,EAAOE,SAAWqN,EAC/CvN,EAAOE,SACPqN,GAAQ,IAEJJ,EAAOtR,EAAOG,SAEhBH,EAAOsR,KAAU7R,GAAQkS,EAAQX,GAEnC/N,IACA0O,GAASlF,GAIb,OAAO6E,CACT,CASO,SAASM,EAAgBzN,EAAQlB,EAAOlC,EAAM8Q,GACnD,MAAMR,EA6BR,SAAmBtQ,EAAM8Q,GACvB,OAAQ9Q,GACR,IAAK,QACL,IAAK,QACH,OAAO,EACT,IAAK,QACL,IAAK,SACH,OAAO,EACT,IAAK,uBACH,IAAK8Q,EAAY,MAAM,IAAInQ,MAAM,yCACjC,OAAOmQ,EACT,QACE,MAAM,IAAInQ,MAAM,6BAA6BX,KAEjD,CA3CgB+Q,CAAU/Q,EAAM8Q,GACxB7P,EAAQ,IAAI/B,WAAWgD,EAAQoO,GACrC,IAAK,IAAIU,EAAI,EAAGA,EAAIV,EAAOU,IACzB,IAAK,IAAI3R,EAAI,EAAGA,EAAI6C,EAAO7C,IACzB4B,EAAM5B,EAAIiR,EAAQU,GAAK5N,EAAOG,KAAKmB,SAAStB,EAAOE,UAIvD,GAAa,UAATtD,EAAkB,OAAO,IAAIiR,aAAahQ,EAAMoD,QAC/C,GAAa,WAATrE,EAAmB,OAAO,IAAIkR,aAAajQ,EAAMoD,QACrD,GAAa,UAATrE,EAAkB,OAAO,IAAIwP,WAAWvO,EAAMoD,QAClD,GAAa,UAATrE,EAAkB,OAAO,IAAImR,cAAclQ,EAAMoD,QACrD,GAAa,yBAATrE,EAAiC,CAExC,MAAMoR,EAAQ,IAAIvR,MAAMqC,GACxB,IAAK,IAAI7C,EAAI,EAAGA,EAAI6C,EAAO7C,IACzB+R,EAAM/R,GAAK4B,EAAMoQ,SAAShS,EAAIiR,GAAQjR,EAAI,GAAKiR,GAEjD,OAAOc,CACX,CACE,MAAM,IAAIzQ,MAAM,+CAA+CX,IACjE,CCvIO,SAASsR,EAAUlO,EAAQpD,EAAMkC,EAAOqP,GAC7C,GAAc,IAAVrP,EAAa,MAAO,GACxB,GAAa,YAATlC,EACF,OA4BJ,SAA0BoD,EAAQlB,GAChC,MAAM6C,EAAS,IAAIlF,MAAMqC,GACzB,IAAK,IAAI7C,EAAI,EAAGA,EAAI6C,EAAO7C,IAAK,CAC9B,MAAMiF,EAAalB,EAAOE,QAAUjE,EAAI,EAAI,GACtCmS,EAAYnS,EAAI,EAChB8B,EAAOiC,EAAOG,KAAKmB,SAASJ,GAClCS,EAAO1F,MAAM8B,EAAO,GAAKqQ,EAC7B,CAEE,OADApO,EAAOE,QAAU5D,KAAKyQ,KAAKjO,EAAQ,GAC5B6C,CACT,CAtCW0M,CAAiBrO,EAAQlB,GAC3B,GAAa,UAATlC,EACT,OA6CJ,SAAwBoD,EAAQlB,GAC9B,MAAM6C,GAAU3B,EAAOG,KAAKe,WAAalB,EAAOE,QAAU,EACtD,IAAIkM,WAAWkC,EAAMtO,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAgB,EAARpB,IACjF,IAAIsN,WAAWpM,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQpB,GAE/E,OADAkB,EAAOE,QAAkB,EAARpB,EACV6C,CACT,CAnDW4M,CAAevO,EAAQlB,GACzB,GAAa,UAATlC,EACT,OA0DJ,SAAwBoD,EAAQlB,GAC9B,MAAM6C,GAAU3B,EAAOG,KAAKe,WAAalB,EAAOE,QAAU,EACtD,IAAI6N,cAAcO,EAAMtO,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAgB,EAARpB,IACpF,IAAIiP,cAAc/N,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQpB,GAElF,OADAkB,EAAOE,QAAkB,EAARpB,EACV6C,CACT,CAhEW6M,CAAexO,EAAQlB,GACzB,GAAa,UAATlC,EACT,OAuEJ,SAAwBoD,EAAQlB,GAC9B,MAAM6C,EAAS,IAAIlF,MAAMqC,GACzB,IAAK,IAAI7C,EAAI,EAAGA,EAAI6C,EAAO7C,IAAK,CAC9B,MAAMwS,EAAMzO,EAAOG,KAAKgJ,YAAYnJ,EAAOE,OAAa,GAAJjE,GAAQ,GACtDyS,EAAO1O,EAAOG,KAAK+I,SAASlJ,EAAOE,OAAa,GAAJjE,EAAS,GAAG,GAC9D0F,EAAO1F,GAAKwB,OAAOiR,IAAS,IAAMD,CACtC,CAEE,OADAzO,EAAOE,QAAkB,GAARpB,EACV6C,CACT,CAhFWgN,CAAe3O,EAAQlB,GACzB,GAAa,UAATlC,EACT,OAuFJ,SAAwBoD,EAAQlB,GAC9B,MAAM6C,GAAU3B,EAAOG,KAAKe,WAAalB,EAAOE,QAAU,EACtD,IAAI2N,aAAaS,EAAMtO,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAgB,EAARpB,IACnF,IAAI+O,aAAa7N,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQpB,GAEjF,OADAkB,EAAOE,QAAkB,EAARpB,EACV6C,CACT,CA7FWiN,CAAe5O,EAAQlB,GACzB,GAAa,WAATlC,EACT,OAoGJ,SAAyBoD,EAAQlB,GAC/B,MAAM6C,GAAU3B,EAAOG,KAAKe,WAAalB,EAAOE,QAAU,EACtD,IAAI4N,aAAaQ,EAAMtO,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAgB,EAARpB,IACnF,IAAIgP,aAAa9N,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQpB,GAEjF,OADAkB,EAAOE,QAAkB,EAARpB,EACV6C,CACT,CA1GWkN,CAAgB7O,EAAQlB,GAC1B,GAAa,eAATlC,EACT,OAiHJ,SAA4BoD,EAAQlB,GAClC,MAAM6C,EAAS,IAAIlF,MAAMqC,GACzB,IAAK,IAAI7C,EAAI,EAAGA,EAAI6C,EAAO7C,IAAK,CAC9B,MAAMD,EAASgE,EAAOG,KAAK+I,SAASlJ,EAAOE,QAAQ,GACnDF,EAAOE,QAAU,EACjByB,EAAO1F,GAAK,IAAIH,WAAWkE,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQlE,GACvFgE,EAAOE,QAAUlE,CACrB,CACE,OAAO2F,CACT,CA1HWmN,CAAmB9O,EAAQlB,GAC7B,GAAa,yBAATlC,EAAiC,CAC1C,IAAKuR,EAAa,MAAM,IAAI5Q,MAAM,gCAClC,OAiIJ,SAAiCyC,EAAQlB,EAAOqP,GAE9C,MAAMxM,EAAS,IAAIlF,MAAMqC,GACzB,IAAK,IAAI7C,EAAI,EAAGA,EAAI6C,EAAO7C,IACzB0F,EAAO1F,GAAK,IAAIH,WAAWkE,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQiO,GACvFnO,EAAOE,QAAUiO,EAEnB,OAAOxM,CACT,CAzIWoN,CAAwB/O,EAAQlB,EAAOqP,EAClD,CACI,MAAM,IAAI5Q,MAAM,2BAA2BX,IAE/C,CA+IA,SAAS0R,EAAMrN,EAAQf,EAAQqB,GAC7B,MAAMyN,EAAU,IAAI3L,YAAY9B,GAEhC,OADA,IAAIzF,WAAWkT,GAASzL,IAAI,IAAIzH,WAAWmF,EAAQf,EAAQqB,IACpDyN,CACT,CC7KA,MAAMC,EAAY,CAAC,EAAG,IAAM,MAAQ,SAAU,YAY9C,SAASC,GAAUC,EAAWC,EAASC,EAASC,EAAOtT,GACrD,IAAK,IAAIC,EAAI,EAAGA,EAAID,EAAQC,IAC1BoT,EAAQC,EAAQrT,GAAKkT,EAAUC,EAAUnT,EAE7C,CAWA,SAASsT,GAAcC,EAAOC,EAAKvP,EAAQlE,GACzC,IAAK,IAAIC,EAAI,EAAGA,EAAID,EAAQC,IAC1BuT,EAAMC,EAAMxT,GAAKuT,EAAMC,EAAMvP,EAASjE,EAE1C,CCtBO,SAASyT,GAAa7R,EAAO8R,EAAMlQ,GAAY7C,KAAEA,IACtD,MAAMuD,EAAO,IAAI4C,SAASlF,EAAMoD,OAAQpD,EAAMqD,WAAYrD,EAAMuC,YAC1DJ,EAAS,CAAEG,OAAMD,OAAQ,GAE/B,IAAI0P,EAGJ,MAAMtG,EAoDR,SAA8BtJ,EAAQ2P,EAAMlQ,GAC1C,GAAIA,EAAWzD,OAAS,EAAG,CACzB,MAAM6T,EAAqBrQ,EAAsBC,GACjD,GAAIoQ,EAAoB,CACtB,MAAMlO,EAAS,IAAIlF,MAAMkT,EAAKhK,YAE9B,OADAsH,EAAuBjN,EAAQsI,EAASuH,GAAqB,EAAGlO,GACzDA,CACb,CACA,CACE,MAAO,EACT,CA9D2BmO,CAAqB9P,EAAQ2P,EAAMlQ,IAEtD4J,iBAAEA,EAAgB0G,SAAEA,GAoE5B,SAA8B/P,EAAQ2P,EAAMlQ,GAC1C,MAAM+J,EAAqB5J,EAAsBH,GACjD,IAAK+J,EAAoB,MAAO,CAAEH,iBAAkB,GAAI0G,SAAU,GAElE,MAAM1G,EAAmB,IAAI5M,MAAMkT,EAAKhK,YACxCsH,EAAuBjN,EAAQsI,EAASkB,GAAqB,EAAGH,GAGhE,IAAI0G,EAAWJ,EAAKhK,WACpB,IAAK,MAAMsE,KAAOZ,EACZY,IAAQT,GAAoBuG,IAEjB,IAAbA,IAAgB1G,EAAiBrN,OAAS,GAE9C,MAAO,CAAEqN,mBAAkB0G,WAC7B,CAnFyCC,CAAqBhQ,EAAQ2P,EAAMlQ,GAIpEwQ,EAAUN,EAAKhK,WAAaoK,EAClC,GAAsB,UAAlBJ,EAAKlU,SAAsB,CAC7B,MAAMuI,YAAEA,GAAgBvE,EAAWA,EAAWzD,OAAS,GAAG4C,QAC1DgR,EAAW1B,EAAUlO,EAAQpD,EAAMqT,EAASjM,EAChD,MAAS,GACa,qBAAlB2L,EAAKlU,UACa,mBAAlBkU,EAAKlU,UACa,QAAlBkU,EAAKlU,SACL,CACA,MAAM6M,EAAoB,YAAT1L,EAAqB,EAAIuD,EAAKmB,SAAStB,EAAOE,UAC3DoI,GACFsH,EAAW,IAAInT,MAAMwT,GACrBhD,EAAuBjN,EAAQsI,EAAUnI,EAAKC,WAAaJ,EAAOE,OAAQ0P,IAE1EA,EAAW,IAAI9T,WAAWmU,EAEhC,KAAS,IAAsB,sBAAlBN,EAAKlU,SAId,MAAM,IAAI8B,MAAM,iCAAiCoS,EAAKlU,YAJN,CAChD,MAAMuI,YAAEA,GAAgBvE,EAAWA,EAAWzD,OAAS,GAAG4C,QAC1DgR,EAAWnC,EAAgBzN,EAAQiQ,EAASrT,EAAMoH,EACtD,CAEA,CAEE,MAAO,CAAEqF,mBAAkBC,mBAAkBsG,WAC/C,CASO,SAASM,GAAmBrS,EAAOsS,EAAMC,EAAgB1C,GAG9D,OAAOQ,EADQ,CAAE/N,KADJ,IAAI4C,SAASlF,EAAMoD,OAAQpD,EAAMqD,WAAYrD,EAAMuC,YACzCF,OAAQ,GACNkQ,EAAexT,KAAMuT,EAAKxK,WAAY+H,EACjE,CAmDO,SAAS2C,GAAeC,EAAiBC,EAAwB7K,EAAO8K,GAE7E,IAAIC,EACJ,MAAMC,EAAqBF,IAAc9K,GACzC,GAAc,iBAAVA,EACF+K,EAAOH,OACF,GAAII,EACTD,EAAOC,EAAmBJ,EAAiBC,OACtC,IAAc,WAAV7K,EAIT,MAAM,IAAInI,MAAM,0CAA0CmI,KAH1D+K,EAAO,IAAI3U,WAAWyU,GD9EnB,SAA0BI,EAAO9U,GACtC,MAAM+U,EAAcD,EAAMvQ,WACpByQ,EAAehV,EAAOuE,WAC5B,IAAIqP,EAAM,EACNqB,EAAS,EAGb,KAAOrB,EAAMmB,GAAa,CACxB,MAAMG,EAAIJ,EAAMlB,GAEhB,GADAA,IACIsB,EAAI,IACN,KAEN,CACE,GAAIF,GAAgBpB,GAAOmB,EACzB,MAAM,IAAIrT,MAAM,gCAGlB,KAAOkS,EAAMmB,GAAa,CACxB,MAAMG,EAAIJ,EAAMlB,GAChB,IAAIuB,EAAM,EAGV,GAFAvB,IAEIA,GAAOmB,EACT,MAAM,IAAIrT,MAAM,sBAIlB,GAAS,EAAJwT,EAsBE,CAEL,IAAI7Q,EAAS,EACb,OAAY,EAAJ6Q,GACR,KAAK,EAEHC,EAAwB,GAAjBD,IAAM,EAAI,GACjB7Q,EAASyQ,EAAMlB,IAAQsB,IAAM,GAAK,GAClCtB,IACA,MACF,KAAK,EAEH,GAAImB,GAAenB,EAAM,EACvB,MAAM,IAAIlS,MAAM,6BAElByT,EAAkB,GAAXD,IAAM,GACb7Q,EAASyQ,EAAMlB,IAAQkB,EAAMlB,EAAM,IAAM,GACzCA,GAAO,EACP,MACF,KAAK,EAEH,GAAImB,GAAenB,EAAM,EACvB,MAAM,IAAIlS,MAAM,6BAElByT,EAAkB,GAAXD,IAAM,GACb7Q,EAASyQ,EAAMlB,IACVkB,EAAMlB,EAAM,IAAM,IAClBkB,EAAMlB,EAAM,IAAM,KAClBkB,EAAMlB,EAAM,IAAM,IACvBA,GAAO,EAKT,GAAe,IAAXvP,GAAgB+Q,MAAM/Q,GACxB,MAAM,IAAI3C,MAAM,kBAAkB2C,SAAcuP,iBAAmBmB,KAErE,GAAI1Q,EAAS4Q,EACX,MAAM,IAAIvT,MAAM,2CAElBgS,GAAc1T,EAAQiV,EAAQ5Q,EAAQ8Q,GACtCF,GAAUE,CAChB,KAhEyB,CAEnB,IAAIA,EAAkB,GAAXD,IAAM,GAEjB,GAAIC,EAAM,GAAI,CACZ,GAAIvB,EAAM,GAAKmB,EACb,MAAM,IAAIrT,MAAM,+CAElB,MAAM2T,EAAaF,EAAM,GACzBA,EAAML,EAAMlB,IACPkB,EAAMlB,EAAM,IAAM,IAClBkB,EAAMlB,EAAM,IAAM,KAClBkB,EAAMlB,EAAM,IAAM,IACvBuB,EAAsC,GAA/BA,EAAM/B,EAAUiC,IACvBzB,GAAOyB,CACf,CACM,GAAIzB,EAAMuB,EAAMJ,EACd,MAAM,IAAIrT,MAAM,6CAElB2R,GAAUyB,EAAOlB,EAAK5T,EAAQiV,EAAQE,GACtCvB,GAAOuB,EACPF,GAAUE,CAChB,CA2CA,CAEE,GAAIF,IAAWD,EAAc,MAAM,IAAItT,MAAM,yBAC/C,CCjBI4T,CAAiBb,EAAiBG,EAGtC,CACE,GAAIA,GAAMzU,SAAWuU,EACnB,MAAM,IAAIhT,MAAM,oCAAoCkT,GAAMzU,gCAAgCuU,KAE5F,OAAOE,CACT,CAaO,SAASW,GAAed,EAAiBe,EAAI5R,EAAY2Q,EAAgBI,GAC9E,MACMxQ,EAAS,CAAEG,KADJ,IAAI4C,SAASuN,EAAgBrP,OAAQqP,EAAgBpP,WAAYoP,EAAgBlQ,YACvEF,OAAQ,IACzBwF,MAAEA,EAAK9I,KAAEA,GAASwT,EAClBkB,EAAQD,EAAGE,oBACjB,IAAKD,EAAO,MAAM,IAAI/T,MAAM,4CAG5B,MAAM+L,EA6DR,SAAgCtJ,EAAQsR,EAAO7R,GAC7C,MAAMoQ,EAAqBrQ,EAAsBC,GACjD,IAAKoQ,EAAoB,MAAO,GAEhC,MAAMlO,EAAS,IAAIlF,MAAM6U,EAAM3L,YAI/B,OAHAsH,EACEjN,EAAQsI,EAASuH,GAAqByB,EAAME,8BAA+B7P,GAEtEA,CACT,CAtE2B8P,CAAuBzR,EAAQsR,EAAO7R,GAC/DO,EAAOE,OAASoR,EAAME,8BAGtB,MAAMnI,EA0ER,SAAgCrJ,EAAQsR,EAAO7R,GAC7C,MAAM+J,EAAqB5J,EAAsBH,GACjD,GAAI+J,EAAoB,CAEtB,MAAM7H,EAAS,IAAIlF,MAAM6U,EAAM3L,YAE/B,OADAsH,EAAuBjN,EAAQsI,EAASkB,GAAqB8H,EAAMI,8BAA+B/P,GAC3FA,CACX,CACA,CAlF2BgQ,CAAuB3R,EAAQsR,EAAO7R,GAGzDmS,EAAuBP,EAAGd,uBAAyBe,EAAMI,8BAAgCJ,EAAME,8BAErG,IAAIf,EAAOH,EAAgBrC,SAASjO,EAAOE,SACf,IAAxBoR,EAAMO,gBACRpB,EAAOJ,GAAeI,EAAMmB,EAAsBlM,EAAO8K,IAE3D,MAAMsB,EAAW,IAAI/O,SAAS0N,EAAKxP,OAAQwP,EAAKvP,WAAYuP,EAAKrQ,YAC3D2R,EAAa,CAAE5R,KAAM2R,EAAU5R,OAAQ,GAI7C,IAAI0P,EACJ,MAAMK,EAAUqB,EAAM3L,WAAa2L,EAAMU,UACzC,GAAuB,UAAnBV,EAAM7V,SAAsB,CAC9B,MAAMuI,YAAEA,GAAgBvE,EAAWA,EAAWzD,OAAS,GAAG4C,QAC1DgR,EAAW1B,EAAU6D,EAAYnV,EAAMqT,EAASjM,EACpD,MAAS,GAAuB,QAAnBsN,EAAM7V,SAEfmU,EAAW,IAAInT,MAAMwT,GACrBhD,EAAuB8E,EAAY,EAAG,EAAGnC,GACzCA,EAAWA,EAAS9S,KAAImV,KAAOA,SAC1B,GACc,qBAAnBX,EAAM7V,UACa,mBAAnB6V,EAAM7V,SACN,CACA,MAAM6M,EAAWwJ,EAASxQ,SAASyQ,EAAW7R,UAC9C0P,EAAW,IAAInT,MAAMwT,GACrBhD,EAAuB8E,EAAYzJ,EAAUsJ,EAAuB,EAAGhC,EAC3E,MAAS,GAAuB,wBAAnB0B,EAAM7V,SAAoC,CAEnDmU,EADuB,UAAThT,EACK,IAAIwP,WAAW6D,GAAW,IAAIlC,cAAckC,GAC/D/D,EAAkB6F,EAAY9B,EAASL,EAC3C,MAAS,GAAuB,4BAAnB0B,EAAM7V,SACfmU,EAAW,IAAInT,MAAMwT,GJ/HlB,SAA8BjQ,EAAQlB,EAAOjD,GAClD,MAAMqW,EAAU,IAAI9F,WAAWtN,GAC/BoN,EAAkBlM,EAAQlB,EAAOoT,GACjC,IAAK,IAAIjW,EAAI,EAAGA,EAAI6C,EAAO7C,IACzBJ,EAAOI,GAAK,IAAIH,WAAWkE,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQgS,EAAQjW,IAC/F+D,EAAOE,QAAUgS,EAAQjW,EAE7B,CIyHIkW,CAAqBJ,EAAY9B,EAASL,QACrC,GAAuB,qBAAnB0B,EAAM7V,SACfmU,EAAW,IAAInT,MAAMwT,GJpHlB,SAAwBjQ,EAAQlB,EAAOjD,GAC5C,MAAMuW,EAAa,IAAIhG,WAAWtN,GAClCoN,EAAkBlM,EAAQlB,EAAOsT,GACjC,MAAMC,EAAa,IAAIjG,WAAWtN,GAClCoN,EAAkBlM,EAAQlB,EAAOuT,GAEjC,IAAK,IAAIpW,EAAI,EAAGA,EAAI6C,EAAO7C,IAAK,CAC9B,MAAMqW,EAAS,IAAIxW,WAAWkE,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQmS,EAAWpW,IACjGmW,EAAWnW,IAEbJ,EAAOI,GAAK,IAAIH,WAAWsW,EAAWnW,GAAKoW,EAAWpW,IACtDJ,EAAOI,GAAGsH,IAAI1H,EAAOI,EAAI,GAAGgS,SAAS,EAAGmE,EAAWnW,KACnDJ,EAAOI,GAAGsH,IAAI+O,EAAQF,EAAWnW,KAEjCJ,EAAOI,GAAKqW,EAEdtS,EAAOE,QAAUmS,EAAWpW,EAChC,CACA,CImGIsW,CAAeR,EAAY9B,EAASL,OAC/B,IAAuB,sBAAnB0B,EAAM7V,SAIf,MAAM,IAAI8B,MAAM,iCAAiC+T,EAAM7V,YAJN,CACjD,MAAMuI,YAAEA,GAAgBvE,EAAWA,EAAWzD,OAAS,GAAG4C,QAC1DgR,EAAWnC,EAAgBzN,EAAQiQ,EAASrT,EAAMoH,EACtD,CAEA,CAEE,MAAO,CAAEqF,mBAAkBC,mBAAkBsG,WAC/C,CCxMO,SAAS4C,GAAOzG,GACrB,QAAYhR,IAARgR,EAAmB,OAAO,KAC9B,GAAmB,iBAARA,EAAkB,OAAOpP,OAAOoP,GAC3C,GAAItP,MAAMgW,QAAQ1G,GAAM,OAAOA,EAAIjP,IAAI0V,IACvC,GAAIzG,aAAejQ,WAAY,OAAOW,MAAMI,KAAKkP,GACjD,GAAIA,aAAe/O,KAAM,OAAO+O,EAAI2G,cACpC,GAAI3G,aAAeE,OAAQ,CAEzB,MAAM0G,EAAS,CAAA,EACf,IAAK,MAAM1K,KAAOgE,OAAOV,KAAKQ,QACXhR,IAAbgR,EAAI9D,KACR0K,EAAO1K,GAAOuK,GAAOzG,EAAI9D,KAE3B,OAAO0K,CACX,CACE,OAAO5G,CACT,CAQO,SAAS6G,GAAOC,EAAKC,GAE1B,IAAK,IAAI7W,EAAI,EAAGA,EAAI6W,EAAI9W,OAAQC,GADlB,IAEZ4W,EAAI1T,QAAQ2T,EAAIjT,MAAM5D,EAAGA,EAFb,KAIhB,CA+BOsG,eAAewQ,IAAmBC,IAAEA,EAAG5S,WAAEA,EAAU6S,YAAEA,IAE1D7S,UAvBKmC,eAAiCyQ,EAAKC,GAC3C,aAAaC,MAAMF,EAAK,IAAKC,EAAaE,OAAQ,SAC/CC,MAAKC,IACJ,IAAKA,EAAIC,GAAI,MAAM,IAAI/V,MAAM,qBAAqB8V,EAAIE,UACtD,MAAMvX,EAASqX,EAAIG,QAAQxI,IAAI,kBAC/B,IAAKhP,EAAQ,MAAM,IAAIuB,MAAM,0BAC7B,OAAOkW,SAASzX,EAAM,GAE5B,CAeuB0X,CAAkBV,EAAKC,GAC5C,MAAMU,EAAOV,GAAe,CAAA,EAC5B,MAAO,CACL7S,aACA,WAAMP,CAAM+T,EAAOC,GAEjB,MAAML,EAAU,IAAIM,QAAQH,EAAKH,SAC3BO,OAAiBhZ,IAAR8Y,EAAoB,GAAKA,EAAM,EAC9CL,EAAQjQ,IAAI,QAAS,SAASqQ,KAASG,KACvC,MAAMV,QAAYH,MAAMF,EAAK,IAAKW,EAAMH,YACxC,IAAKH,EAAIC,KAAOD,EAAIW,KAAM,MAAM,IAAIzW,MAAM,gBAAgB8V,EAAIE,UAC9D,OAAOF,EAAI5P,aACZ,EAEL,CClEO,SAASwQ,GAAWjU,EAAQkU,EAAU9D,EAAgB3Q,GAAY+Q,YAAEA,EAAW9U,KAAEA,IACtF,MAAMkD,QAAEA,GAAYa,EAAWA,EAAWzD,OAAS,GAEnD,IAAIT,EAEJ,MAAM4Y,EAAU,GAEhB,KAAOA,EAAQnY,OAASkY,GAAU,CAEhC,MAAM9G,EAASgH,GAAcpU,GAIvBsQ,EAAkB,IAAIxU,WAC1BkE,EAAOG,KAAKc,OAAQjB,EAAOG,KAAKe,WAAalB,EAAOE,OAAQkN,EAAOiH,sBAKrE,IAAI1S,EACJ,GAAoB,cAAhByL,EAAOxQ,KAAsB,CAC/B,MAAM+S,EAAOvC,EAAOkH,iBACpB,IAAK3E,EAAM,MAAM,IAAIpS,MAAM,yCAE3B,MAAMkT,EAAOJ,GAAeC,EAAiB3T,OAAOyQ,EAAOmD,wBAAyBH,EAAe1K,MAAO8K,IACpGnH,iBAAEA,EAAgBC,iBAAEA,EAAgBsG,SAAEA,GAAaF,GAAae,EAAMd,EAAMlQ,EAAY2Q,GAK9F,GADAzO,EAAStG,EAAsBuU,EAAUrU,EAAYqD,EAAS+Q,EAAKlU,SAAUC,GACzE4N,EAAiBtN,QAAUqN,GAAkBrN,OAAQ,CACvD,MAAMwN,EAAqB5J,EAAsBH,GAC3C8J,EAAiB9J,EAAW3C,KAAI,EAAG8B,aAAcA,EAAQe,kBAC/DyJ,EACE+K,EAAS9K,EAAkBC,EAAkB3H,EAAQ4H,EAAgBC,EAE/E,KAAa,CAEL,IAAK,IAAIvN,EAAI,EAAGA,EAAIwD,EAAWzD,OAAQC,IACS,aAA1CwD,EAAWxD,GAAG2C,QAAQe,kBACxBgC,EAASlF,MAAMI,KAAK8E,GAAQmD,GAAK,CAACA,MAGtC8N,GAAOuB,EAASxS,EACxB,CACA,MAAW,GAAoB,iBAAhByL,EAAOxQ,KAAyB,CACzC,MAAM0U,EAAQlE,EAAOmE,oBACrB,IAAKD,EAAO,MAAM,IAAI/T,MAAM,4CAE5B,MAAM8L,iBAAEA,EAAgBC,iBAAEA,EAAgBsG,SAAEA,GAAawB,GACvDd,EAAiBlD,EAAQ3N,EAAY2Q,EAAgBI,GAKvD,GADA7O,EAAStG,EAAsBuU,EAAUrU,EAAYqD,EAAS0S,EAAM7V,SAAUC,GAC1E4N,EAAiBtN,QAAUqN,GAAkBrN,OAAQ,CACvD,MAAMwN,EAAqB5J,EAAsBH,GAC3C8J,EAAiB9J,EAAW3C,KAAI,EAAG8B,aAAcA,EAAQe,kBAC/DyJ,EACE+K,EAAS9K,EAAkBC,EAAkB3H,EAAQ4H,EAAgBC,EAE/E,MACQoJ,GAAOuB,EAASxS,EAExB,KAAW,IAAoB,oBAAhByL,EAAOxQ,KAShB,MAAM,IAAIW,MAAM,kCAAkC6P,EAAOxQ,QATb,CAC5C,MAAMuT,EAAO/C,EAAOmH,uBACpB,IAAKpE,EAAM,MAAM,IAAI5S,MAAM,+CAK3BhC,EAAa2U,GAHAG,GACXC,EAAiB3T,OAAOyQ,EAAOmD,wBAAyBH,EAAe1K,MAAO8K,GAE1CL,EAAMC,EAAgBxR,EAAQoF,YAC1E,CAEA,CACIhE,EAAOE,QAAUkN,EAAOiH,oBAC5B,CACE,GAAIF,EAAQnY,OAASkY,EACnB,MAAM,IAAI3W,MAAM,2BAA2B4W,EAAQnY,yCAAyCkY,MAK9F,OAHIC,EAAQnY,OAASkY,IACnBC,EAAQnY,OAASkY,GAEZC,CACT,CAQO,SAASK,IAAevO,uBAAEA,EAAsBF,iBAAEA,EAAgBF,sBAAEA,IACzE,IAAI4O,EAAexO,EAInB,QAHKwO,GAAgB1O,EAAmB0O,KACtCA,EAAe1O,GAEV,CAAC0O,EAAcA,EAAe5O,EACvC,CASA,SAASuO,GAAcpU,GACrB,MAAMoN,EAASrN,EAA4BC,GAsC3C,MAAO,CACLpD,KApCWzB,EAASiS,EAAOvJ,SAqC3B0M,uBApC6BnD,EAAOtJ,QAqCpCuQ,qBApC2BjH,EAAOnJ,QAqClCyQ,IApCUtH,EAAOlJ,QAqCjBoQ,iBApCuBlH,EAAOjJ,SAAW,CACzCwB,WAAYyH,EAAOjJ,QAAQN,QAC3BpI,SAAUX,EAASsS,EAAOjJ,QAAQL,SAClC6Q,0BAA2B7Z,EAASsS,EAAOjJ,QAAQF,SACnD2Q,0BAA2B9Z,EAASsS,EAAOjJ,QAAQD,SACnDiC,WAAYiH,EAAOjJ,QAAQA,SAAW,CACpCvB,IAAKwK,EAAOjJ,QAAQA,QAAQN,QAC5B6E,IAAK0E,EAAOjJ,QAAQA,QAAQL,QAC5B6E,WAAYyE,EAAOjJ,QAAQA,QAAQF,QACnC2E,eAAgBwE,EAAOjJ,QAAQA,QAAQD,QACvC2E,UAAWuE,EAAOjJ,QAAQA,QAAQA,QAClC2E,UAAWsE,EAAOjJ,QAAQA,QAAQC,UA0BpCyQ,kBAvBwBzH,EAAOhJ,QAwB/BmQ,uBAvB6BnH,EAAO/I,SAAW,CAC/CsB,WAAYyH,EAAO/I,QAAQR,QAC3BpI,SAAUX,EAASsS,EAAO/I,QAAQP,SAClCgR,UAAW1H,EAAO/I,QAAQJ,SAqB1BsN,oBAnB0BnE,EAAO7I,SAAW,CAC5CoB,WAAYyH,EAAO7I,QAAQV,QAC3BmO,UAAW5E,EAAO7I,QAAQT,QAC1BiB,SAAUqI,EAAO7I,QAAQN,QACzBxI,SAAUX,EAASsS,EAAO7I,QAAQL,SAClCwN,8BAA+BtE,EAAO7I,QAAQJ,QAC9CqN,8BAA+BpE,EAAO7I,QAAQH,QAC9CyN,mBAA0C9W,IAA3BqS,EAAO7I,QAAQF,SAA+B+I,EAAO7I,QAAQF,QAC5E8B,WAAYiH,EAAO7I,QAAQA,SAa/B,CC9GOhC,eAAewS,GAAaC,EAAS/P,EAAUgQ,EAAYf,GAChE,MAAMgB,KAAEA,EAAIvR,SAAEA,EAAQuB,QAAEA,GAAY8P,EACpC,IAAKrR,EAAU,MAAM,IAAIpG,MAAM,oCACdxC,IAAbmZ,GAA0BA,EAAWjP,EAASF,YAAUmP,EAAWvX,OAAOsI,EAASF,WAGvF,IAeIoQ,GAfCC,EAAgBC,GAAgB,CAACH,EAAK9U,WAAY,GAUvD,GATA6E,EAASC,QAAQoQ,SAAQ,EAAG/P,UAAW6K,MACrC,IAAKA,EAAgB,MAAM,IAAI7S,MAAM,wCAErC,GAAI2H,IAAYA,EAAQqQ,SAASnF,EAAe3K,eAAe,IAAK,OAEpE,MAAO+P,EAAiBC,GAAiBjB,GAAepE,GAAgBtT,IAAIH,QAC5EyY,EAAiB9Y,KAAKoM,IAAI0M,EAAgBI,GAC1CH,EAAe/Y,KAAKsG,IAAIyS,EAAcI,EAAa,IAEjDL,GAAkBC,GAAgBnQ,GAASlJ,OAE7C,MAAM,IAAIuB,MAAM,8BAA8B2H,EAAQsF,KAAK,SAIzD6K,EAAeD,GAAkB,GAAK,KAGxCD,QAAoBD,EAAKrV,MAAMuV,EAAgBC,IAGjD,MAAMK,EAAW,IAEX7W,SAAEA,GAAaO,EAAcuE,EAASlF,OAAQ,IAAI,GAClDkX,EAAiB,IAAIC,IAAI/W,EAAS/B,KAAImC,GAAS,CAACA,EAAML,QAAQM,KAAM2W,GAAc5W,OAClFqL,EAAgB,IAAIsL,IAE1B,IAAK,IAAIxQ,EAAc,EAAGA,EAAcH,EAASC,QAAQlJ,OAAQoJ,IAAe,CAC9E,MAAMgL,EAAiBnL,EAASC,QAAQE,GAAaG,UACrD,IAAK6K,EAAgB,MAAM,IAAI7S,MAAM,wCAGrC,MAAMuY,EAAa1F,EAAe3K,eAAe,GACjD,GAAIP,IAAYA,EAAQqQ,SAASO,GAAa,SAE9C,MAAON,EAAiBC,GAAiBjB,GAAepE,GAAgBtT,IAAIH,QACtEoZ,EAAcN,EAAgBD,EAIpC,GAAIO,EAAc,GAAK,GAAI,CACzBC,QAAQC,KAAK,iCAAiC7F,EAAe3K,mBAAmBsQ,EAAYG,0BAE5F,QACN,CAII,IAAIjV,EACAkV,EAAe,EACfhB,GACFlU,EAASmV,QAAQC,QAAQlB,GACzBgB,EAAeX,EAAkBJ,GAGjCnU,EAASmV,QAAQC,QAAQnB,EAAKrV,MAAM2V,EAAiBC,IAIvDC,EAASvW,KAAK8B,EAAOmS,MAAK3P,IACxB,MAAMhE,EAAaL,EAAcuE,EAASlF,OAAQ2R,EAAe3K,gBAGjE,IAAI6Q,EAAarC,GAFF,CAAE9T,KAAM,IAAI4C,SAASU,GAAcvD,OAAQiW,GAEtBjC,EAAU9D,EAAgB3Q,EAAYuV,GAK1E,MAAMjK,EAAYqF,EAAe3K,eAAe+E,KAAK,KACrDF,EAAc/G,IAAIwH,EAAWuL,GAC7BA,OAAavb,EAEb,MAAMwb,EAAaZ,EAAe3K,IAAI8K,GACtC,GAAIS,GAAYC,OAAMtX,GAAQoL,EAAcmM,IAAIvX,OAE9CmL,EAAeC,EAAe7K,EAAW,IACzC6W,EAAahM,EAAcU,IAAI8K,IAC1BQ,GACH,MAAM,IAAI/Y,MAAM,sCAAsCuY,KAKrDQ,GAELtB,EAAQ0B,UAAU,CAChBZ,aACAQ,aACAK,SAAU1B,EACV2B,OAAQ3B,EAAaqB,EAAWta,QACjC,IAEP,CAEE,SADMoa,QAAQS,IAAInB,GACdV,EAAQ8B,WAAY,CAEtB,MAAMC,EAAY,IAAIta,MAAMyX,GACtB8C,EAAsBnY,EACzB/B,KAAImC,GAASA,EAAML,QAAQM,OAC3B2F,QAAO3F,IAASgG,GAAWA,EAAQqQ,SAASrW,KACzC+X,EAAc/R,GAAW8R,EACzBE,EAAkBD,EACrBna,KAAIoC,GAAQ8X,EAAoBzB,SAASrW,GAAQoL,EAAcU,IAAI9L,QAAQnE,IAE9E,IAAK,IAAIoc,EAAM,EAAGA,EAAMjD,EAAUiD,IAChC,GAA0B,WAAtBnC,EAAQoC,UAAwB,CAGlC,MAAMjD,EAAU,CAAA,EAChB8C,EAAY3B,SAAQ,CAACpW,EAAMmY,KACzBlD,EAAQjV,GAAQgY,EAAgBG,KAASF,EAAG,IAE9CJ,EAAUI,GAAOhD,CACzB,MAEQ4C,EAAUI,GAAOD,EAAgBpa,KAAIqI,GAAUA,IAASgS,KAG5D,OAAOJ,CACX,CACE,MAAO,EACT,CAWA,SAASlB,GAAcpX,EAAQ5C,EAAS,IACtC,GAAI4C,EAAOI,SAAS7C,OAClB,IAAK,MAAMiD,KAASR,EAAOI,SACzBgX,GAAc5W,EAAOpD,QAGvBA,EAAOsD,KAAKV,EAAOE,KAAK6L,KAAK,MAE/B,OAAO3O,CACT,CCxMO0G,eAAe+U,GAAatC,GACjC,MAAME,KAAEA,EAAIyB,SAAEA,EAAQC,OAAEA,EAAMW,QAAEA,GAAYvC,EAK5C,GAJAA,EAAQrR,iBAAmBnB,EAAqB0S,GAIzB,iBAAZqC,EAAsB,CAE/B,MAAMC,QAAoBC,GAAmB,IAAKzC,EAAS2B,cAAU5b,EAAW6b,YAAQ7b,EAAWmK,QAAS,CAACqS,KAGvGG,EAAgBjb,MAAMI,KAAK2a,GAAa,CAACG,EAAGN,IAAUA,IACzDO,MAAK,CAACC,EAAGjK,IAuEhB,SAAiBiK,EAAGjK,GAClB,OAAIiK,EAAIjK,GAAW,EACD,CAEpB,CA3EsBkK,CAAQN,EAAYK,GAAGN,GAAUC,EAAY5J,GAAG2J,MAC/D1X,MAAM8W,EAAUC,GAEbmB,QAeVxV,eAA+ByS,GAC7B,MAAME,KAAEA,EAAI8C,KAAEA,GAAShD,EACvBA,EAAQrR,iBAAmBnB,EAAqB0S,GAChD,MAAQlQ,WAAYiT,GAAcjD,EAAQrR,SAEpCuU,EAAgBzb,MAAMwb,EAAUjc,QAAQmc,MAAK,GACnD,IAAIlD,EAAa,EACjB,MAAMmD,EAAYH,EAAUnb,KAAIub,GAASpD,GAActY,OAAO0b,EAAMtT,YACpE,IAAK,MAAMsS,KAASW,EAAM,CAExBE,EADmBE,EAAUE,WAAUzE,GAAOwD,EAAQxD,MAC1B,CAChC,CAGE,MAAM0E,EAAY,GAClB,IAAIC,EACJvD,EAAa,EACb,IAAK,IAAIhZ,EAAI,EAAGA,EAAIic,EAAclc,OAAQC,IAAK,CAC7C,MAAMwc,EAAWxD,EAAatY,OAAOsb,EAAUhc,GAAG8I,UAC9CmT,EAAcjc,QACGlB,IAAfyd,IACFA,EAAavD,QAGIla,IAAfyd,IACFD,EAAUpZ,KAAK,CAACqZ,EAAYC,IAC5BD,OAAazd,GAGjBka,EAAawD,CACjB,MACqB1d,IAAfyd,GACFD,EAAUpZ,KAAK,CAACqZ,EAAYvD,IAI9B,MAAM8C,EAAa,IAAItb,MAAME,OAAOqY,EAAQrR,SAASoB,WACrD,IAAK,MAAOyT,EAAYE,KAAaH,EAAW,CAE9C,MAAMxB,QAAkBU,GAAmB,IAAKzC,EAAS2B,SAAU6B,EAAY5B,OAAQ8B,IACvF,IAAK,IAAIzc,EAAIuc,EAAYvc,EAAIyc,EAAUzc,IACrC8b,EAAW9b,GAAK8a,EAAU9a,EAAIuc,GAC9BT,EAAW9b,GAAG0c,UAAY1c,CAEhC,CACE,OAAO8b,CACT,CA7D6Ba,CAAgB,IAAK5D,EAASgD,KAAMN,IAE7D,OADaA,EAAc5a,KAAIua,GAASU,EAAWV,IAEvD,CACI,aAAaI,GAAmBzC,EAEpC,CClBO,SAASyC,GAAmBzC,GACjC,OAAO,IAAIoB,SAAQ,CAACU,EAAY+B,MFI3BtW,eAA2ByS,GAChC,IAAKA,EAAQE,KAAM,MAAM,IAAI3X,MAAM,4BAInC,GADAyX,EAAQrR,iBAAmBnB,EAAqBwS,EAAQE,OACnDF,EAAQrR,SAAU,MAAM,IAAIpG,MAAM,8BAEvC,MAAMoG,SAAEA,EAAQmT,WAAEA,EAAUF,OAAEA,GAAW5B,EACnC2B,EAAW3B,EAAQ2B,UAAY,EAE/BxC,EAAU,GAGhB,IAAIc,EAAa,EACjB,IAAK,MAAMhQ,KAAYtB,EAASqB,WAAY,CAE1C,MAAM8T,EAAYnc,OAAOsI,EAASF,UAElC,GAAIkQ,EAAa6D,GAAanC,SAAwB5b,IAAX6b,GAAwB3B,EAAa2B,GAAS,CAEvF,MAAM1C,EAAW0C,GAAUA,EAAS3B,EAC9B8B,QAAkBhC,GAAaC,EAAS/P,EAAUgQ,EAAYf,GACpE,GAAI4C,EAAY,CAEd,MAAMlD,EAAQtX,KAAKsG,IAAI+T,EAAW1B,EAAY,GACxCpB,OAAiB9Y,IAAX6b,OAAuB7b,EAAY6b,EAAS3B,EACxDrC,GAAOuB,EAAS4C,EAAUlX,MAAM+T,EAAOC,GAC/C,CACA,CACIoB,GAAc6D,CAClB,CAEMhC,GAAYA,EAAW3C,EAC7B,EEpCI4E,CAAY,CACV3B,UAAW,YACRpC,EACH8B,eACCkC,MAAMH,EAAM,GAEnB,CC4DA,SAASI,GAAUC,GACjB,IAAIhZ,EAAS,EAGb,MAAMiZ,EAAYD,EAAIhZ,GAASA,GAAU,EACzC,MAAMkZ,EAAgC,IAAdD,EAQxB,SAASE,EAAWC,EAAKC,GAEvB,OADW,IAAIxW,SAASuW,EAAIrY,OAAQqY,EAAIpY,WAAYoY,EAAIlZ,YAC9C4C,UAAUuW,EAAKH,EAC7B,CAOE,SAASI,EAAWF,EAAKC,GAEvB,OADW,IAAIxW,SAASuW,EAAIrY,OAAQqY,EAAIpY,WAAYoY,EAAIlZ,YAC9CU,WAAWyY,EAAKH,EAC9B,CAGE,MAAMK,EAAeJ,EAAWH,EAAKhZ,GAMrC,GALAA,GAAU,EAKW,IAAjBuZ,EAAoB,CAEtB,MAAMC,EAAWL,EAAWH,EAAKhZ,GAASA,GAAU,EACpD,MAAMyZ,EAAS,GACf,IAAK,IAAIC,EAAI,EAAGA,EAAIF,EAAUE,IAAK,CACjC,MAAMC,EAAYR,EAAWH,EAAKhZ,GAASA,GAAU,EACrD,MAAM4Z,EAAO,GACb,IAAK,IAAIC,EAAI,EAAGA,EAAIF,EAAWE,IAAK,CAClC,MAAM9H,EAAIuH,EAAWN,EAAKhZ,GAASA,GAAU,EAC7C,MAAM8Z,EAAIR,EAAWN,EAAKhZ,GAASA,GAAU,EAC7C4Z,EAAK3a,KAAK,CAAC8S,EAAE+H,GACrB,CACML,EAAOxa,KAAK2a,EAClB,CACI,MAAO,CAAEld,KAAM,UAAWqd,YAAaN,EAE3C,CAAS,GAAqB,IAAjBF,EAAoB,CAE7B,MAAMS,EAAcb,EAAWH,EAAKhZ,GAASA,GAAU,EACvD,MAAMia,EAAW,GACjB,IAAK,IAAIle,EAAI,EAAGA,EAAIie,EAAaje,IAAK,CAEpC,MAAMme,EAAclB,EAAIhZ,GAASA,GAAU,EAC3C,MAAMma,EAAoC,IAAhBD,EACpBE,EAAS,WACb,MACMC,EADK,IAAIxX,SAASmW,EAAIjY,OAAQiY,EAAIhY,WAAYgY,EAAI9Y,YACzC4C,UAAU9C,EAAQma,GAEjC,OADAna,GAAU,EACHqa,CACR,CALc,GAOf,GAAe,IAAXD,EAAc,MAAM,IAAI/c,MAAM,yCAAyC+c,KAE3E,MAAMZ,EAAW,WACf,MACMa,EADK,IAAIxX,SAASmW,EAAIjY,OAAQiY,EAAIhY,WAAYgY,EAAI9Y,YACzC4C,UAAU9C,EAAQma,GAEjC,OADAna,GAAU,EACHqa,CACR,CALgB,GAOXC,EAAW,GACjB,IAAK,IAAIZ,EAAI,EAAGA,EAAIF,EAAUE,IAAK,CACjC,MAAMC,EAAY,WAChB,MACMU,EADK,IAAIxX,SAASmW,EAAIjY,OAAQiY,EAAIhY,WAAYgY,EAAI9Y,YACzC4C,UAAU9C,EAAQma,GAEjC,OADAna,GAAU,EACHqa,CACR,CALiB,GAMZT,EAAO,GACb,IAAK,IAAIC,EAAI,EAAGA,EAAIF,EAAWE,IAAK,CAClC,MAAMU,EAAK,IAAI1X,SAASmW,EAAIjY,OAAQiY,EAAIhY,WAAYgY,EAAI9Y,YAClD6R,EAAIwI,EAAG3Z,WAAWZ,EAAQma,GAAmBna,GAAU,EAC7D,MAAM8Z,EAAIS,EAAG3Z,WAAWZ,EAAQma,GAAmBna,GAAU,EAC7D4Z,EAAK3a,KAAK,CAAC8S,EAAE+H,GACvB,CACQQ,EAASrb,KAAK2a,EACtB,CACMK,EAAShb,KAAKqb,EACpB,CACI,MAAO,CAAE5d,KAAM,eAAgBqd,YAAaE,EAChD,CACI,MAAM,IAAI5c,MAAM,8CAAgDkc,EAEpE,CClLA,IAAI3c,GAEJ4d,OAAOC,QAAUpY,iBAEf,MAAMqY,EAAMC,SAASC,eAAe,OACpChe,GAAM,IAAIie,OAAOC,KAAKpF,IAAIgF,EAAK,CAC7BK,OAAQ,CAAEC,IAAK,QAASC,KAAM,QAC9BC,KAAM,IAMR,IAEE,MAAM3Y,QAAoBsQ,GAAmB,CAAEC,IAJ9B,iCAKXqI,QDVH9Y,eAAkCE,GACvC,MAAMkB,QAAiBnB,EAAqBC,GACtC6Y,EAAc3X,EAASmC,oBAAoBvG,MAAKgc,GAAiB,QAAXA,EAAGtT,MAC/D,IAAKqT,EACH,MAAM,IAAI/d,MAAM,mDAElB,MAAMie,EAAYpe,KAAKC,MAAMie,EAAYxd,OAAS,MAClDkY,QAAQyF,IAAI,qBAAsBD,GAGlC,MAAMlgB,QAAagc,GAAa,CAAEpC,KAAMzS,IACxCuT,QAAQyF,IAAI,mBAAoBjJ,GAAOlX,IAOvC,MAAMogB,EAAW,GAIXC,EAAgBH,EAAUI,gBAAkB,WAElD,IAAK,MAAMzE,KAAO7b,EAAM,CACtB,MAAMugB,EAAS1E,EAAIwE,GACnB,IAAKE,EAEH,SAKF,MAAMC,EAAS,IAAIhgB,WAAW+f,EAAO7f,QACrC,IAAK,IAAIC,EAAE,EAAGA,EAAE4f,EAAO7f,OAAQC,IAC7B6f,EAAO7f,GAAK4f,EAAOE,WAAW9f,GAMhC,MAAM+f,EAAW/C,GAAU6C,GAIrBG,EAAa,CAAA,EACnB,IAAK,MAAMhU,KAAOgE,OAAOV,KAAK4L,GACxBlP,IAAQ0T,IACVM,EAAWhU,GAAOkP,EAAIlP,IAK1B,MAAMiU,EAAU,CACdtf,KAAM,UACNof,WACAC,cAGFP,EAASvc,KAAK+c,EAClB,CAEE,MAAO,CACLtf,KAAM,oBACN8e,WAEJ,CCxD0BS,CAAmB1Z,GAGzC3F,GAAIxB,KAAK8gB,WAAWf,EACrB,CAAC,MAAOgB,GACPrG,QAAQqG,MAAM,4CAA6CA,EAC/D,CACA","x_google_ignoreList":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]}