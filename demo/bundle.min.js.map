{"version":3,"file":"bundle.min.js","sources":["../node_modules/hyparquet/src/constants.js","../node_modules/hyparquet/src/convert.js","../node_modules/hyparquet/src/schema.js","../node_modules/hyparquet/src/thrift.js","../node_modules/hyparquet/src/metadata.js","../node_modules/hyparquet/src/utils.js","../node_modules/hyparquet/src/plan.js","../node_modules/hyparquet/src/assemble.js","../node_modules/hyparquet/src/delta.js","../node_modules/hyparquet/src/encoding.js","../node_modules/hyparquet/src/plain.js","../node_modules/hyparquet/src/snappy.js","../node_modules/hyparquet/src/datapage.js","../node_modules/hyparquet/src/column.js","../node_modules/hyparquet/src/rowgroup.js","../node_modules/hyparquet/src/read.js","../src/wkb.js","../src/toGeoJson.js","demo.js"],"sourcesContent":["/** @type {import('../src/types.d.ts').ParquetType[]} */\nexport const ParquetType = [\n  'BOOLEAN',\n  'INT32',\n  'INT64',\n  'INT96', // deprecated\n  'FLOAT',\n  'DOUBLE',\n  'BYTE_ARRAY',\n  'FIXED_LEN_BYTE_ARRAY',\n]\n\n/** @type {import('../src/types.d.ts').Encoding[]} */\nexport const Encoding = [\n  'PLAIN',\n  'GROUP_VAR_INT', // deprecated\n  'PLAIN_DICTIONARY',\n  'RLE',\n  'BIT_PACKED', // deprecated\n  'DELTA_BINARY_PACKED',\n  'DELTA_LENGTH_BYTE_ARRAY',\n  'DELTA_BYTE_ARRAY',\n  'RLE_DICTIONARY',\n  'BYTE_STREAM_SPLIT',\n]\n\n/** @type {import('../src/types.d.ts').FieldRepetitionType[]} */\nexport const FieldRepetitionType = [\n  'REQUIRED',\n  'OPTIONAL',\n  'REPEATED',\n]\n\n/** @type {import('../src/types.d.ts').ConvertedType[]} */\nexport const ConvertedType = [\n  'UTF8',\n  'MAP',\n  'MAP_KEY_VALUE',\n  'LIST',\n  'ENUM',\n  'DECIMAL',\n  'DATE',\n  'TIME_MILLIS',\n  'TIME_MICROS',\n  'TIMESTAMP_MILLIS',\n  'TIMESTAMP_MICROS',\n  'UINT_8',\n  'UINT_16',\n  'UINT_32',\n  'UINT_64',\n  'INT_8',\n  'INT_16',\n  'INT_32',\n  'INT_64',\n  'JSON',\n  'BSON',\n  'INTERVAL',\n]\n\n/** @type {import('../src/types.d.ts').CompressionCodec[]} */\nexport const CompressionCodec = [\n  'UNCOMPRESSED',\n  'SNAPPY',\n  'GZIP',\n  'LZO',\n  'BROTLI',\n  'LZ4',\n  'ZSTD',\n  'LZ4_RAW',\n]\n\n/** @type {import('../src/types.d.ts').PageType[]} */\nexport const PageType = [\n  'DATA_PAGE',\n  'INDEX_PAGE',\n  'DICTIONARY_PAGE',\n  'DATA_PAGE_V2',\n]\n\n/** @type {import('../src/types.d.ts').BoundaryOrder[]} */\nexport const BoundaryOrder = [\n  'UNORDERED',\n  'ASCENDING',\n  'DESCENDING',\n]\n","/**\n * @import {ColumnDecoder, DecodedArray, Encoding, ParquetParsers, SchemaElement} from '../src/types.d.ts'\n */\n\n/**\n * Default type parsers when no custom ones are given\n * @type ParquetParsers\n */\nexport const DEFAULT_PARSERS = {\n  timestampFromMilliseconds(millis) {\n    return new Date(Number(millis))\n  },\n  timestampFromMicroseconds(micros) {\n    return new Date(Number(micros / 1000n))\n  },\n  timestampFromNanoseconds(nanos) {\n    return new Date(Number(nanos / 1000000n))\n  },\n  dateFromDays(days) {\n    const dayInMillis = 86400000\n    return new Date(days * dayInMillis)\n  },\n}\n\n/**\n * Convert known types from primitive to rich, and dereference dictionary.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {DecodedArray | undefined} dictionary\n * @param {Encoding} encoding\n * @param {ColumnDecoder} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convertWithDictionary(data, dictionary, encoding, columnDecoder) {\n  if (dictionary && encoding.endsWith('_DICTIONARY')) {\n    let output = data\n    if (data instanceof Uint8Array && !(dictionary instanceof Uint8Array)) {\n      // @ts-expect-error upgrade data to match dictionary type with fancy constructor\n      output = new dictionary.constructor(data.length)\n    }\n    for (let i = 0; i < data.length; i++) {\n      output[i] = dictionary[data[i]]\n    }\n    return output\n  } else {\n    return convert(data, columnDecoder)\n  }\n}\n\n/**\n * Convert known types from primitive to rich.\n *\n * @param {DecodedArray} data series of primitive types\n * @param {Pick<ColumnDecoder, \"element\" | \"utf8\" | \"parsers\">} columnDecoder\n * @returns {DecodedArray} series of rich types\n */\nexport function convert(data, columnDecoder) {\n  const { element, parsers, utf8 = true } = columnDecoder\n  const { type, converted_type: ctype, logical_type: ltype } = element\n  if (ctype === 'DECIMAL') {\n    const scale = element.scale || 0\n    const factor = 10 ** -scale\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      if (data[0] instanceof Uint8Array) {\n        arr[i] = parseDecimal(data[i]) * factor\n      } else {\n        arr[i] = Number(data[i]) * factor\n      }\n    }\n    return arr\n  }\n  if (!ctype && type === 'INT96') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parsers.timestampFromNanoseconds(parseInt96Nanos(data[i]))\n    }\n    return arr\n  }\n  if (ctype === 'DATE') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parsers.dateFromDays(data[i])\n    }\n    return arr\n  }\n  if (ctype === 'TIMESTAMP_MILLIS') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parsers.timestampFromMilliseconds(data[i])\n    }\n    return arr\n  }\n  if (ctype === 'TIMESTAMP_MICROS') {\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parsers.timestampFromMicroseconds(data[i])\n    }\n    return arr\n  }\n  if (ctype === 'JSON') {\n    const decoder = new TextDecoder()\n    return data.map(v => JSON.parse(decoder.decode(v)))\n  }\n  if (ctype === 'BSON') {\n    throw new Error('parquet bson not supported')\n  }\n  if (ctype === 'INTERVAL') {\n    throw new Error('parquet interval not supported')\n  }\n  if (ctype === 'UTF8' || ltype?.type === 'STRING' || utf8 && type === 'BYTE_ARRAY') {\n    const decoder = new TextDecoder()\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = data[i] && decoder.decode(data[i])\n    }\n    return arr\n  }\n  if (ctype === 'UINT_64' || ltype?.type === 'INTEGER' && ltype.bitWidth === 64 && !ltype.isSigned) {\n    if (data instanceof BigInt64Array) {\n      return new BigUint64Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new BigUint64Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = BigInt(data[i])\n    return arr\n  }\n  if (ctype === 'UINT_32' || ltype?.type === 'INTEGER' && ltype.bitWidth === 32 && !ltype.isSigned) {\n    if (data instanceof Int32Array) {\n      return new Uint32Array(data.buffer, data.byteOffset, data.length)\n    }\n    const arr = new Uint32Array(data.length)\n    for (let i = 0; i < arr.length; i++) arr[i] = data[i]\n    return arr\n  }\n  if (ltype?.type === 'FLOAT16') {\n    return Array.from(data).map(parseFloat16)\n  }\n  if (ltype?.type === 'TIMESTAMP') {\n    const { unit } = ltype\n    /** @type {ParquetParsers[keyof ParquetParsers]} */\n    let parser = parsers.timestampFromMilliseconds\n    if (unit === 'MICROS') parser = parsers.timestampFromMicroseconds\n    if (unit === 'NANOS') parser = parsers.timestampFromNanoseconds\n    const arr = new Array(data.length)\n    for (let i = 0; i < arr.length; i++) {\n      arr[i] = parser(data[i])\n    }\n    return arr\n  }\n  return data\n}\n\n/**\n * @param {Uint8Array} bytes\n * @returns {number}\n */\nexport function parseDecimal(bytes) {\n  let value = 0\n  for (const byte of bytes) {\n    value = value * 256 + byte\n  }\n\n  // handle signed\n  const bits = bytes.length * 8\n  if (value >= 2 ** (bits - 1)) {\n    value -= 2 ** bits\n  }\n\n  return value\n}\n\n/**\n * Converts INT96 date format (hi 32bit days, lo 64bit nanos) to nanos since epoch\n * @param {bigint} value\n * @returns {bigint}\n */\nfunction parseInt96Nanos(value) {\n  const days = (value >> 64n) - 2440588n\n  const nano = value & 0xffffffffffffffffn\n  return days * 86400000000000n + nano\n}\n\n/**\n * @param {Uint8Array | undefined} bytes\n * @returns {number | undefined}\n */\nexport function parseFloat16(bytes) {\n  if (!bytes) return undefined\n  const int16 = bytes[1] << 8 | bytes[0]\n  const sign = int16 >> 15 ? -1 : 1\n  const exp = int16 >> 10 & 0x1f\n  const frac = int16 & 0x3ff\n  if (exp === 0) return sign * 2 ** -14 * (frac / 1024) // subnormals\n  if (exp === 0x1f) return frac ? NaN : sign * Infinity\n  return sign * 2 ** (exp - 15) * (1 + frac / 1024)\n}\n","/**\n * Build a tree from the schema elements.\n *\n * @import {SchemaElement, SchemaTree} from '../src/types.d.ts'\n * @param {SchemaElement[]} schema\n * @param {number} rootIndex index of the root element\n * @param {string[]} path path to the element\n * @returns {SchemaTree} tree of schema elements\n */\nfunction schemaTree(schema, rootIndex, path) {\n  const element = schema[rootIndex]\n  const children = []\n  let count = 1\n\n  // Read the specified number of children\n  if (element.num_children) {\n    while (children.length < element.num_children) {\n      const childElement = schema[rootIndex + count]\n      const child = schemaTree(schema, rootIndex + count, [...path, childElement.name])\n      count += child.count\n      children.push(child)\n    }\n  }\n\n  return { count, element, children, path }\n}\n\n/**\n * Get schema elements from the root to the given element name.\n *\n * @param {SchemaElement[]} schema\n * @param {string[]} name path to the element\n * @returns {SchemaTree[]} list of schema elements\n */\nexport function getSchemaPath(schema, name) {\n  let tree = schemaTree(schema, 0, [])\n  const path = [tree]\n  for (const part of name) {\n    const child = tree.children.find(child => child.element.name === part)\n    if (!child) throw new Error(`parquet schema element not found: ${name}`)\n    path.push(child)\n    tree = child\n  }\n  return path\n}\n\n/**\n * Get the max repetition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max repetition level\n */\nexport function getMaxRepetitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath) {\n    if (element.repetition_type === 'REPEATED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Get the max definition level for a given schema path.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {number} max definition level\n */\nexport function getMaxDefinitionLevel(schemaPath) {\n  let maxLevel = 0\n  for (const { element } of schemaPath.slice(1)) {\n    if (element.repetition_type !== 'REQUIRED') {\n      maxLevel++\n    }\n  }\n  return maxLevel\n}\n\n/**\n * Check if a column is list-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if list-like\n */\nexport function isListLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'LIST') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length > 1) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Check if a column is map-like.\n *\n * @param {SchemaTree} schema\n * @returns {boolean} true if map-like\n */\nexport function isMapLike(schema) {\n  if (!schema) return false\n  if (schema.element.converted_type !== 'MAP') return false\n  if (schema.children.length > 1) return false\n\n  const firstChild = schema.children[0]\n  if (firstChild.children.length !== 2) return false\n  if (firstChild.element.repetition_type !== 'REPEATED') return false\n\n  const keyChild = firstChild.children.find(child => child.element.name === 'key')\n  if (keyChild?.element.repetition_type === 'REPEATED') return false\n\n  const valueChild = firstChild.children.find(child => child.element.name === 'value')\n  if (valueChild?.element.repetition_type === 'REPEATED') return false\n\n  return true\n}\n\n/**\n * Returns true if a column is non-nested.\n *\n * @param {SchemaTree[]} schemaPath\n * @returns {boolean}\n */\nexport function isFlatColumn(schemaPath) {\n  if (schemaPath.length !== 2) return false\n  const [, column] = schemaPath\n  if (column.element.repetition_type === 'REPEATED') return false\n  if (column.children.length) return false\n  return true\n}\n","// TCompactProtocol types\nexport const CompactType = {\n  STOP: 0,\n  TRUE: 1,\n  FALSE: 2,\n  BYTE: 3,\n  I16: 4,\n  I32: 5,\n  I64: 6,\n  DOUBLE: 7,\n  BINARY: 8,\n  LIST: 9,\n  SET: 10,\n  MAP: 11,\n  STRUCT: 12,\n  UUID: 13,\n}\n\n/**\n * Parse TCompactProtocol\n *\n * @param {DataReader} reader\n * @returns {{ [key: `field_${number}`]: any }}\n */\nexport function deserializeTCompactProtocol(reader) {\n  let lastFid = 0\n  /** @type {ThriftObject} */\n  const value = {}\n\n  while (reader.offset < reader.view.byteLength) {\n    // Parse each field based on its type and add to the result object\n    const [type, fid, newLastFid] = readFieldBegin(reader, lastFid)\n    lastFid = newLastFid\n\n    if (type === CompactType.STOP) {\n      break\n    }\n\n    // Handle the field based on its type\n    value[`field_${fid}`] = readElement(reader, type)\n  }\n\n  return value\n}\n\n/**\n * Read a single element based on its type\n *\n * @import {DataReader, ThriftObject, ThriftType} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} type\n * @returns {ThriftType}\n */\nfunction readElement(reader, type) {\n  switch (type) {\n  case CompactType.TRUE:\n    return true\n  case CompactType.FALSE:\n    return false\n  case CompactType.BYTE:\n    // read byte directly\n    return reader.view.getInt8(reader.offset++)\n  case CompactType.I16:\n  case CompactType.I32:\n    return readZigZag(reader)\n  case CompactType.I64:\n    return readZigZagBigInt(reader)\n  case CompactType.DOUBLE: {\n    const value = reader.view.getFloat64(reader.offset, true)\n    reader.offset += 8\n    return value\n  }\n  case CompactType.BINARY: {\n    const stringLength = readVarInt(reader)\n    const strBytes = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, stringLength)\n    reader.offset += stringLength\n    return strBytes\n  }\n  case CompactType.LIST: {\n    const [elemType, listSize] = readCollectionBegin(reader)\n    const boolType = elemType === CompactType.TRUE || elemType === CompactType.FALSE\n    const values = new Array(listSize)\n    for (let i = 0; i < listSize; i++) {\n      values[i] = boolType ? readElement(reader, CompactType.BYTE) === 1 : readElement(reader, elemType)\n    }\n    return values\n  }\n  case CompactType.STRUCT: {\n    /** @type {ThriftObject} */\n    const structValues = {}\n    let structLastFid = 0\n    while (true) {\n      let structFieldType, structFid\n      [structFieldType, structFid, structLastFid] = readFieldBegin(reader, structLastFid)\n      if (structFieldType === CompactType.STOP) {\n        break\n      }\n      structValues[`field_${structFid}`] = readElement(reader, structFieldType)\n    }\n    return structValues\n  }\n  // TODO: MAP, SET, UUID\n  default:\n    throw new Error(`thrift unhandled type: ${type}`)\n  }\n}\n\n/**\n * Var int, also known as Unsigned LEB128.\n * Var ints take 1 to 5 bytes (int32) or 1 to 10 bytes (int64).\n * Reads groups of 7 low bits until high bit is 0.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readVarInt(reader) {\n  let result = 0\n  let shift = 0\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= (byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7\n  }\n}\n\n/**\n * Read a varint as a bigint.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nfunction readVarBigInt(reader) {\n  let result = 0n\n  let shift = 0n\n  while (true) {\n    const byte = reader.view.getUint8(reader.offset++)\n    result |= BigInt(byte & 0x7f) << shift\n    if (!(byte & 0x80)) {\n      return result\n    }\n    shift += 7n\n  }\n}\n\n/**\n * Values of type int32 and int64 are transformed to a zigzag int.\n * A zigzag int folds positive and negative numbers into the positive number space.\n *\n * @param {DataReader} reader\n * @returns {number}\n */\nexport function readZigZag(reader) {\n  const zigzag = readVarInt(reader)\n  // convert zigzag to int\n  return zigzag >>> 1 ^ -(zigzag & 1)\n}\n\n/**\n * A zigzag int folds positive and negative numbers into the positive number space.\n * This version returns a BigInt.\n *\n * @param {DataReader} reader\n * @returns {bigint}\n */\nexport function readZigZagBigInt(reader) {\n  const zigzag = readVarBigInt(reader)\n  // convert zigzag to int\n  return zigzag >> 1n ^ -(zigzag & 1n)\n}\n\n/**\n * Get thrift type from half a byte\n *\n * @param {number} byte\n * @returns {number}\n */\nfunction getCompactType(byte) {\n  return byte & 0x0f\n}\n\n/**\n * Read field type and field id\n *\n * @param {DataReader} reader\n * @param {number} lastFid\n * @returns {[number, number, number]} [type, fid, newLastFid]\n */\nfunction readFieldBegin(reader, lastFid) {\n  const type = reader.view.getUint8(reader.offset++)\n  if ((type & 0x0f) === CompactType.STOP) {\n    // STOP also ends a struct\n    return [0, 0, lastFid]\n  }\n  const delta = type >> 4\n  let fid // field id\n  if (delta) {\n    // add delta to last field id\n    fid = lastFid + delta\n  } else {\n    throw new Error('non-delta field id not supported')\n  }\n  return [getCompactType(type), fid, fid]\n}\n\n/**\n * Read collection type and size\n *\n * @param {DataReader} reader\n * @returns {[number, number]} [type, size]\n */\nfunction readCollectionBegin(reader) {\n  const sizeType = reader.view.getUint8(reader.offset++)\n  const size = sizeType >> 4\n  const type = getCompactType(sizeType)\n  if (size === 15) {\n    const newSize = readVarInt(reader)\n    return [type, newSize]\n  }\n  return [type, size]\n}\n","import { CompressionCodec, ConvertedType, Encoding, FieldRepetitionType, PageType, ParquetType } from './constants.js'\nimport { DEFAULT_PARSERS, parseDecimal, parseFloat16 } from './convert.js'\nimport { getSchemaPath } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\n\nexport const defaultInitialFetchSize = 1 << 19 // 512kb\n\n/**\n * Read parquet metadata from an async buffer.\n *\n * An AsyncBuffer is like an ArrayBuffer, but the slices are loaded\n * asynchronously, possibly over the network.\n *\n * You must provide the byteLength of the buffer, typically from a HEAD request.\n *\n * In theory, you could use suffix-range requests to fetch the end of the file,\n * and save a round trip. But in practice, this doesn't work because chrome\n * deems suffix-range requests as a not-safe-listed header, and will require\n * a pre-flight. So the byteLength is required.\n *\n * To make this efficient, we initially request the last 512kb of the file,\n * which is likely to contain the metadata. If the metadata length exceeds the\n * initial fetch, 512kb, we request the rest of the metadata from the AsyncBuffer.\n *\n * This ensures that we either make one 512kb initial request for the metadata,\n * or a second request for up to the metadata size.\n *\n * @param {AsyncBuffer} asyncBuffer parquet file contents\n * @param {MetadataOptions & { initialFetchSize?: number }} options initial fetch size in bytes (default 512kb)\n * @returns {Promise<FileMetaData>} parquet metadata object\n */\nexport async function parquetMetadataAsync(asyncBuffer, { parsers, initialFetchSize = defaultInitialFetchSize } = {}) {\n  if (!asyncBuffer || !(asyncBuffer.byteLength >= 0)) throw new Error('parquet expected AsyncBuffer')\n\n  // fetch last bytes (footer) of the file\n  const footerOffset = Math.max(0, asyncBuffer.byteLength - initialFetchSize)\n  const footerBuffer = await asyncBuffer.slice(footerOffset, asyncBuffer.byteLength)\n\n  // Check for parquet magic number \"PAR1\"\n  const footerView = new DataView(footerBuffer)\n  if (footerView.getUint32(footerBuffer.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLength = footerView.getUint32(footerBuffer.byteLength - 8, true)\n  if (metadataLength > asyncBuffer.byteLength - 8) {\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${asyncBuffer.byteLength - 8}`)\n  }\n\n  // check if metadata size fits inside the initial fetch\n  if (metadataLength + 8 > initialFetchSize) {\n    // fetch the rest of the metadata\n    const metadataOffset = asyncBuffer.byteLength - metadataLength - 8\n    const metadataBuffer = await asyncBuffer.slice(metadataOffset, footerOffset)\n    // combine initial fetch with the new slice\n    const combinedBuffer = new ArrayBuffer(metadataLength + 8)\n    const combinedView = new Uint8Array(combinedBuffer)\n    combinedView.set(new Uint8Array(metadataBuffer))\n    combinedView.set(new Uint8Array(footerBuffer), footerOffset - metadataOffset)\n    return parquetMetadata(combinedBuffer, { parsers })\n  } else {\n    // parse metadata from the footer\n    return parquetMetadata(footerBuffer, { parsers })\n  }\n}\n\n/**\n * Read parquet metadata from a buffer synchronously.\n *\n * @param {ArrayBuffer} arrayBuffer parquet file footer\n * @param {MetadataOptions} options metadata parsing options\n * @returns {FileMetaData} parquet metadata object\n */\nexport function parquetMetadata(arrayBuffer, { parsers } = {}) {\n  if (!(arrayBuffer instanceof ArrayBuffer)) throw new Error('parquet expected ArrayBuffer')\n  const view = new DataView(arrayBuffer)\n\n  // Use default parsers if not given\n  parsers = { ...DEFAULT_PARSERS, ...parsers }\n\n  // Validate footer magic number \"PAR1\"\n  if (view.byteLength < 8) {\n    throw new Error('parquet file is too short')\n  }\n  if (view.getUint32(view.byteLength - 4, true) !== 0x31524150) {\n    throw new Error('parquet file invalid (footer != PAR1)')\n  }\n\n  // Parquet files store metadata at the end of the file\n  // Metadata length is 4 bytes before the last PAR1\n  const metadataLengthOffset = view.byteLength - 8\n  const metadataLength = view.getUint32(metadataLengthOffset, true)\n  if (metadataLength > view.byteLength - 8) {\n    // {metadata}, metadata_length, PAR1\n    throw new Error(`parquet metadata length ${metadataLength} exceeds available buffer ${view.byteLength - 8}`)\n  }\n\n  const metadataOffset = metadataLengthOffset - metadataLength\n  const reader = { view, offset: metadataOffset }\n  const metadata = deserializeTCompactProtocol(reader)\n  const decoder = new TextDecoder()\n  function decode(/** @type {Uint8Array} */ value) {\n    return value && decoder.decode(value)\n  }\n\n  // Parse metadata from thrift data\n  const version = metadata.field_1\n  /** @type {SchemaElement[]} */\n  const schema = metadata.field_2.map((/** @type {any} */ field) => ({\n    type: ParquetType[field.field_1],\n    type_length: field.field_2,\n    repetition_type: FieldRepetitionType[field.field_3],\n    name: decode(field.field_4),\n    num_children: field.field_5,\n    converted_type: ConvertedType[field.field_6],\n    scale: field.field_7,\n    precision: field.field_8,\n    field_id: field.field_9,\n    logical_type: logicalType(field.field_10),\n  }))\n  // schema element per column index\n  const columnSchema = schema.filter(e => e.type)\n  const num_rows = metadata.field_3\n  const row_groups = metadata.field_4.map((/** @type {any} */ rowGroup) => ({\n    columns: rowGroup.field_1.map((/** @type {any} */ column, /** @type {number} */ columnIndex) => ({\n      file_path: decode(column.field_1),\n      file_offset: column.field_2,\n      meta_data: column.field_3 && {\n        type: ParquetType[column.field_3.field_1],\n        encodings: column.field_3.field_2?.map((/** @type {number} */ e) => Encoding[e]),\n        path_in_schema: column.field_3.field_3.map(decode),\n        codec: CompressionCodec[column.field_3.field_4],\n        num_values: column.field_3.field_5,\n        total_uncompressed_size: column.field_3.field_6,\n        total_compressed_size: column.field_3.field_7,\n        key_value_metadata: column.field_3.field_8,\n        data_page_offset: column.field_3.field_9,\n        index_page_offset: column.field_3.field_10,\n        dictionary_page_offset: column.field_3.field_11,\n        statistics: convertStats(column.field_3.field_12, columnSchema[columnIndex], parsers),\n        encoding_stats: column.field_3.field_13?.map((/** @type {any} */ encodingStat) => ({\n          page_type: PageType[encodingStat.field_1],\n          encoding: Encoding[encodingStat.field_2],\n          count: encodingStat.field_3,\n        })),\n        bloom_filter_offset: column.field_3.field_14,\n        bloom_filter_length: column.field_3.field_15,\n        size_statistics: column.field_3.field_16 && {\n          unencoded_byte_array_data_bytes: column.field_3.field_16.field_1,\n          repetition_level_histogram: column.field_3.field_16.field_2,\n          definition_level_histogram: column.field_3.field_16.field_3,\n        },\n      },\n      offset_index_offset: column.field_4,\n      offset_index_length: column.field_5,\n      column_index_offset: column.field_6,\n      column_index_length: column.field_7,\n      crypto_metadata: column.field_8,\n      encrypted_column_metadata: column.field_9,\n    })),\n    total_byte_size: rowGroup.field_2,\n    num_rows: rowGroup.field_3,\n    sorting_columns: rowGroup.field_4?.map((/** @type {any} */ sortingColumn) => ({\n      column_idx: sortingColumn.field_1,\n      descending: sortingColumn.field_2,\n      nulls_first: sortingColumn.field_3,\n    })),\n    file_offset: rowGroup.field_5,\n    total_compressed_size: rowGroup.field_6,\n    ordinal: rowGroup.field_7,\n  }))\n  const key_value_metadata = metadata.field_5?.map((/** @type {any} */ keyValue) => ({\n    key: decode(keyValue.field_1),\n    value: decode(keyValue.field_2),\n  }))\n  const created_by = decode(metadata.field_6)\n\n  return {\n    version,\n    schema,\n    num_rows,\n    row_groups,\n    key_value_metadata,\n    created_by,\n    metadata_length: metadataLength,\n  }\n}\n\n/**\n * Return a tree of schema elements from parquet metadata.\n *\n * @param {{schema: SchemaElement[]}} metadata parquet metadata object\n * @returns {SchemaTree} tree of schema elements\n */\nexport function parquetSchema({ schema }) {\n  return getSchemaPath(schema, [])[0]\n}\n\n/**\n * @param {any} logicalType\n * @returns {LogicalType | undefined}\n */\nfunction logicalType(logicalType) {\n  if (logicalType?.field_1) return { type: 'STRING' }\n  if (logicalType?.field_2) return { type: 'MAP' }\n  if (logicalType?.field_3) return { type: 'LIST' }\n  if (logicalType?.field_4) return { type: 'ENUM' }\n  if (logicalType?.field_5) return {\n    type: 'DECIMAL',\n    scale: logicalType.field_5.field_1,\n    precision: logicalType.field_5.field_2,\n  }\n  if (logicalType?.field_6) return { type: 'DATE' }\n  if (logicalType?.field_7) return {\n    type: 'TIME',\n    isAdjustedToUTC: logicalType.field_7.field_1,\n    unit: timeUnit(logicalType.field_7.field_2),\n  }\n  if (logicalType?.field_8) return {\n    type: 'TIMESTAMP',\n    isAdjustedToUTC: logicalType.field_8.field_1,\n    unit: timeUnit(logicalType.field_8.field_2),\n  }\n  if (logicalType?.field_10) return {\n    type: 'INTEGER',\n    bitWidth: logicalType.field_10.field_1,\n    isSigned: logicalType.field_10.field_2,\n  }\n  if (logicalType?.field_11) return { type: 'NULL' }\n  if (logicalType?.field_12) return { type: 'JSON' }\n  if (logicalType?.field_13) return { type: 'BSON' }\n  if (logicalType?.field_14) return { type: 'UUID' }\n  if (logicalType?.field_15) return { type: 'FLOAT16' }\n  return logicalType\n}\n\n/**\n * @param {any} unit\n * @returns {TimeUnit}\n */\nfunction timeUnit(unit) {\n  if (unit.field_1) return 'MILLIS'\n  if (unit.field_2) return 'MICROS'\n  if (unit.field_3) return 'NANOS'\n  throw new Error('parquet time unit required')\n}\n\n/**\n * Convert column statistics based on column type.\n *\n * @import {AsyncBuffer, FileMetaData, LogicalType, MetadataOptions, MinMaxType, ParquetParsers, SchemaElement, SchemaTree, Statistics, TimeUnit} from '../src/types.d.ts'\n * @param {any} stats\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {Statistics}\n */\nfunction convertStats(stats, schema, parsers) {\n  return stats && {\n    max: convertMetadata(stats.field_1, schema, parsers),\n    min: convertMetadata(stats.field_2, schema, parsers),\n    null_count: stats.field_3,\n    distinct_count: stats.field_4,\n    max_value: convertMetadata(stats.field_5, schema, parsers),\n    min_value: convertMetadata(stats.field_6, schema, parsers),\n    is_max_value_exact: stats.field_7,\n    is_min_value_exact: stats.field_8,\n  }\n}\n\n/**\n * @param {Uint8Array | undefined} value\n * @param {SchemaElement} schema\n * @param {ParquetParsers} parsers\n * @returns {MinMaxType | undefined}\n */\nexport function convertMetadata(value, schema, parsers) {\n  const { type, converted_type, logical_type } = schema\n  if (value === undefined) return value\n  if (type === 'BOOLEAN') return value[0] === 1\n  if (type === 'BYTE_ARRAY') return new TextDecoder().decode(value)\n  const view = new DataView(value.buffer, value.byteOffset, value.byteLength)\n  if (type === 'FLOAT' && view.byteLength === 4) return view.getFloat32(0, true)\n  if (type === 'DOUBLE' && view.byteLength === 8) return view.getFloat64(0, true)\n  if (type === 'INT32' && converted_type === 'DATE') return parsers.dateFromDays(view.getInt32(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MILLIS') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && converted_type === 'TIMESTAMP_MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'NANOS') return parsers.timestampFromNanoseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP' && logical_type?.unit === 'MICROS') return parsers.timestampFromMicroseconds(view.getBigInt64(0, true))\n  if (type === 'INT64' && logical_type?.type === 'TIMESTAMP') return parsers.timestampFromMilliseconds(view.getBigInt64(0, true))\n  if (type === 'INT32' && view.byteLength === 4) return view.getInt32(0, true)\n  if (type === 'INT64' && view.byteLength === 8) return view.getBigInt64(0, true)\n  if (converted_type === 'DECIMAL') return parseDecimal(value) * 10 ** -(schema.scale || 0)\n  if (logical_type?.type === 'FLOAT16') return parseFloat16(value)\n  if (type === 'FIXED_LEN_BYTE_ARRAY') return value\n  // assert(false)\n  return value\n}\n","import { defaultInitialFetchSize } from './metadata.js'\n\n/**\n * Replace bigint, date, etc with legal JSON types.\n *\n * @param {any} obj object to convert\n * @returns {unknown} converted object\n */\nexport function toJson(obj) {\n  if (obj === undefined) return null\n  if (typeof obj === 'bigint') return Number(obj)\n  if (Array.isArray(obj)) return obj.map(toJson)\n  if (obj instanceof Uint8Array) return Array.from(obj)\n  if (obj instanceof Date) return obj.toISOString()\n  if (obj instanceof Object) {\n    /** @type {Record<string, unknown>} */\n    const newObj = {}\n    for (const key of Object.keys(obj)) {\n      if (obj[key] === undefined) continue\n      newObj[key] = toJson(obj[key])\n    }\n    return newObj\n  }\n  return obj\n}\n\n/**\n * Concatenate two arrays fast.\n *\n * @param {any[]} aaa first array\n * @param {DecodedArray} bbb second array\n */\nexport function concat(aaa, bbb) {\n  const chunk = 10000\n  for (let i = 0; i < bbb.length; i += chunk) {\n    aaa.push(...bbb.slice(i, i + chunk))\n  }\n}\n\n/**\n * Deep equality comparison\n *\n * @param {any} a First object to compare\n * @param {any} b Second object to compare\n * @returns {boolean} true if objects are equal\n */\nexport function equals(a, b) {\n  if (a === b) return true\n  if (a instanceof Uint8Array && b instanceof Uint8Array) return equals(Array.from(a), Array.from(b))\n  if (!a || !b || typeof a !== typeof b) return false\n  return Array.isArray(a) && Array.isArray(b)\n    ? a.length === b.length && a.every((v, i) => equals(v, b[i]))\n    : typeof a === 'object' && Object.keys(a).length === Object.keys(b).length && Object.keys(a).every(k => equals(a[k], b[k]))\n}\n\n/**\n * Get the byte length of a URL using a HEAD request.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {string} url\n * @param {RequestInit} [requestInit] fetch options\n * @param {typeof globalThis.fetch} [customFetch] fetch function to use\n * @returns {Promise<number>}\n */\nexport async function byteLengthFromUrl(url, requestInit, customFetch) {\n  const fetch = customFetch ?? globalThis.fetch\n  return await fetch(url, { ...requestInit, method: 'HEAD' })\n    .then(res => {\n      if (!res.ok) throw new Error(`fetch head failed ${res.status}`)\n      const length = res.headers.get('Content-Length')\n      if (!length) throw new Error('missing content length')\n      return parseInt(length)\n    })\n}\n\n/**\n * Construct an AsyncBuffer for a URL.\n * If byteLength is not provided, will make a HEAD request to get the file size.\n * If fetch is provided, it will be used instead of the global fetch.\n * If requestInit is provided, it will be passed to fetch.\n *\n * @param {object} options\n * @param {string} options.url\n * @param {number} [options.byteLength]\n * @param {typeof globalThis.fetch} [options.fetch] fetch function to use\n * @param {RequestInit} [options.requestInit]\n * @returns {Promise<AsyncBuffer>}\n */\nexport async function asyncBufferFromUrl({ url, byteLength, requestInit, fetch: customFetch }) {\n  if (!url) throw new Error('missing url')\n  const fetch = customFetch ?? globalThis.fetch\n  // byte length from HEAD request\n  byteLength ||= await byteLengthFromUrl(url, requestInit, fetch)\n\n  /**\n   * A promise for the whole buffer, if range requests are not supported.\n   * @type {Promise<ArrayBuffer>|undefined}\n   */\n  let buffer = undefined\n  const init = requestInit || {}\n\n  return {\n    byteLength,\n    async slice(start, end) {\n      if (buffer) {\n        return buffer.then(buffer => buffer.slice(start, end))\n      }\n\n      const headers = new Headers(init.headers)\n      const endStr = end === undefined ? '' : end - 1\n      headers.set('Range', `bytes=${start}-${endStr}`)\n\n      const res = await fetch(url, { ...init, headers })\n      if (!res.ok || !res.body) throw new Error(`fetch failed ${res.status}`)\n\n      if (res.status === 200) {\n        // Endpoint does not support range requests and returned the whole object\n        buffer = res.arrayBuffer()\n        return buffer.then(buffer => buffer.slice(start, end))\n      } else if (res.status === 206) {\n        // The endpoint supports range requests and sent us the requested range\n        return res.arrayBuffer()\n      } else {\n        throw new Error(`fetch received unexpected status code ${res.status}`)\n      }\n    },\n  }\n}\n\n/**\n * Returns a cached layer on top of an AsyncBuffer. For caching slices of a file\n * that are read multiple times, possibly over a network.\n *\n * @param {AsyncBuffer} file file-like object to cache\n * @param {{ minSize?: number }} [options]\n * @returns {AsyncBuffer} cached file-like object\n */\nexport function cachedAsyncBuffer({ byteLength, slice }, { minSize = defaultInitialFetchSize } = {}) {\n  if (byteLength < minSize) {\n    // Cache whole file if it's small\n    const buffer = slice(0, byteLength)\n    return {\n      byteLength,\n      async slice(start, end) {\n        return (await buffer).slice(start, end)\n      },\n    }\n  }\n  const cache = new Map()\n  return {\n    byteLength,\n    /**\n     * @param {number} start\n     * @param {number} [end]\n     * @returns {Awaitable<ArrayBuffer>}\n     */\n    slice(start, end) {\n      const key = cacheKey(start, end, byteLength)\n      const cached = cache.get(key)\n      if (cached) return cached\n      // cache miss, read from file\n      const promise = slice(start, end)\n      cache.set(key, promise)\n      return promise\n    },\n  }\n}\n\n\n/**\n * Returns canonical cache key for a byte range 'start,end'.\n * Normalize int-range and suffix-range requests to the same key.\n *\n * @import {AsyncBuffer, Awaitable, DecodedArray} from '../src/types.d.ts'\n * @param {number} start start byte of range\n * @param {number} [end] end byte of range, or undefined for suffix range\n * @param {number} [size] size of file, or undefined for suffix range\n * @returns {string}\n */\nfunction cacheKey(start, end, size) {\n  if (start < 0) {\n    if (end !== undefined) throw new Error(`invalid suffix range [${start}, ${end}]`)\n    if (size === undefined) return `${start},`\n    return `${size + start},${size}`\n  } else if (end !== undefined) {\n    if (start > end) throw new Error(`invalid empty range [${start}, ${end}]`)\n    return `${start},${end}`\n  } else if (size === undefined) {\n    return `${start},`\n  } else {\n    return `${start},${size}`\n  }\n}\n\n/**\n * Flatten a list of lists into a single list.\n *\n * @param {DecodedArray[]} [chunks]\n * @returns {DecodedArray}\n */\nexport function flatten(chunks) {\n  if (!chunks) return []\n  if (chunks.length === 1) return chunks[0]\n  /** @type {any[]} */\n  const output = []\n  for (const chunk of chunks) {\n    concat(output, chunk)\n  }\n  return output\n}\n","import { concat } from './utils.js'\n\n// Combine column chunks into a single byte range if less than 32mb\nconst columnChunkAggregation = 1 << 25 // 32mb\n\n/**\n * @import {AsyncBuffer, ByteRange, ColumnMetaData, GroupPlan, ParquetReadOptions, QueryPlan} from '../src/types.js'\n */\n/**\n * Plan which byte ranges to read to satisfy a read request.\n * Metadata must be non-null.\n *\n * @param {ParquetReadOptions} options\n * @returns {QueryPlan}\n */\nexport function parquetPlan({ metadata, rowStart = 0, rowEnd = Infinity, columns }) {\n  if (!metadata) throw new Error('parquetPlan requires metadata')\n  /** @type {GroupPlan[]} */\n  const groups = []\n  /** @type {ByteRange[]} */\n  const fetches = []\n\n  // find which row groups to read\n  let groupStart = 0 // first row index of the current group\n  for (const rowGroup of metadata.row_groups) {\n    const groupRows = Number(rowGroup.num_rows)\n    const groupEnd = groupStart + groupRows\n    // if row group overlaps with row range, add it to the plan\n    if (groupRows > 0 && groupEnd >= rowStart && groupStart < rowEnd) {\n      /** @type {ByteRange[]} */\n      const ranges = []\n      // loop through each column chunk\n      for (const { file_path, meta_data } of rowGroup.columns) {\n        if (file_path) throw new Error('parquet file_path not supported')\n        if (!meta_data) throw new Error('parquet column metadata is undefined')\n        // add included columns to the plan\n        if (!columns || columns.includes(meta_data.path_in_schema[0])) {\n          ranges.push(getColumnRange(meta_data))\n        }\n      }\n      const selectStart = Math.max(rowStart - groupStart, 0)\n      const selectEnd = Math.min(rowEnd - groupStart, groupRows)\n      groups.push({ ranges, rowGroup, groupStart, groupRows, selectStart, selectEnd })\n\n      // map group plan to ranges\n      const groupSize = ranges[ranges.length - 1]?.endByte - ranges[0]?.startByte\n      if (!columns && groupSize < columnChunkAggregation) {\n        // full row group\n        fetches.push({\n          startByte: ranges[0].startByte,\n          endByte: ranges[ranges.length - 1].endByte,\n        })\n      } else if (ranges.length) {\n        concat(fetches, ranges)\n      } else if (columns?.length) {\n        throw new Error(`parquet columns not found: ${columns.join(', ')}`)\n      }\n    }\n\n    groupStart = groupEnd\n  }\n  if (!isFinite(rowEnd)) rowEnd = groupStart\n\n  return { metadata, rowStart, rowEnd, columns, fetches, groups }\n}\n\n/**\n * @param {ColumnMetaData} columnMetadata\n * @returns {ByteRange}\n */\nexport function getColumnRange({ dictionary_page_offset, data_page_offset, total_compressed_size }) {\n  const columnOffset = dictionary_page_offset || data_page_offset\n  return {\n    startByte: Number(columnOffset),\n    endByte: Number(columnOffset + total_compressed_size),\n  }\n}\n\n/**\n * Prefetch byte ranges from an AsyncBuffer.\n *\n * @param {AsyncBuffer} file\n * @param {QueryPlan} plan\n * @returns {AsyncBuffer}\n */\nexport function prefetchAsyncBuffer(file, { fetches }) {\n  // fetch byte ranges from the file\n  const promises = fetches.map(({ startByte, endByte }) => file.slice(startByte, endByte))\n  return {\n    byteLength: file.byteLength,\n    slice(start, end = file.byteLength) {\n      // find matching slice\n      const index = fetches.findIndex(({ startByte, endByte }) => startByte <= start && end <= endByte)\n      if (index < 0) throw new Error(`no prefetch for range [${start}, ${end}]`)\n      if (fetches[index].startByte !== start || fetches[index].endByte !== end) {\n        // slice a subrange of the prefetch\n        const startOffset = start - fetches[index].startByte\n        const endOffset = end - fetches[index].startByte\n        if (promises[index] instanceof Promise) {\n          return promises[index].then(buffer => buffer.slice(startOffset, endOffset))\n        } else {\n          return promises[index].slice(startOffset, endOffset)\n        }\n      } else {\n        return promises[index]\n      }\n    },\n  }\n}\n","import { getMaxDefinitionLevel, isListLike, isMapLike } from './schema.js'\n\n/**\n * Reconstructs a complex nested structure from flat arrays of values and\n * definition and repetition levels, according to Dremel encoding.\n *\n * @param {any[]} output\n * @param {number[] | undefined} definitionLevels\n * @param {number[]} repetitionLevels\n * @param {DecodedArray} values\n * @param {SchemaTree[]} schemaPath\n * @returns {DecodedArray}\n */\nexport function assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath) {\n  const n = definitionLevels?.length || repetitionLevels.length\n  if (!n) return values\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  const repetitionPath = schemaPath.map(({ element }) => element.repetition_type)\n  let valueIndex = 0\n\n  // Track state of nested structures\n  const containerStack = [output]\n  let currentContainer = output\n  let currentDepth = 0 // schema depth\n  let currentDefLevel = 0 // list depth\n  let currentRepLevel = 0\n\n  if (repetitionLevels[0]) {\n    // continue previous row\n    while (currentDepth < repetitionPath.length - 2 && currentRepLevel < repetitionLevels[0]) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        // go into last list\n        currentContainer = currentContainer.at(-1)\n        containerStack.push(currentContainer)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n  }\n\n  for (let i = 0; i < n; i++) {\n    // assert(currentDefLevel === containerStack.length - 1)\n    const def = definitionLevels?.length ? definitionLevels[i] : maxDefinitionLevel\n    const rep = repetitionLevels[i]\n\n    // Pop up to start of rep level\n    while (currentDepth && (rep < currentRepLevel || repetitionPath[currentDepth] !== 'REPEATED')) {\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        containerStack.pop()\n        currentDefLevel--\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel--\n      currentDepth--\n    }\n    // @ts-expect-error won't be empty\n    currentContainer = containerStack.at(-1)\n\n    // Go deeper to end of definition level\n    while (\n      (currentDepth < repetitionPath.length - 2 || repetitionPath[currentDepth + 1] === 'REPEATED') &&\n      (currentDefLevel < def || repetitionPath[currentDepth + 1] === 'REQUIRED')\n    ) {\n      currentDepth++\n      if (repetitionPath[currentDepth] !== 'REQUIRED') {\n        /** @type {any[]} */\n        const newList = []\n        currentContainer.push(newList)\n        currentContainer = newList\n        containerStack.push(newList)\n        currentDefLevel++\n      }\n      if (repetitionPath[currentDepth] === 'REPEATED') currentRepLevel++\n    }\n\n    // Add value or null based on definition level\n    if (def === maxDefinitionLevel) {\n      // assert(currentDepth === maxDefinitionLevel || currentDepth === repetitionPath.length - 2)\n      currentContainer.push(values[valueIndex++])\n    } else if (currentDepth === repetitionPath.length - 2) {\n      currentContainer.push(null)\n    } else {\n      currentContainer.push([])\n    }\n  }\n\n  // Handle edge cases for empty inputs or single-level data\n  if (!output.length) {\n    // return max definition level of nested lists\n    for (let i = 0; i < maxDefinitionLevel; i++) {\n      /** @type {any[]} */\n      const newList = []\n      currentContainer.push(newList)\n      currentContainer = newList\n    }\n  }\n\n  return output\n}\n\n/**\n * Assemble a nested structure from subcolumn data.\n * https://github.com/apache/parquet-format/blob/apache-parquet-format-2.10.0/LogicalTypes.md#nested-types\n *\n * @param {Map<string, DecodedArray>} subcolumnData\n * @param {SchemaTree} schema top-level schema element\n * @param {number} [depth] depth of nested structure\n */\nexport function assembleNested(subcolumnData, schema, depth = 0) {\n  const path = schema.path.join('.')\n  const optional = schema.element.repetition_type === 'OPTIONAL'\n  const nextDepth = optional ? depth + 1 : depth\n\n  if (isListLike(schema)) {\n    let sublist = schema.children[0]\n    let subDepth = nextDepth\n    if (sublist.children.length === 1) {\n      sublist = sublist.children[0]\n      subDepth++\n    }\n    assembleNested(subcolumnData, sublist, subDepth)\n\n    const subcolumn = sublist.path.join('.')\n    const values = subcolumnData.get(subcolumn)\n    if (!values) throw new Error('parquet list column missing values')\n    if (optional) flattenAtDepth(values, depth)\n    subcolumnData.set(path, values)\n    subcolumnData.delete(subcolumn)\n    return\n  }\n\n  if (isMapLike(schema)) {\n    const mapName = schema.children[0].element.name\n\n    // Assemble keys and values\n    assembleNested(subcolumnData, schema.children[0].children[0], nextDepth + 1)\n    assembleNested(subcolumnData, schema.children[0].children[1], nextDepth + 1)\n\n    const keys = subcolumnData.get(`${path}.${mapName}.key`)\n    const values = subcolumnData.get(`${path}.${mapName}.value`)\n\n    if (!keys) throw new Error('parquet map column missing keys')\n    if (!values) throw new Error('parquet map column missing values')\n    if (keys.length !== values.length) {\n      throw new Error('parquet map column key/value length mismatch')\n    }\n\n    const out = assembleMaps(keys, values, nextDepth)\n    if (optional) flattenAtDepth(out, depth)\n\n    subcolumnData.delete(`${path}.${mapName}.key`)\n    subcolumnData.delete(`${path}.${mapName}.value`)\n    subcolumnData.set(path, out)\n    return\n  }\n\n  // Struct-like column\n  if (schema.children.length) {\n    // construct a meta struct and then invert\n    const invertDepth = schema.element.repetition_type === 'REQUIRED' ? depth : depth + 1\n    /** @type {Record<string, any>} */\n    const struct = {}\n    for (const child of schema.children) {\n      assembleNested(subcolumnData, child, invertDepth)\n      const childData = subcolumnData.get(child.path.join('.'))\n      if (!childData) throw new Error('parquet struct missing child data')\n      struct[child.element.name] = childData\n    }\n    // remove children\n    for (const child of schema.children) {\n      subcolumnData.delete(child.path.join('.'))\n    }\n    // invert struct by depth\n    const inverted = invertStruct(struct, invertDepth)\n    if (optional) flattenAtDepth(inverted, depth)\n    subcolumnData.set(path, inverted)\n  }\n}\n\n/**\n * @import {DecodedArray, SchemaTree} from '../src/types.d.ts'\n * @param {DecodedArray} arr\n * @param {number} depth\n */\nfunction flattenAtDepth(arr, depth) {\n  for (let i = 0; i < arr.length; i++) {\n    if (depth) {\n      flattenAtDepth(arr[i], depth - 1)\n    } else {\n      arr[i] = arr[i][0]\n    }\n  }\n}\n\n/**\n * @param {DecodedArray} keys\n * @param {DecodedArray} values\n * @param {number} depth\n * @returns {any[]}\n */\nfunction assembleMaps(keys, values, depth) {\n  const out = []\n  for (let i = 0; i < keys.length; i++) {\n    if (depth) {\n      out.push(assembleMaps(keys[i], values[i], depth - 1)) // go deeper\n    } else {\n      if (keys[i]) {\n        /** @type {Record<string, any>} */\n        const obj = {}\n        for (let j = 0; j < keys[i].length; j++) {\n          const value = values[i][j]\n          obj[keys[i][j]] = value === undefined ? null : value\n        }\n        out.push(obj)\n      } else {\n        out.push(undefined)\n      }\n    }\n  }\n  return out\n}\n\n/**\n * Invert a struct-like object by depth.\n *\n * @param {Record<string, any[]>} struct\n * @param {number} depth\n * @returns {any[]}\n */\nfunction invertStruct(struct, depth) {\n  const keys = Object.keys(struct)\n  const length = struct[keys[0]]?.length\n  const out = []\n  for (let i = 0; i < length; i++) {\n    /** @type {Record<string, any>} */\n    const obj = {}\n    for (const key of keys) {\n      if (struct[key].length !== length) throw new Error('parquet struct parsing error')\n      obj[key] = struct[key][i]\n    }\n    if (depth) {\n      out.push(invertStruct(obj, depth - 1)) // deeper\n    } else {\n      out.push(obj)\n    }\n  }\n  return out\n}\n","import { readVarInt, readZigZagBigInt } from './thrift.js'\n\n/**\n * @import {DataReader} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @param {number} count number of values to read\n * @param {Int32Array | BigInt64Array} output\n */\nexport function deltaBinaryUnpack(reader, count, output) {\n  const int32 = output instanceof Int32Array\n  const blockSize = readVarInt(reader)\n  const miniblockPerBlock = readVarInt(reader)\n  readVarInt(reader) // assert(=== count)\n  let value = readZigZagBigInt(reader) // first value\n  let outputIndex = 0\n  output[outputIndex++] = int32 ? Number(value) : value\n\n  const valuesPerMiniblock = blockSize / miniblockPerBlock\n\n  while (outputIndex < count) {\n    // new block\n    const minDelta = readZigZagBigInt(reader)\n    const bitWidths = new Uint8Array(miniblockPerBlock)\n    for (let i = 0; i < miniblockPerBlock; i++) {\n      bitWidths[i] = reader.view.getUint8(reader.offset++)\n    }\n\n    for (let i = 0; i < miniblockPerBlock && outputIndex < count; i++) {\n      // new miniblock\n      const bitWidth = BigInt(bitWidths[i])\n      if (bitWidth) {\n        let bitpackPos = 0n\n        let miniblockCount = valuesPerMiniblock\n        const mask = (1n << bitWidth) - 1n\n        while (miniblockCount && outputIndex < count) {\n          let bits = BigInt(reader.view.getUint8(reader.offset)) >> bitpackPos & mask // TODO: don't re-read value every time\n          bitpackPos += bitWidth\n          while (bitpackPos >= 8) {\n            bitpackPos -= 8n\n            reader.offset++\n            if (bitpackPos) {\n              bits |= BigInt(reader.view.getUint8(reader.offset)) << bitWidth - bitpackPos & mask\n            }\n          }\n          const delta = minDelta + bits\n          value += delta\n          output[outputIndex++] = int32 ? Number(value) : value\n          miniblockCount--\n        }\n        if (miniblockCount) {\n          // consume leftover miniblock\n          reader.offset += Math.ceil((miniblockCount * Number(bitWidth) + Number(bitpackPos)) / 8)\n        }\n      } else {\n        for (let j = 0; j < valuesPerMiniblock && outputIndex < count; j++) {\n          value += minDelta\n          output[outputIndex++] = int32 ? Number(value) : value\n        }\n      }\n    }\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaLengthByteArray(reader, count, output) {\n  const lengths = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, lengths)\n  for (let i = 0; i < count; i++) {\n    output[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, lengths[i])\n    reader.offset += lengths[i]\n  }\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {Uint8Array[]} output\n */\nexport function deltaByteArray(reader, count, output) {\n  const prefixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, prefixData)\n  const suffixData = new Int32Array(count)\n  deltaBinaryUnpack(reader, count, suffixData)\n\n  for (let i = 0; i < count; i++) {\n    const suffix = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, suffixData[i])\n    if (prefixData[i]) {\n      // copy from previous value\n      output[i] = new Uint8Array(prefixData[i] + suffixData[i])\n      output[i].set(output[i - 1].subarray(0, prefixData[i]))\n      output[i].set(suffix, prefixData[i])\n    } else {\n      output[i] = suffix\n    }\n    reader.offset += suffixData[i]\n  }\n}\n","import { readVarInt } from './thrift.js'\n\n/**\n * Minimum bits needed to store value.\n *\n * @param {number} value\n * @returns {number}\n */\nexport function bitWidth(value) {\n  return 32 - Math.clz32(value)\n}\n\n/**\n * Read values from a run-length encoded/bit-packed hybrid encoding.\n *\n * If length is zero, then read int32 length at the start.\n *\n * @param {DataReader} reader\n * @param {number} width - bitwidth\n * @param {DecodedArray} output\n * @param {number} [length] - length of the encoded data\n */\nexport function readRleBitPackedHybrid(reader, width, output, length) {\n  if (length === undefined) {\n    length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n  }\n  const startOffset = reader.offset\n  let seen = 0\n  while (seen < output.length) {\n    const header = readVarInt(reader)\n    if (header & 1) {\n      // bit-packed\n      seen = readBitPacked(reader, header, width, output, seen)\n    } else {\n      // rle\n      const count = header >>> 1\n      readRle(reader, count, width, output, seen)\n      seen += count\n    }\n  }\n  reader.offset = startOffset + length // duckdb writes an empty block\n}\n\n/**\n * Run-length encoding: read value with bitWidth and repeat it count times.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n */\nfunction readRle(reader, count, bitWidth, output, seen) {\n  const width = bitWidth + 7 >> 3\n  let value = 0\n  for (let i = 0; i < width; i++) {\n    value |= reader.view.getUint8(reader.offset++) << (i << 3)\n  }\n  // assert(value < 1 << bitWidth)\n\n  // repeat value count times\n  for (let i = 0; i < count; i++) {\n    output[seen + i] = value\n  }\n}\n\n/**\n * Read a bit-packed run of the rle/bitpack hybrid.\n * Supports width > 8 (crossing bytes).\n *\n * @param {DataReader} reader\n * @param {number} header - bit-pack header\n * @param {number} bitWidth\n * @param {DecodedArray} output\n * @param {number} seen\n * @returns {number} total output values so far\n */\nfunction readBitPacked(reader, header, bitWidth, output, seen) {\n  let count = header >> 1 << 3 // values to read\n  const mask = (1 << bitWidth) - 1\n\n  let data = 0\n  if (reader.offset < reader.view.byteLength) {\n    data = reader.view.getUint8(reader.offset++)\n  } else if (mask) {\n    // sometimes out-of-bounds reads are masked out\n    throw new Error(`parquet bitpack offset ${reader.offset} out of range`)\n  }\n  let left = 8\n  let right = 0\n\n  // read values\n  while (count) {\n    // if we have crossed a byte boundary, shift the data\n    if (right > 8) {\n      right -= 8\n      left -= 8\n      data >>>= 8\n    } else if (left - right < bitWidth) {\n      // if we don't have bitWidth number of bits to read, read next byte\n      data |= reader.view.getUint8(reader.offset) << left\n      reader.offset++\n      left += 8\n    } else {\n      if (seen < output.length) {\n        // emit value\n        output[seen++] = data >> right & mask\n      }\n      count--\n      right += bitWidth\n    }\n  }\n\n  return seen\n}\n\n/**\n * @param {DataReader} reader\n * @param {number} count\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {DecodedArray}\n */\nexport function byteStreamSplit(reader, count, type, typeLength) {\n  const width = byteWidth(type, typeLength)\n  const bytes = new Uint8Array(count * width)\n  for (let b = 0; b < width; b++) {\n    for (let i = 0; i < count; i++) {\n      bytes[i * width + b] = reader.view.getUint8(reader.offset++)\n    }\n  }\n  // interpret bytes as typed array\n  if (type === 'FLOAT') return new Float32Array(bytes.buffer)\n  else if (type === 'DOUBLE') return new Float64Array(bytes.buffer)\n  else if (type === 'INT32') return new Int32Array(bytes.buffer)\n  else if (type === 'INT64') return new BigInt64Array(bytes.buffer)\n  else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    // split into arrays of typeLength\n    const split = new Array(count)\n    for (let i = 0; i < count; i++) {\n      split[i] = bytes.subarray(i * width, (i + 1) * width)\n    }\n    return split\n  }\n  throw new Error(`parquet byte_stream_split unsupported type: ${type}`)\n}\n\n/**\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ParquetType} type\n * @param {number | undefined} typeLength\n * @returns {number}\n */\nfunction byteWidth(type, typeLength) {\n  switch (type) {\n  case 'INT32':\n  case 'FLOAT':\n    return 4\n  case 'INT64':\n  case 'DOUBLE':\n    return 8\n  case 'FIXED_LEN_BYTE_ARRAY':\n    if (!typeLength) throw new Error('parquet byteWidth missing type_length')\n    return typeLength\n  default:\n    throw new Error(`parquet unsupported type: ${type}`)\n  }\n}\n","/**\n * Read `count` values of the given type from the reader.view.\n *\n * @param {DataReader} reader - buffer to read data from\n * @param {ParquetType} type - parquet type of the data\n * @param {number} count - number of values to read\n * @param {number | undefined} fixedLength - length of each fixed length byte array\n * @returns {DecodedArray} array of values\n */\nexport function readPlain(reader, type, count, fixedLength) {\n  if (count === 0) return []\n  if (type === 'BOOLEAN') {\n    return readPlainBoolean(reader, count)\n  } else if (type === 'INT32') {\n    return readPlainInt32(reader, count)\n  } else if (type === 'INT64') {\n    return readPlainInt64(reader, count)\n  } else if (type === 'INT96') {\n    return readPlainInt96(reader, count)\n  } else if (type === 'FLOAT') {\n    return readPlainFloat(reader, count)\n  } else if (type === 'DOUBLE') {\n    return readPlainDouble(reader, count)\n  } else if (type === 'BYTE_ARRAY') {\n    return readPlainByteArray(reader, count)\n  } else if (type === 'FIXED_LEN_BYTE_ARRAY') {\n    if (!fixedLength) throw new Error('parquet missing fixed length')\n    return readPlainByteArrayFixed(reader, count, fixedLength)\n  } else {\n    throw new Error(`parquet unhandled type: ${type}`)\n  }\n}\n\n/**\n * Read `count` boolean values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {boolean[]}\n */\nfunction readPlainBoolean(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const byteOffset = reader.offset + (i / 8 | 0)\n    const bitOffset = i % 8\n    const byte = reader.view.getUint8(byteOffset)\n    values[i] = (byte & 1 << bitOffset) !== 0\n  }\n  reader.offset += Math.ceil(count / 8)\n  return values\n}\n\n/**\n * Read `count` int32 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Int32Array}\n */\nfunction readPlainInt32(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Int32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Int32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` int64 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {BigInt64Array}\n */\nfunction readPlainInt64(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new BigInt64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new BigInt64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` int96 values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {bigint[]}\n */\nfunction readPlainInt96(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const low = reader.view.getBigInt64(reader.offset + i * 12, true)\n    const high = reader.view.getInt32(reader.offset + i * 12 + 8, true)\n    values[i] = BigInt(high) << 64n | low\n  }\n  reader.offset += count * 12\n  return values\n}\n\n/**\n * Read `count` float values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float32Array}\n */\nfunction readPlainFloat(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 4\n    ? new Float32Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 4))\n    : new Float32Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 4\n  return values\n}\n\n/**\n * Read `count` double values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Float64Array}\n */\nfunction readPlainDouble(reader, count) {\n  const values = (reader.view.byteOffset + reader.offset) % 8\n    ? new Float64Array(align(reader.view.buffer, reader.view.byteOffset + reader.offset, count * 8))\n    : new Float64Array(reader.view.buffer, reader.view.byteOffset + reader.offset, count)\n  reader.offset += count * 8\n  return values\n}\n\n/**\n * Read `count` byte array values.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArray(reader, count) {\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    const length = reader.view.getUint32(reader.offset, true)\n    reader.offset += 4\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, length)\n    reader.offset += length\n  }\n  return values\n}\n\n/**\n * Read a fixed length byte array.\n *\n * @param {DataReader} reader\n * @param {number} count\n * @param {number} fixedLength\n * @returns {Uint8Array[]}\n */\nfunction readPlainByteArrayFixed(reader, count, fixedLength) {\n  // assert(reader.view.byteLength - reader.offset >= count * fixedLength)\n  const values = new Array(count)\n  for (let i = 0; i < count; i++) {\n    values[i] = new Uint8Array(reader.view.buffer, reader.view.byteOffset + reader.offset, fixedLength)\n    reader.offset += fixedLength\n  }\n  return values\n}\n\n/**\n * Create a new buffer with the offset and size.\n *\n * @import {DataReader, DecodedArray, ParquetType} from '../src/types.d.ts'\n * @param {ArrayBufferLike} buffer\n * @param {number} offset\n * @param {number} size\n * @returns {ArrayBuffer}\n */\nfunction align(buffer, offset, size) {\n  const aligned = new ArrayBuffer(size)\n  new Uint8Array(aligned).set(new Uint8Array(buffer, offset, size))\n  return aligned\n}\n","/**\n * The MIT License (MIT)\n * Copyright (c) 2016 Zhipeng Jia\n * https://github.com/zhipeng-jia/snappyjs\n */\n\nconst WORD_MASK = [0, 0xff, 0xffff, 0xffffff, 0xffffffff]\n\n/**\n * Copy bytes from one array to another\n *\n * @param {Uint8Array} fromArray source array\n * @param {number} fromPos source position\n * @param {Uint8Array} toArray destination array\n * @param {number} toPos destination position\n * @param {number} length number of bytes to copy\n */\nfunction copyBytes(fromArray, fromPos, toArray, toPos, length) {\n  for (let i = 0; i < length; i++) {\n    toArray[toPos + i] = fromArray[fromPos + i]\n  }\n}\n\n/**\n * Decompress snappy data.\n * Accepts an output buffer to avoid allocating a new buffer for each call.\n *\n * @param {Uint8Array} input compressed data\n * @param {Uint8Array} output output buffer\n */\nexport function snappyUncompress(input, output) {\n  const inputLength = input.byteLength\n  const outputLength = output.byteLength\n  let pos = 0\n  let outPos = 0\n\n  // skip preamble (contains uncompressed length as varint)\n  while (pos < inputLength) {\n    const c = input[pos]\n    pos++\n    if (c < 128) {\n      break\n    }\n  }\n  if (outputLength && pos >= inputLength) {\n    throw new Error('invalid snappy length header')\n  }\n\n  while (pos < inputLength) {\n    const c = input[pos]\n    let len = 0\n    pos++\n\n    if (pos >= inputLength) {\n      throw new Error('missing eof marker')\n    }\n\n    // There are two types of elements, literals and copies (back references)\n    if ((c & 0x3) === 0) {\n      // Literals are uncompressed data stored directly in the byte stream\n      let len = (c >>> 2) + 1\n      // Longer literal length is encoded in multiple bytes\n      if (len > 60) {\n        if (pos + 3 >= inputLength) {\n          throw new Error('snappy error literal pos + 3 >= inputLength')\n        }\n        const lengthSize = len - 60 // length bytes - 1\n        len = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        len = (len & WORD_MASK[lengthSize]) + 1\n        pos += lengthSize\n      }\n      if (pos + len > inputLength) {\n        throw new Error('snappy error literal exceeds input length')\n      }\n      copyBytes(input, pos, output, outPos, len)\n      pos += len\n      outPos += len\n    } else {\n      // Copy elements\n      let offset = 0 // offset back from current position to read\n      switch (c & 0x3) {\n      case 1:\n        // Copy with 1-byte offset\n        len = (c >>> 2 & 0x7) + 4\n        offset = input[pos] + (c >>> 5 << 8)\n        pos++\n        break\n      case 2:\n        // Copy with 2-byte offset\n        if (inputLength <= pos + 1) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos] + (input[pos + 1] << 8)\n        pos += 2\n        break\n      case 3:\n        // Copy with 4-byte offset\n        if (inputLength <= pos + 3) {\n          throw new Error('snappy error end of input')\n        }\n        len = (c >>> 2) + 1\n        offset = input[pos]\n          + (input[pos + 1] << 8)\n          + (input[pos + 2] << 16)\n          + (input[pos + 3] << 24)\n        pos += 4\n        break\n      default:\n        break\n      }\n      if (offset === 0 || isNaN(offset)) {\n        throw new Error(`invalid offset ${offset} pos ${pos} inputLength ${inputLength}`)\n      }\n      if (offset > outPos) {\n        throw new Error('cannot copy from before start of buffer')\n      }\n      copyBytes(output, outPos - offset, output, outPos, len)\n      outPos += len\n    }\n  }\n\n  if (outPos !== outputLength) throw new Error('premature end of input')\n}\n","import { deltaBinaryUnpack, deltaByteArray, deltaLengthByteArray } from './delta.js'\nimport { bitWidth, byteStreamSplit, readRleBitPackedHybrid } from './encoding.js'\nimport { readPlain } from './plain.js'\nimport { getMaxDefinitionLevel, getMaxRepetitionLevel } from './schema.js'\nimport { snappyUncompress } from './snappy.js'\n\n/**\n * Read a data page from uncompressed reader.\n *\n * @param {Uint8Array} bytes raw page data (should already be decompressed)\n * @param {DataPageHeader} daph data page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPage(bytes, daph, { type, element, schemaPath }) {\n  const view = new DataView(bytes.buffer, bytes.byteOffset, bytes.byteLength)\n  const reader = { view, offset: 0 }\n  /** @type {DecodedArray} */\n  let dataPage\n\n  // repetition and definition levels\n  const repetitionLevels = readRepetitionLevels(reader, daph, schemaPath)\n  // assert(!repetitionLevels.length || repetitionLevels.length === daph.num_values)\n  const { definitionLevels, numNulls } = readDefinitionLevels(reader, daph, schemaPath)\n  // assert(!definitionLevels.length || definitionLevels.length === daph.num_values)\n\n  // read values based on encoding\n  const nValues = daph.num_values - numNulls\n  if (daph.encoding === 'PLAIN') {\n    dataPage = readPlain(reader, type, nValues, element.type_length)\n  } else if (\n    daph.encoding === 'PLAIN_DICTIONARY' ||\n    daph.encoding === 'RLE_DICTIONARY' ||\n    daph.encoding === 'RLE'\n  ) {\n    const bitWidth = type === 'BOOLEAN' ? 1 : view.getUint8(reader.offset++)\n    if (bitWidth) {\n      dataPage = new Array(nValues)\n      if (type === 'BOOLEAN') {\n        readRleBitPackedHybrid(reader, bitWidth, dataPage)\n        dataPage = dataPage.map(x => !!x) // convert to boolean\n      } else {\n        // assert(daph.encoding.endsWith('_DICTIONARY'))\n        readRleBitPackedHybrid(reader, bitWidth, dataPage, view.byteLength - reader.offset)\n      }\n    } else {\n      dataPage = new Uint8Array(nValues) // nValue zeroes\n    }\n  } else if (daph.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(reader, nValues, type, element.type_length)\n  } else if (daph.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(reader, nValues, dataPage)\n  } else if (daph.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(reader, nValues, dataPage)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @import {ColumnDecoder, CompressionCodec, Compressors, DataPage, DataPageHeader, DataPageHeaderV2, DataReader, DecodedArray, PageHeader, SchemaTree} from '../src/types.d.ts'\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels and number of bytes read\n */\nfunction readRepetitionLevels(reader, daph, schemaPath) {\n  if (schemaPath.length > 1) {\n    const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n    if (maxRepetitionLevel) {\n      const values = new Array(daph.num_values)\n      readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values)\n      return values\n    }\n  }\n  return []\n}\n\n/**\n * @param {DataReader} reader data view for the page\n * @param {DataPageHeader} daph data page header\n * @param {SchemaTree[]} schemaPath\n * @returns {{ definitionLevels: number[], numNulls: number }} definition levels\n */\nfunction readDefinitionLevels(reader, daph, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (!maxDefinitionLevel) return { definitionLevels: [], numNulls: 0 }\n\n  const definitionLevels = new Array(daph.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), definitionLevels)\n\n  // count nulls\n  let numNulls = daph.num_values\n  for (const def of definitionLevels) {\n    if (def === maxDefinitionLevel) numNulls--\n  }\n  if (numNulls === 0) definitionLevels.length = 0\n\n  return { definitionLevels, numNulls }\n}\n\n/**\n * @param {Uint8Array} compressedBytes\n * @param {number} uncompressed_page_size\n * @param {CompressionCodec} codec\n * @param {Compressors | undefined} compressors\n * @returns {Uint8Array}\n */\nexport function decompressPage(compressedBytes, uncompressed_page_size, codec, compressors) {\n  /** @type {Uint8Array} */\n  let page\n  const customDecompressor = compressors?.[codec]\n  if (codec === 'UNCOMPRESSED') {\n    page = compressedBytes\n  } else if (customDecompressor) {\n    page = customDecompressor(compressedBytes, uncompressed_page_size)\n  } else if (codec === 'SNAPPY') {\n    page = new Uint8Array(uncompressed_page_size)\n    snappyUncompress(compressedBytes, page)\n  } else {\n    throw new Error(`parquet unsupported compression codec: ${codec}`)\n  }\n  if (page?.length !== uncompressed_page_size) {\n    throw new Error(`parquet decompressed page length ${page?.length} does not match header ${uncompressed_page_size}`)\n  }\n  return page\n}\n\n\n/**\n * Read a data page from the given Uint8Array.\n *\n * @param {Uint8Array} compressedBytes raw page data\n * @param {PageHeader} ph page header\n * @param {ColumnDecoder} columnDecoder\n * @returns {DataPage} definition levels, repetition levels, and array of values\n */\nexport function readDataPageV2(compressedBytes, ph, columnDecoder) {\n  const view = new DataView(compressedBytes.buffer, compressedBytes.byteOffset, compressedBytes.byteLength)\n  const reader = { view, offset: 0 }\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  const daph2 = ph.data_page_header_v2\n  if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n  // repetition levels\n  const repetitionLevels = readRepetitionLevelsV2(reader, daph2, schemaPath)\n  reader.offset = daph2.repetition_levels_byte_length // readVarInt() => len for boolean v2?\n\n  // definition levels\n  const definitionLevels = readDefinitionLevelsV2(reader, daph2, schemaPath)\n  // assert(reader.offset === daph2.repetition_levels_byte_length + daph2.definition_levels_byte_length)\n\n  const uncompressedPageSize = ph.uncompressed_page_size - daph2.definition_levels_byte_length - daph2.repetition_levels_byte_length\n\n  let page = compressedBytes.subarray(reader.offset)\n  if (daph2.is_compressed !== false) {\n    page = decompressPage(page, uncompressedPageSize, codec, compressors)\n  }\n  const pageView = new DataView(page.buffer, page.byteOffset, page.byteLength)\n  const pageReader = { view: pageView, offset: 0 }\n\n  // read values based on encoding\n  /** @type {DecodedArray} */\n  let dataPage\n  const nValues = daph2.num_values - daph2.num_nulls\n  if (daph2.encoding === 'PLAIN') {\n    dataPage = readPlain(pageReader, type, nValues, element.type_length)\n  } else if (daph2.encoding === 'RLE') {\n    // assert(type === 'BOOLEAN')\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, 1, dataPage)\n    dataPage = dataPage.map(x => !!x)\n  } else if (\n    daph2.encoding === 'PLAIN_DICTIONARY' ||\n    daph2.encoding === 'RLE_DICTIONARY'\n  ) {\n    const bitWidth = pageView.getUint8(pageReader.offset++)\n    dataPage = new Array(nValues)\n    readRleBitPackedHybrid(pageReader, bitWidth, dataPage, uncompressedPageSize - 1)\n  } else if (daph2.encoding === 'DELTA_BINARY_PACKED') {\n    const int32 = type === 'INT32'\n    dataPage = int32 ? new Int32Array(nValues) : new BigInt64Array(nValues)\n    deltaBinaryUnpack(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_LENGTH_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaLengthByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'DELTA_BYTE_ARRAY') {\n    dataPage = new Array(nValues)\n    deltaByteArray(pageReader, nValues, dataPage)\n  } else if (daph2.encoding === 'BYTE_STREAM_SPLIT') {\n    dataPage = byteStreamSplit(reader, nValues, type, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported encoding: ${daph2.encoding}`)\n  }\n\n  return { definitionLevels, repetitionLevels, dataPage }\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {any[]} repetition levels\n */\nfunction readRepetitionLevelsV2(reader, daph2, schemaPath) {\n  const maxRepetitionLevel = getMaxRepetitionLevel(schemaPath)\n  if (!maxRepetitionLevel) return []\n\n  const values = new Array(daph2.num_values)\n  readRleBitPackedHybrid(reader, bitWidth(maxRepetitionLevel), values, daph2.repetition_levels_byte_length)\n  return values\n}\n\n/**\n * @param {DataReader} reader\n * @param {DataPageHeaderV2} daph2 data page header v2\n * @param {SchemaTree[]} schemaPath\n * @returns {number[] | undefined} definition levels\n */\nfunction readDefinitionLevelsV2(reader, daph2, schemaPath) {\n  const maxDefinitionLevel = getMaxDefinitionLevel(schemaPath)\n  if (maxDefinitionLevel) {\n    // V2 we know the length\n    const values = new Array(daph2.num_values)\n    readRleBitPackedHybrid(reader, bitWidth(maxDefinitionLevel), values, daph2.definition_levels_byte_length)\n    return values\n  }\n}\n","import { assembleLists } from './assemble.js'\nimport { Encoding, PageType } from './constants.js'\nimport { convert, convertWithDictionary } from './convert.js'\nimport { decompressPage, readDataPage, readDataPageV2 } from './datapage.js'\nimport { readPlain } from './plain.js'\nimport { isFlatColumn } from './schema.js'\nimport { deserializeTCompactProtocol } from './thrift.js'\n\n/**\n * Parse column data from a buffer.\n *\n * @param {DataReader} reader\n * @param {RowGroupSelect} rowGroupSelect row group selection\n * @param {ColumnDecoder} columnDecoder column decoder params\n * @param {(chunk: ColumnData) => void} [onPage] callback for each page\n * @returns {DecodedArray[]}\n */\nexport function readColumn(reader, { groupStart, selectStart, selectEnd }, columnDecoder, onPage) {\n  const { columnName } = columnDecoder\n  /** @type {DecodedArray[]} */\n  const chunks = []\n  /** @type {DecodedArray | undefined} */\n  let dictionary = undefined\n  /** @type {DecodedArray | undefined} */\n  let lastChunk = undefined\n  let rowCount = 0\n\n  const emitLastChunk = onPage && (() => {\n    lastChunk && onPage({\n      columnName,\n      columnData: lastChunk,\n      rowStart: groupStart + rowCount - lastChunk.length,\n      rowEnd: groupStart + rowCount,\n    })\n  })\n\n  while (rowCount < selectEnd) {\n    if (reader.offset >= reader.view.byteLength - 1) break // end of reader\n\n    // read page header\n    const header = parquetHeader(reader)\n    if (header.type === 'DICTIONARY_PAGE') {\n      // assert(!dictionary)\n      dictionary = readPage(reader, header, columnDecoder, dictionary, undefined, 0)\n      dictionary = convert(dictionary, columnDecoder)\n    } else {\n      const lastChunkLength = lastChunk?.length || 0\n      const values = readPage(reader, header, columnDecoder, dictionary, lastChunk, selectStart - rowCount)\n      if (lastChunk === values) {\n        // continued from previous page\n        rowCount += values.length - lastChunkLength\n      } else {\n        emitLastChunk?.()\n        chunks.push(values)\n        rowCount += values.length\n        lastChunk = values\n      }\n    }\n  }\n  emitLastChunk?.()\n  // assert(rowCount >= selectEnd)\n  if (rowCount > selectEnd && lastChunk) {\n    // truncate last chunk to row limit\n    chunks[chunks.length - 1] = lastChunk.slice(0, selectEnd - (rowCount - lastChunk.length))\n  }\n  return chunks\n}\n\n/**\n * Read a page (data or dictionary) from a buffer.\n *\n * @param {DataReader} reader\n * @param {PageHeader} header\n * @param {ColumnDecoder} columnDecoder\n * @param {DecodedArray | undefined} dictionary\n * @param {DecodedArray | undefined} previousChunk\n * @param {number} pageStart skip this many rows in the page\n * @returns {DecodedArray}\n */\nexport function readPage(reader, header, columnDecoder, dictionary, previousChunk, pageStart) {\n  const { type, element, schemaPath, codec, compressors } = columnDecoder\n  // read compressed_page_size bytes\n  const compressedBytes = new Uint8Array(\n    reader.view.buffer, reader.view.byteOffset + reader.offset, header.compressed_page_size\n  )\n  reader.offset += header.compressed_page_size\n\n  // parse page data by type\n  if (header.type === 'DATA_PAGE') {\n    const daph = header.data_page_header\n    if (!daph) throw new Error('parquet data page header is undefined')\n\n    // skip unnecessary non-nested pages\n    if (pageStart > daph.num_values && isFlatColumn(schemaPath)) {\n      return new Array(daph.num_values) // TODO: don't allocate array\n    }\n\n    const page = decompressPage(compressedBytes, Number(header.uncompressed_page_size), codec, compressors)\n    const { definitionLevels, repetitionLevels, dataPage } = readDataPage(page, daph, columnDecoder)\n    // assert(!daph.statistics?.null_count || daph.statistics.null_count === BigInt(daph.num_values - dataPage.length))\n\n    // convert types, dereference dictionary, and assemble lists\n    let values = convertWithDictionary(dataPage, dictionary, daph.encoding, columnDecoder)\n    if (repetitionLevels.length || definitionLevels?.length) {\n      const output = Array.isArray(previousChunk) ? previousChunk : []\n      return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n    } else {\n      // wrap nested flat data by depth\n      for (let i = 2; i < schemaPath.length; i++) {\n        if (schemaPath[i].element.repetition_type !== 'REQUIRED') {\n          values = Array.from(values, e => [e])\n        }\n      }\n      return values\n    }\n  } else if (header.type === 'DATA_PAGE_V2') {\n    const daph2 = header.data_page_header_v2\n    if (!daph2) throw new Error('parquet data page header v2 is undefined')\n\n    // skip unnecessary pages\n    if (pageStart > daph2.num_rows) {\n      return new Array(daph2.num_values) // TODO: don't allocate array\n    }\n\n    const { definitionLevels, repetitionLevels, dataPage } =\n      readDataPageV2(compressedBytes, header, columnDecoder)\n\n    // convert types, dereference dictionary, and assemble lists\n    const values = convertWithDictionary(dataPage, dictionary, daph2.encoding, columnDecoder)\n    const output = Array.isArray(previousChunk) ? previousChunk : []\n    return assembleLists(output, definitionLevels, repetitionLevels, values, schemaPath)\n  } else if (header.type === 'DICTIONARY_PAGE') {\n    const diph = header.dictionary_page_header\n    if (!diph) throw new Error('parquet dictionary page header is undefined')\n\n    const page = decompressPage(\n      compressedBytes, Number(header.uncompressed_page_size), codec, compressors\n    )\n\n    const reader = { view: new DataView(page.buffer, page.byteOffset, page.byteLength), offset: 0 }\n    return readPlain(reader, type, diph.num_values, element.type_length)\n  } else {\n    throw new Error(`parquet unsupported page type: ${header.type}`)\n  }\n}\n\n/**\n * Read parquet header from a buffer.\n *\n * @import {ColumnData, ColumnDecoder, DataReader, DecodedArray, PageHeader, RowGroupSelect} from '../src/types.d.ts'\n * @param {DataReader} reader\n * @returns {PageHeader}\n */\nfunction parquetHeader(reader) {\n  const header = deserializeTCompactProtocol(reader)\n\n  // Parse parquet header from thrift data\n  const type = PageType[header.field_1]\n  const uncompressed_page_size = header.field_2\n  const compressed_page_size = header.field_3\n  const crc = header.field_4\n  const data_page_header = header.field_5 && {\n    num_values: header.field_5.field_1,\n    encoding: Encoding[header.field_5.field_2],\n    definition_level_encoding: Encoding[header.field_5.field_3],\n    repetition_level_encoding: Encoding[header.field_5.field_4],\n    statistics: header.field_5.field_5 && {\n      max: header.field_5.field_5.field_1,\n      min: header.field_5.field_5.field_2,\n      null_count: header.field_5.field_5.field_3,\n      distinct_count: header.field_5.field_5.field_4,\n      max_value: header.field_5.field_5.field_5,\n      min_value: header.field_5.field_5.field_6,\n    },\n  }\n  const index_page_header = header.field_6\n  const dictionary_page_header = header.field_7 && {\n    num_values: header.field_7.field_1,\n    encoding: Encoding[header.field_7.field_2],\n    is_sorted: header.field_7.field_3,\n  }\n  const data_page_header_v2 = header.field_8 && {\n    num_values: header.field_8.field_1,\n    num_nulls: header.field_8.field_2,\n    num_rows: header.field_8.field_3,\n    encoding: Encoding[header.field_8.field_4],\n    definition_levels_byte_length: header.field_8.field_5,\n    repetition_levels_byte_length: header.field_8.field_6,\n    is_compressed: header.field_8.field_7 === undefined ? true : header.field_8.field_7, // default true\n    statistics: header.field_8.field_8,\n  }\n\n  return {\n    type,\n    uncompressed_page_size,\n    compressed_page_size,\n    crc,\n    data_page_header,\n    index_page_header,\n    dictionary_page_header,\n    data_page_header_v2,\n  }\n}\n","import { assembleNested } from './assemble.js'\nimport { readColumn } from './column.js'\nimport { DEFAULT_PARSERS } from './convert.js'\nimport { getColumnRange } from './plan.js'\nimport { getSchemaPath } from './schema.js'\nimport { flatten } from './utils.js'\n\n/**\n * @import {AsyncColumn, AsyncRowGroup, DecodedArray, GroupPlan, ParquetParsers, ParquetReadOptions, QueryPlan, RowGroup, SchemaTree} from './types.js'\n */\n/**\n * Read a row group from a file-like object.\n *\n * @param {ParquetReadOptions} options\n * @param {QueryPlan} plan\n * @param {GroupPlan} groupPlan\n * @returns {AsyncRowGroup} resolves to column data\n */\nexport function readRowGroup(options, { metadata, columns }, groupPlan) {\n  const { file, compressors, utf8 } = options\n\n  /** @type {AsyncColumn[]} */\n  const asyncColumns = []\n  /** @type {ParquetParsers} */\n  const parsers = { ...DEFAULT_PARSERS, ...options.parsers }\n\n  // read column data\n  for (const { file_path, meta_data } of groupPlan.rowGroup.columns) {\n    if (file_path) throw new Error('parquet file_path not supported')\n    if (!meta_data) throw new Error('parquet column metadata is undefined')\n\n    // skip columns that are not requested\n    const columnName = meta_data.path_in_schema[0]\n    if (columns && !columns.includes(columnName)) continue\n\n    const { startByte, endByte } = getColumnRange(meta_data)\n    const columnBytes = endByte - startByte\n\n    // skip columns larger than 1gb\n    // TODO: stream process the data, returning only the requested rows\n    if (columnBytes > 1 << 30) {\n      console.warn(`parquet skipping huge column \"${meta_data.path_in_schema}\" ${columnBytes} bytes`)\n      // TODO: set column to new Error('parquet column too large')\n      continue\n    }\n\n    // wrap awaitable to ensure it's a promise\n    /** @type {Promise<ArrayBuffer>} */\n    const buffer = Promise.resolve(file.slice(startByte, endByte))\n\n    // read column data async\n    asyncColumns.push({\n      pathInSchema: meta_data.path_in_schema,\n      data: buffer.then(arrayBuffer => {\n        const schemaPath = getSchemaPath(metadata.schema, meta_data.path_in_schema)\n        const reader = { view: new DataView(arrayBuffer), offset: 0 }\n        const subcolumn = meta_data.path_in_schema.join('.')\n        const columnDecoder = {\n          columnName: subcolumn,\n          type: meta_data.type,\n          element: schemaPath[schemaPath.length - 1].element,\n          schemaPath,\n          codec: meta_data.codec,\n          parsers,\n          compressors,\n          utf8,\n        }\n        return readColumn(reader, groupPlan, columnDecoder, options.onPage)\n      }),\n    })\n  }\n\n  return { groupStart: groupPlan.groupStart, groupRows: groupPlan.groupRows, asyncColumns }\n}\n\n/**\n * @param {AsyncRowGroup} asyncGroup\n * @param {number} selectStart\n * @param {number} selectEnd\n * @param {string[] | undefined} columns\n * @param {'object' | 'array'} [rowFormat]\n * @returns {Promise<Record<string, any>[]>} resolves to row data\n */\nexport async function asyncGroupToRows({ asyncColumns }, selectStart, selectEnd, columns, rowFormat) {\n  const groupData = new Array(selectEnd)\n\n  // columnData[i] for asyncColumns[i]\n  // TODO: do it without flatten\n  const columnDatas = await Promise.all(asyncColumns.map(({ data }) => data.then(flatten)))\n\n  // careful mapping of column order for rowFormat: array\n  const includedColumnNames = asyncColumns\n    .map(child => child.pathInSchema[0])\n    .filter(name => !columns || columns.includes(name))\n  const columnOrder = columns ?? includedColumnNames\n  const columnIndexes = columnOrder.map(name => asyncColumns.findIndex(column => column.pathInSchema[0] === name))\n\n  // transpose columns into rows\n  for (let row = selectStart; row < selectEnd; row++) {\n    if (rowFormat === 'object') {\n      // return each row as an object\n      /** @type {Record<string, any>} */\n      const rowData = {}\n      for (let i = 0; i < asyncColumns.length; i++) {\n        rowData[asyncColumns[i].pathInSchema[0]] = columnDatas[i][row]\n      }\n      groupData[row] = rowData\n    } else {\n      // return each row as an array\n      const rowData = new Array(asyncColumns.length)\n      for (let i = 0; i < columnOrder.length; i++) {\n        if (columnIndexes[i] >= 0) {\n          rowData[i] = columnDatas[columnIndexes[i]][row]\n        }\n      }\n      groupData[row] = rowData\n    }\n  }\n  return groupData\n}\n\n/**\n * Assemble physical columns into top-level columns asynchronously.\n *\n * @param {AsyncRowGroup} asyncRowGroup\n * @param {SchemaTree} schemaTree\n * @returns {AsyncRowGroup}\n */\nexport function assembleAsync(asyncRowGroup, schemaTree) {\n  const { asyncColumns } = asyncRowGroup\n  /** @type {AsyncColumn[]} */\n  const assembled = []\n  for (const child of schemaTree.children) {\n    if (child.children.length) {\n      const childColumns = asyncColumns.filter(column => column.pathInSchema[0] === child.element.name)\n      if (!childColumns.length) continue\n\n      // wait for all child columns to be read\n      /** @type {Map<string, DecodedArray>} */\n      const flatData = new Map()\n      const data = Promise.all(childColumns.map(column => {\n        return column.data.then(columnData => {\n          flatData.set(column.pathInSchema.join('.'), flatten(columnData))\n        })\n      })).then(() => {\n        // assemble the column\n        assembleNested(flatData, child)\n        const flatColumn = flatData.get(child.path.join('.'))\n        if (!flatColumn) throw new Error('parquet column data not assembled')\n        return [flatColumn]\n      })\n\n      assembled.push({ pathInSchema: child.path, data })\n    } else {\n      // leaf node, return the column\n      const asyncColumn = asyncColumns.find(column => column.pathInSchema[0] === child.element.name)\n      if (asyncColumn) {\n        assembled.push(asyncColumn)\n      }\n    }\n  }\n  return { ...asyncRowGroup, asyncColumns: assembled }\n}\n","import { parquetMetadataAsync, parquetSchema } from './metadata.js'\nimport { parquetPlan, prefetchAsyncBuffer } from './plan.js'\nimport { assembleAsync, asyncGroupToRows, readRowGroup } from './rowgroup.js'\nimport { concat, flatten } from './utils.js'\n\n/**\n * @import {AsyncRowGroup, DecodedArray, ParquetReadOptions} from '../src/types.js'\n */\n/**\n * Read parquet data rows from a file-like object.\n * Reads the minimal number of row groups and columns to satisfy the request.\n *\n * Returns a void promise when complete.\n * Errors are thrown on the returned promise.\n * Data is returned in callbacks onComplete, onChunk, onPage, NOT the return promise.\n * See parquetReadObjects for a more convenient API.\n *\n * @param {ParquetReadOptions} options read options\n * @returns {Promise<void>} resolves when all requested rows and columns are parsed, all errors are thrown here\n */\nexport async function parquetRead(options) {\n  // load metadata if not provided\n  options.metadata ??= await parquetMetadataAsync(options.file)\n\n  // read row groups\n  const asyncGroups = parquetReadAsync(options)\n\n  const { rowStart = 0, rowEnd, columns, onChunk, onComplete, rowFormat } = options\n\n  // skip assembly if no onComplete or onChunk, but wait for reading to finish\n  if (!onComplete && !onChunk) {\n    for (const { asyncColumns } of asyncGroups) {\n      for (const { data } of asyncColumns) await data\n    }\n    return\n  }\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  // onChunk emit all chunks (don't await)\n  if (onChunk) {\n    for (const asyncGroup of assembled) {\n      for (const asyncColumn of asyncGroup.asyncColumns) {\n        asyncColumn.data.then(columnDatas => {\n          let rowStart = asyncGroup.groupStart\n          for (const columnData of columnDatas) {\n            onChunk({\n              columnName: asyncColumn.pathInSchema[0],\n              columnData,\n              rowStart,\n              rowEnd: rowStart + columnData.length,\n            })\n            rowStart += columnData.length\n          }\n        })\n      }\n    }\n  }\n\n  // onComplete transpose column chunks to rows\n  if (onComplete) {\n    /** @type {any[][]} */\n    const rows = []\n    for (const asyncGroup of assembled) {\n      // filter to rows in range\n      const selectStart = Math.max(rowStart - asyncGroup.groupStart, 0)\n      const selectEnd = Math.min((rowEnd ?? Infinity) - asyncGroup.groupStart, asyncGroup.groupRows)\n      // transpose column chunks to rows in output\n      const groupData = await asyncGroupToRows(asyncGroup, selectStart, selectEnd, columns, rowFormat)\n      concat(rows, groupData.slice(selectStart, selectEnd))\n    }\n    onComplete(rows)\n  } else {\n    // wait for all async groups to finish (complete takes care of this)\n    for (const { asyncColumns } of assembled) {\n      for (const { data } of asyncColumns) await data\n    }\n  }\n}\n\n/**\n * @param {ParquetReadOptions} options read options\n * @returns {AsyncRowGroup[]}\n */\nexport function parquetReadAsync(options) {\n  if (!options.metadata) throw new Error('parquet requires metadata')\n  // TODO: validate options (start, end, columns, etc)\n\n  // prefetch byte ranges\n  const plan = parquetPlan(options)\n  options.file = prefetchAsyncBuffer(options.file, plan)\n\n  // read row groups\n  return plan.groups.map(groupPlan => readRowGroup(options, plan, groupPlan))\n}\n\n/**\n * Reads a single column from a parquet file.\n *\n * @param {ParquetReadOptions} options\n * @returns {Promise<DecodedArray>}\n */\nexport async function parquetReadColumn(options) {\n  if (options.columns?.length !== 1) {\n    throw new Error('parquetReadColumn expected columns: [columnName]')\n  }\n  options.metadata ??= await parquetMetadataAsync(options.file)\n  const asyncGroups = parquetReadAsync(options)\n\n  // assemble struct columns\n  const schemaTree = parquetSchema(options.metadata)\n  const assembled = asyncGroups.map(arg => assembleAsync(arg, schemaTree))\n\n  /** @type {DecodedArray[]} */\n  const columnData = []\n  for (const rg of assembled) {\n    columnData.push(flatten(await rg.asyncColumns[0].data))\n  }\n  return flatten(columnData)\n}\n\n/**\n * This is a helper function to read parquet row data as a promise.\n * It is a wrapper around the more configurable parquetRead function.\n *\n * @param {Omit<ParquetReadOptions, 'onComplete'>} options\n * @returns {Promise<Record<string, any>[]>} resolves when all requested rows and columns are parsed\n*/\nexport function parquetReadObjects(options) {\n  return new Promise((onComplete, reject) => {\n    parquetRead({\n      rowFormat: 'object',\n      ...options,\n      onComplete,\n    }).catch(reject)\n  })\n}\n","\nconst geometryTypePoint = 1\nconst geometryTypeLineString = 2\nconst geometryTypePolygon = 3\nconst geometryTypeMultiPoint = 4\nconst geometryTypeMultiLineString = 5\nconst geometryTypeMultiPolygon = 6\nconst geometryTypeGeometryCollection = 7\nconst geometryTypeCircularString = 8\nconst geometryTypeCompoundCurve = 9\nconst geometryTypeCurvePolygon = 10\nconst geometryTypeMultiCurve = 11\nconst geometryTypeMultiSurface = 12\nconst geometryTypeCurve = 13\nconst geometryTypeSurface = 14\nconst geometryTypePolyhedralSurface = 15\nconst geometryTypeTIN = 16\nconst geometryTypeTriangle = 17\nconst geometryTypeCircle = 18\nconst geometryTypeGeodesicString = 19\nconst geometryTypeEllipticalCurve = 20\nconst geometryTypeNurbsCurve = 21\nconst geometryTypeClothoid = 22\nconst geometryTypeSpiralCurve = 23\nconst geometryTypeCompoundSurface = 24\n\n/**\n * WKB (Well Known Binary) decoder for geometry objects.\n *\n * @import { Geometry } from '../src/geojson.js'\n * @param {Uint8Array} wkb\n * @returns {Geometry} GeoJSON geometry object\n */\nexport function decodeWKB(wkb) {\n  const dv = new DataView(wkb.buffer, wkb.byteOffset, wkb.byteLength)\n  let offset = 0\n\n  // Byte order: 0 = big-endian, 1 = little-endian\n  const byteOrder = wkb[offset]; offset += 1\n  const isLittleEndian = byteOrder === 1\n\n  // Read geometry type\n  const geometryType = dv.getUint32(offset, isLittleEndian)\n  offset += 4\n\n  // WKB geometry types (OGC):\n  if (geometryType === geometryTypePoint) {\n    // Point\n    const x = dv.getFloat64(offset, isLittleEndian); offset += 8\n    const y = dv.getFloat64(offset, isLittleEndian); offset += 8\n    return { type: 'Point', coordinates: [x, y] }\n  } else if (geometryType === geometryTypeLineString) {\n    // LineString\n    const numPoints = dv.getUint32(offset, isLittleEndian); offset += 4\n    const coords = []\n    for (let i = 0; i < numPoints; i++) {\n      const x = dv.getFloat64(offset, isLittleEndian); offset += 8\n      const y = dv.getFloat64(offset, isLittleEndian); offset += 8\n      coords.push([x, y])\n    }\n    return { type: 'LineString', coordinates: coords }\n  } else if (geometryType === geometryTypePolygon) {\n    // Polygon\n    const numRings = dv.getUint32(offset, isLittleEndian); offset += 4\n    const coords = []\n    for (let r = 0; r < numRings; r++) {\n      const numPoints = dv.getUint32(offset, isLittleEndian); offset += 4\n      const ring = []\n      for (let p = 0; p < numPoints; p++) {\n        const x = dv.getFloat64(offset, isLittleEndian); offset += 8\n        const y = dv.getFloat64(offset, isLittleEndian); offset += 8\n        ring.push([x, y])\n      }\n      coords.push(ring)\n    }\n    return { type: 'Polygon', coordinates: coords }\n  } else if (geometryType === geometryTypeMultiPolygon) {\n    // MultiPolygon\n    const numPolygons = dv.getUint32(offset, isLittleEndian); offset += 4\n    const polygons = []\n    for (let i = 0; i < numPolygons; i++) {\n      // Each polygon has its own byte order & geometry type\n      const polyIsLittleEndian = wkb[offset] === 1; offset += 1\n      const polyType = dv.getUint32(offset, polyIsLittleEndian); offset += 4\n      if (polyType !== geometryTypePolygon) {\n        throw new Error(`Expected Polygon in MultiPolygon, got ${polyType}`)\n      }\n      const numRings = dv.getUint32(offset, polyIsLittleEndian); offset += 4\n\n      const pgCoords = []\n      for (let r = 0; r < numRings; r++) {\n        const numPoints = dv.getUint32(offset, polyIsLittleEndian); offset += 4\n        const ring = []\n        for (let p = 0; p < numPoints; p++) {\n          const x = dv.getFloat64(offset, polyIsLittleEndian); offset += 8\n          const y = dv.getFloat64(offset, polyIsLittleEndian); offset += 8\n          ring.push([x, y])\n        }\n        pgCoords.push(ring)\n      }\n      polygons.push(pgCoords)\n    }\n    return { type: 'MultiPolygon', coordinates: polygons }\n  } else if (geometryType === geometryTypeMultiPoint) {\n    // MultiPoint\n    const numPoints = dv.getUint32(offset, isLittleEndian); offset += 4\n    const points = []\n    for (let i = 0; i < numPoints; i++) {\n      // Each point has its own byte order & geometry type\n      const pointIsLittleEndian = wkb[offset] === 1; offset += 1\n      const pointType = dv.getUint32(offset, pointIsLittleEndian); offset += 4\n      if (pointType !== geometryTypePoint) {\n        throw new Error(`Expected Point in MultiPoint, got ${pointType}`)\n      }\n      const x = dv.getFloat64(offset, pointIsLittleEndian); offset += 8\n      const y = dv.getFloat64(offset, pointIsLittleEndian); offset += 8\n      points.push([x, y])\n    }\n    return { type: 'MultiPoint', coordinates: points }\n  } else if (geometryType === geometryTypeMultiLineString) {\n    // MultiLineString\n    const numLineStrings = dv.getUint32(offset, isLittleEndian); offset += 4\n    const lineStrings = []\n    for (let i = 0; i < numLineStrings; i++) {\n      // Each line has its own byte order & geometry type\n      const lineIsLittleEndian = wkb[offset] === 1; offset += 1\n      const lineType = dv.getUint32(offset, lineIsLittleEndian); offset += 4\n      if (lineType !== geometryTypeLineString) {\n        throw new Error(`Expected LineString in MultiLineString, got ${lineType}`)\n      }\n      const numPoints = dv.getUint32(offset, isLittleEndian); offset += 4\n      const coords = []\n      for (let p = 0; p < numPoints; p++) {\n        const x = dv.getFloat64(offset, lineIsLittleEndian); offset += 8\n        const y = dv.getFloat64(offset, lineIsLittleEndian); offset += 8\n        coords.push([x, y])\n      }\n      lineStrings.push(coords)\n    }\n    return { type: 'MultiLineString', coordinates: lineStrings }\n  } else {\n    throw new Error(`Unsupported geometry type: ${geometryType}`)\n  }\n}\n","import { parquetMetadataAsync, parquetReadObjects } from 'hyparquet'\nimport { decodeWKB } from './wkb.js'\n\n/**\n * Convert a GeoParquet file to GeoJSON.\n * Input is an AsyncBuffer representing a GeoParquet file.\n * An AsyncBuffer is a buffer-like object that can be read asynchronously.\n *\n * @import { AsyncBuffer, Compressors } from 'hyparquet'\n * @import { Feature, GeoJSON } from '../src/geojson.js'\n * @param {Object} options\n * @param {AsyncBuffer} options.file\n * @param {Compressors} [options.compressors]\n * @returns {Promise<GeoJSON>}\n */\nexport async function toGeoJson({ file, compressors }) {\n  const metadata = await parquetMetadataAsync(file)\n  const geoMetadata = metadata.key_value_metadata?.find(kv => kv.key === 'geo')\n  if (!geoMetadata) {\n    throw new Error('Invalid GeoParquet file: missing \"geo\" metadata')\n  }\n\n  // Geoparquet metadata\n  const geoSchema = JSON.parse(geoMetadata.value || '{}')\n\n  // Read all parquet data\n  const data = await parquetReadObjects({ file, metadata, utf8: false, compressors })\n\n  /** @type {Feature[]} */\n  const features = []\n  const primaryColumn = geoSchema.primary_column || 'geometry'\n  for (const row of data) {\n    const wkb = row[primaryColumn]\n    if (!wkb) {\n      // No geometry\n      continue\n    }\n\n    const geometry = decodeWKB(wkb)\n\n    // Extract properties (all fields except geometry)\n    /** @type {Record<string, any>} */\n    const properties = {}\n    for (const key of Object.keys(row)) {\n      const value = row[key]\n      if (key !== primaryColumn && value !== null) {\n        properties[key] = value\n      }\n    }\n\n    /** @type {Feature} */\n    const feature = {\n      type: 'Feature',\n      geometry,\n      properties,\n    }\n\n    features.push(feature)\n  }\n\n  return {\n    type: 'FeatureCollection',\n    features,\n  }\n}\n","import { asyncBufferFromUrl, cachedAsyncBuffer } from 'hyparquet'\nimport { toGeoJson } from '../src/index.js'\n\nasync function initMap() {\n  // @ts-expect-error MapsLibrary\n  const { Map } = await google.maps.importLibrary('maps')\n  const div = /** @type {HTMLElement} */document.getElementById('map')\n  // Create a new map\n  const map = new Map(div, {\n    center: { lat: 39, lng: -98 },\n    zoom: 4,\n  })\n\n  // URL or path to your GeoParquet file\n  const parquetUrl = 'https://hyparam.github.io/geoparquet/demo/polys.parquet'\n\n  try {\n    // Read the GeoParquet file and convert to GeoJSON\n    const file = cachedAsyncBuffer(\n      await asyncBufferFromUrl({ url: parquetUrl, byteLength: 29838 })\n    )\n    console.log('GeoParquet file:', file)\n    const geojson = await toGeoJson({ file })\n\n    console.log('GeoJSON:', geojson)\n\n    // Add the GeoJSON data to the map\n    map.data.addGeoJson(geojson)\n  } catch (error) {\n    console.error('Error loading or parsing GeoParquet file:', error)\n  }\n}\ninitMap()\n"],"names":["ParquetType","Encoding","FieldRepetitionType","ConvertedType","CompressionCodec","PageType","DEFAULT_PARSERS","timestampFromMilliseconds","millis","Date","Number","timestampFromMicroseconds","micros","timestampFromNanoseconds","nanos","dateFromDays","days","convertWithDictionary","data","dictionary","encoding","columnDecoder","endsWith","output","Uint8Array","constructor","length","i","convert","element","parsers","utf8","type","converted_type","ctype","logical_type","ltype","factor","scale","arr","Array","parseDecimal","parseInt96Nanos","decoder","TextDecoder","map","v","JSON","parse","decode","Error","bitWidth","isSigned","BigInt64Array","BigUint64Array","buffer","byteOffset","BigInt","Int32Array","Uint32Array","from","parseFloat16","unit","parser","bytes","value","byte","bits","int16","sign","exp","frac","NaN","Infinity","schemaTree","schema","rootIndex","path","children","count","num_children","childElement","child","name","push","getSchemaPath","tree","part","find","getMaxRepetitionLevel","schemaPath","maxLevel","repetition_type","getMaxDefinitionLevel","slice","CompactType","deserializeTCompactProtocol","reader","lastFid","offset","view","byteLength","fid","newLastFid","readFieldBegin","readElement","getInt8","zigzag","readVarInt","readZigZag","readZigZagBigInt","getFloat64","stringLength","strBytes","elemType","listSize","sizeType","getUint8","size","getCompactType","readCollectionBegin","boolType","values","structValues","structLastFid","structFieldType","structFid","result","shift","readVarBigInt","delta","async","parquetMetadataAsync","asyncBuffer","initialFetchSize","defaultInitialFetchSize","footerOffset","Math","max","footerBuffer","footerView","DataView","getUint32","metadataLength","metadataOffset","metadataBuffer","combinedBuffer","ArrayBuffer","combinedView","set","parquetMetadata","arrayBuffer","metadataLengthOffset","metadata","version","field_1","field_2","field","type_length","field_3","field_4","field_5","field_6","field_7","precision","field_8","field_id","field_9","logicalType","field_10","columnSchema","filter","e","num_rows","row_groups","rowGroup","columns","column","columnIndex","file_path","file_offset","meta_data","encodings","path_in_schema","codec","num_values","total_uncompressed_size","total_compressed_size","key_value_metadata","data_page_offset","index_page_offset","dictionary_page_offset","field_11","statistics","convertStats","field_12","encoding_stats","field_13","encodingStat","page_type","bloom_filter_offset","field_14","bloom_filter_length","field_15","size_statistics","field_16","unencoded_byte_array_data_bytes","repetition_level_histogram","definition_level_histogram","offset_index_offset","offset_index_length","column_index_offset","column_index_length","crypto_metadata","encrypted_column_metadata","total_byte_size","sorting_columns","sortingColumn","column_idx","descending","nulls_first","ordinal","keyValue","key","created_by","metadata_length","isAdjustedToUTC","timeUnit","stats","convertMetadata","min","null_count","distinct_count","max_value","min_value","is_max_value_exact","is_min_value_exact","undefined","getFloat32","getInt32","getBigInt64","concat","aaa","bbb","asyncBufferFromUrl","url","requestInit","fetch","customFetch","globalThis","method","then","res","ok","status","headers","get","parseInt","byteLengthFromUrl","init","start","end","Headers","endStr","body","cachedAsyncBuffer","minSize","cache","Map","cacheKey","cached","promise","flatten","chunks","chunk","getColumnRange","columnOffset","startByte","endByte","assembleLists","definitionLevels","repetitionLevels","n","maxDefinitionLevel","repetitionPath","valueIndex","containerStack","currentContainer","currentDepth","currentDefLevel","currentRepLevel","at","def","rep","pop","newList","assembleNested","subcolumnData","depth","join","optional","nextDepth","firstChild","isListLike","sublist","subDepth","subcolumn","flattenAtDepth","delete","keyChild","valueChild","isMapLike","mapName","keys","out","assembleMaps","invertDepth","struct","childData","inverted","invertStruct","obj","j","Object","deltaBinaryUnpack","int32","blockSize","miniblockPerBlock","outputIndex","valuesPerMiniblock","minDelta","bitWidths","bitpackPos","miniblockCount","mask","ceil","deltaLengthByteArray","lengths","clz32","readRleBitPackedHybrid","width","startOffset","seen","header","readBitPacked","readRle","left","right","byteStreamSplit","typeLength","byteWidth","b","Float32Array","Float64Array","split","subarray","readPlain","fixedLength","bitOffset","readPlainBoolean","align","readPlainInt32","readPlainInt64","low","high","readPlainInt96","readPlainFloat","readPlainDouble","readPlainByteArray","readPlainByteArrayFixed","aligned","WORD_MASK","copyBytes","fromArray","fromPos","toArray","toPos","readDataPage","daph","dataPage","maxRepetitionLevel","readRepetitionLevels","numNulls","readDefinitionLevels","nValues","x","decompressPage","compressedBytes","uncompressed_page_size","compressors","page","customDecompressor","input","inputLength","outputLength","pos","outPos","c","len","isNaN","lengthSize","snappyUncompress","readDataPageV2","ph","daph2","data_page_header_v2","repetition_levels_byte_length","readRepetitionLevelsV2","definition_levels_byte_length","readDefinitionLevelsV2","uncompressedPageSize","is_compressed","pageView","pageReader","num_nulls","prefixData","suffixData","suffix","deltaByteArray","readColumn","groupStart","selectStart","selectEnd","onPage","columnName","lastChunk","rowCount","emitLastChunk","columnData","rowStart","rowEnd","parquetHeader","readPage","lastChunkLength","previousChunk","pageStart","compressed_page_size","data_page_header","isFlatColumn","isArray","diph","dictionary_page_header","crc","definition_level_encoding","repetition_level_encoding","index_page_header","is_sorted","asyncGroupToRows","asyncColumns","rowFormat","groupData","columnDatas","Promise","all","includedColumnNames","pathInSchema","includes","columnOrder","columnIndexes","findIndex","row","rowData","parquetRead","options","file","asyncGroups","plan","groups","fetches","groupRows","groupEnd","ranges","groupSize","isFinite","parquetPlan","promises","index","endOffset","prefetchAsyncBuffer","groupPlan","columnBytes","console","warn","resolve","readRowGroup","parquetReadAsync","onChunk","onComplete","parquetSchema","assembled","arg","asyncRowGroup","childColumns","flatData","flatColumn","asyncColumn","assembleAsync","asyncGroup","rows","decodeWKB","wkb","dv","byteOrder","isLittleEndian","geometryType","y","coordinates","numPoints","coords","numRings","r","ring","p","numPolygons","polygons","polyIsLittleEndian","polyType","pgCoords","points","pointIsLittleEndian","pointType","numLineStrings","lineStrings","lineIsLittleEndian","lineType","toGeoJson","geoMetadata","kv","geoSchema","reject","catch","features","primaryColumn","primary_column","geometry","properties","feature","google","maps","importLibrary","document","getElementById","center","lat","lng","zoom","log","geojson","addGeoJson","error","initMap"],"mappings":"AACO,MAAMA,EAAc,CACzB,UACA,QACA,QACA,QACA,QACA,SACA,aACA,wBAIWC,EAAW,CACtB,QACA,gBACA,mBACA,MACA,aACA,sBACA,0BACA,mBACA,iBACA,qBAIWC,EAAsB,CACjC,WACA,WACA,YAIWC,EAAgB,CAC3B,OACA,MACA,gBACA,OACA,OACA,UACA,OACA,cACA,cACA,mBACA,mBACA,SACA,UACA,UACA,UACA,QACA,SACA,SACA,SACA,OACA,OACA,YAIWC,EAAmB,CAC9B,eACA,SACA,OACA,MACA,SACA,MACA,OACA,WAIWC,EAAW,CACtB,YACA,aACA,kBACA,gBCpEWC,EAAkB,CAC7BC,0BAA0BC,GACjB,IAAIC,KAAKC,OAAOF,IAEzBG,0BAA0BC,GACjB,IAAIH,KAAKC,OAAOE,EAAS,QAElCC,yBAAyBC,GAChB,IAAIL,KAAKC,OAAOI,EAAQ,WAEjCC,aAAaC,GAEJ,IAAIP,KADS,MACJO,IAab,SAASC,EAAsBC,EAAMC,EAAYC,EAAUC,GAChE,GAAIF,GAAcC,EAASE,SAAS,eAAgB,CAClD,IAAIC,EAASL,EACTA,aAAgBM,cAAgBL,aAAsBK,cAExDD,EAAS,IAAIJ,EAAWM,YAAYP,EAAKQ,SAE3C,IAAK,IAAIC,EAAI,EAAGA,EAAIT,EAAKQ,OAAQC,IAC/BJ,EAAOI,GAAKR,EAAWD,EAAKS,IAE9B,OAAOJ,CACT,CACE,OAAOK,EAAQV,EAAMG,EAEzB,CASO,SAASO,EAAQV,EAAMG,GAC5B,MAAMQ,QAAEA,EAAOC,QAAEA,EAAOC,KAAEA,GAAO,GAASV,GACpCW,KAAEA,EAAMC,eAAgBC,EAAOC,aAAcC,GAAUP,EAC7D,GAAc,YAAVK,EAAqB,CACvB,MACMG,EAAS,MADDR,EAAQS,OAAS,GAEzBC,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC1BT,EAAK,aAAcM,WACrBe,EAAIZ,GAAKc,EAAavB,EAAKS,IAAMU,EAEjCE,EAAIZ,GAAKjB,OAAOQ,EAAKS,IAAMU,EAG/B,OAAOE,CACT,CACA,IAAKL,GAAkB,UAATF,EAAkB,CAC9B,MAAMO,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKG,EAAQjB,yBAAyB6B,EAAgBxB,EAAKS,KAEjE,OAAOY,CACT,CACA,GAAc,SAAVL,EAAkB,CACpB,MAAMK,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKG,EAAQf,aAAaG,EAAKS,IAErC,OAAOY,CACT,CACA,GAAc,qBAAVL,EAA8B,CAChC,MAAMK,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKG,EAAQvB,0BAA0BW,EAAKS,IAElD,OAAOY,CACT,CACA,GAAc,qBAAVL,EAA8B,CAChC,MAAMK,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKG,EAAQnB,0BAA0BO,EAAKS,IAElD,OAAOY,CACT,CACA,GAAc,SAAVL,EAAkB,CACpB,MAAMS,EAAU,IAAIC,YACpB,OAAO1B,EAAK2B,IAAIC,GAAKC,KAAKC,MAAML,EAAQM,OAAOH,IACjD,CACA,GAAc,SAAVZ,EACF,MAAM,IAAIgB,MAAM,8BAElB,GAAc,aAAVhB,EACF,MAAM,IAAIgB,MAAM,kCAElB,GAAc,SAAVhB,GAAoC,WAAhBE,GAAOJ,MAAqBD,GAAiB,eAATC,EAAuB,CACjF,MAAMW,EAAU,IAAIC,YACdL,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKT,EAAKS,IAAMgB,EAAQM,OAAO/B,EAAKS,IAE1C,OAAOY,CACT,CACA,GAAc,YAAVL,GAAuC,YAAhBE,GAAOJ,MAAyC,KAAnBI,EAAMe,WAAoBf,EAAMgB,SAAU,CAChG,GAAIlC,aAAgBmC,cAClB,OAAO,IAAIC,eAAepC,EAAKqC,OAAQrC,EAAKsC,WAAYtC,EAAKQ,QAE/D,MAAMa,EAAM,IAAIe,eAAepC,EAAKQ,QACpC,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAAKY,EAAIZ,GAAK8B,OAAOvC,EAAKS,IAC1D,OAAOY,CACT,CACA,GAAc,YAAVL,GAAuC,YAAhBE,GAAOJ,MAAyC,KAAnBI,EAAMe,WAAoBf,EAAMgB,SAAU,CAChG,GAAIlC,aAAgBwC,WAClB,OAAO,IAAIC,YAAYzC,EAAKqC,OAAQrC,EAAKsC,WAAYtC,EAAKQ,QAE5D,MAAMa,EAAM,IAAIoB,YAAYzC,EAAKQ,QACjC,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAAKY,EAAIZ,GAAKT,EAAKS,GACnD,OAAOY,CACT,CACA,GAAoB,YAAhBH,GAAOJ,KACT,OAAOQ,MAAMoB,KAAK1C,GAAM2B,IAAIgB,GAE9B,GAAoB,cAAhBzB,GAAOJ,KAAsB,CAC/B,MAAM8B,KAAEA,GAAS1B,EAEjB,IAAI2B,EAASjC,EAAQvB,0BACR,WAATuD,IAAmBC,EAASjC,EAAQnB,2BAC3B,UAATmD,IAAkBC,EAASjC,EAAQjB,0BACvC,MAAM0B,EAAM,IAAIC,MAAMtB,EAAKQ,QAC3B,IAAK,IAAIC,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC9BY,EAAIZ,GAAKoC,EAAO7C,EAAKS,IAEvB,OAAOY,CACT,CACA,OAAOrB,CACT,CAMO,SAASuB,EAAauB,GAC3B,IAAIC,EAAQ,EACZ,IAAK,MAAMC,KAAQF,EACjBC,EAAgB,IAARA,EAAcC,EAIxB,MAAMC,EAAsB,EAAfH,EAAMtC,OAKnB,OAJIuC,GAAS,IAAME,EAAO,KACxBF,GAAS,GAAKE,GAGTF,CACT,CAOA,SAASvB,EAAgBuB,GAGvB,OAAc,kBAFAA,GAAS,KAAO,WACT,oBAARA,EAEf,CAMO,SAASJ,EAAaG,GAC3B,IAAKA,EAAO,OACZ,MAAMI,EAAQJ,EAAM,IAAM,EAAIA,EAAM,GAC9BK,EAAOD,GAAS,MAAU,EAC1BE,EAAMF,GAAS,GAAK,GACpBG,EAAe,KAARH,EACb,OAAY,IAARE,EAAkBD,EAAO,IAAK,IAAOE,EAAO,MACpC,KAARD,EAAqBC,EAAOC,IAAMH,GAAOI,KACtCJ,EAAO,IAAMC,EAAM,KAAO,EAAIC,EAAO,KAC9C,CC1LA,SAASG,EAAWC,EAAQC,EAAWC,GACrC,MAAMhD,EAAU8C,EAAOC,GACjBE,EAAW,GACjB,IAAIC,EAAQ,EAGZ,GAAIlD,EAAQmD,aACV,KAAOF,EAASpD,OAASG,EAAQmD,cAAc,CAC7C,MAAMC,EAAeN,EAAOC,EAAYG,GAClCG,EAAQR,EAAWC,EAAQC,EAAYG,EAAO,IAAIF,EAAMI,EAAaE,OAC3EJ,GAASG,EAAMH,MACfD,EAASM,KAAKF,EAChB,CAGF,MAAO,CAAEH,QAAOlD,UAASiD,WAAUD,OACrC,CASO,SAASQ,EAAcV,EAAQQ,GACpC,IAAIG,EAAOZ,EAAWC,EAAQ,EAAG,IACjC,MAAME,EAAO,CAACS,GACd,IAAK,MAAMC,KAAQJ,EAAM,CACvB,MAAMD,EAAQI,EAAKR,SAASU,KAAKN,GAASA,EAAMrD,QAAQsD,OAASI,GACjE,IAAKL,EAAO,MAAM,IAAIhC,MAAM,qCAAqCiC,KACjEN,EAAKO,KAAKF,GACVI,EAAOJ,CACT,CACA,OAAOL,CACT,CAQO,SAASY,EAAsBC,GACpC,IAAIC,EAAW,EACf,IAAK,MAAM9D,QAAEA,KAAa6D,EACQ,aAA5B7D,EAAQ+D,iBACVD,IAGJ,OAAOA,CACT,CAQO,SAASE,EAAsBH,GACpC,IAAIC,EAAW,EACf,IAAK,MAAM9D,QAAEA,KAAa6D,EAAWI,MAAM,GACT,aAA5BjE,EAAQ+D,iBACVD,IAGJ,OAAOA,CACT,CC3EO,MAAMI,EACL,EADKA,EAEL,EAFKA,EAGJ,EAHIA,EAIL,EAJKA,EAKN,EALMA,EAMN,EANMA,EAON,EAPMA,EAQH,EARGA,EASH,EATGA,EAUL,EAVKA,EAaH,GAUH,SAASC,EAA4BC,GAC1C,IAAIC,EAAU,EAEd,MAAMjC,EAAQ,CAAA,EAEd,KAAOgC,EAAOE,OAASF,EAAOG,KAAKC,YAAY,CAE7C,MAAOrE,EAAMsE,EAAKC,GAAcC,EAAeP,EAAQC,GAGvD,GAFAA,EAAUK,EAENvE,IAAS+D,EACX,MAIF9B,EAAM,SAASqC,KAASG,EAAYR,EAAQjE,EAC9C,CAEA,OAAOiC,CACT,CAUA,SAASwC,EAAYR,EAAQjE,GAC3B,OAAQA,GACR,KAAK+D,EACH,OAAO,EACT,KAAKA,EACH,OAAO,EACT,KAAKA,EAEH,OAAOE,EAAOG,KAAKM,QAAQT,EAAOE,UACpC,KAAKJ,EACL,KAAKA,EACH,OA0FG,SAAoBE,GACzB,MAAMU,EAASC,EAAWX,GAE1B,OAAOU,IAAW,IAAe,EAATA,EAC1B,CA9FWE,CAAWZ,GACpB,KAAKF,EACH,OAAOe,EAAiBb,GAC1B,KAAKF,EAAoB,CACvB,MAAM9B,EAAQgC,EAAOG,KAAKW,WAAWd,EAAOE,QAAQ,GAEpD,OADAF,EAAOE,QAAU,EACVlC,CACT,CACA,KAAK8B,EAAoB,CACvB,MAAMiB,EAAeJ,EAAWX,GAC1BgB,EAAW,IAAIzF,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQa,GAE5F,OADAf,EAAOE,QAAUa,EACVC,CACT,CACA,KAAKlB,EAAkB,CACrB,MAAOmB,EAAUC,GAsIrB,SAA6BlB,GAC3B,MAAMmB,EAAWnB,EAAOG,KAAKiB,SAASpB,EAAOE,UACvCmB,EAAOF,GAAY,EACnBpF,EAAOuF,EAAeH,GAC5B,GAAa,KAATE,EAAa,CAEf,MAAO,CAACtF,EADQ4E,EAAWX,GAE7B,CACA,MAAO,CAACjE,EAAMsF,EAChB,CA/IiCE,CAAoBvB,GAC3CwB,EAAWP,IAAanB,GAAoBmB,IAAanB,EACzD2B,EAAS,IAAIlF,MAAM2E,GACzB,IAAK,IAAIxF,EAAI,EAAGA,EAAIwF,EAAUxF,IAC5B+F,EAAO/F,GAAK8F,EAAqD,IAA1ChB,EAAYR,EAAQF,GAA0BU,EAAYR,EAAQiB,GAE3F,OAAOQ,CACT,CACA,KAAK3B,EAAoB,CAEvB,MAAM4B,EAAe,CAAA,EACrB,IAAIC,EAAgB,EACpB,OAAa,CACX,IAAIC,EAAiBC,EAErB,IADCD,EAAiBC,EAAWF,GAAiBpB,EAAeP,EAAQ2B,GACjEC,IAAoB9B,EACtB,MAEF4B,EAAa,SAASG,KAAerB,EAAYR,EAAQ4B,EAC3D,CACA,OAAOF,CACT,CAEA,QACE,MAAM,IAAIzE,MAAM,0BAA0BlB,KAE9C,CAUO,SAAS4E,EAAWX,GACzB,IAAI8B,EAAS,EACTC,EAAQ,EACZ,OAAa,CACX,MAAM9D,EAAO+B,EAAOG,KAAKiB,SAASpB,EAAOE,UAEzC,GADA4B,IAAkB,IAAP7D,IAAgB8D,IACd,IAAP9D,GACJ,OAAO6D,EAETC,GAAS,CACX,CACF,CAyCO,SAASlB,EAAiBb,GAC/B,MAAMU,EAlCR,SAAuBV,GACrB,IAAI8B,EAAS,GACTC,EAAQ,GACZ,OAAa,CACX,MAAM9D,EAAO+B,EAAOG,KAAKiB,SAASpB,EAAOE,UAEzC,GADA4B,GAAUtE,OAAc,IAAPS,IAAgB8D,IACpB,IAAP9D,GACJ,OAAO6D,EAETC,GAAS,EACX,CACF,CAuBiBC,CAAchC,GAE7B,OAAOU,GAAU,KAAgB,GAATA,EAC1B,CAQA,SAASY,EAAerD,GACtB,OAAc,GAAPA,CACT,CASA,SAASsC,EAAeP,EAAQC,GAC9B,MAAMlE,EAAOiE,EAAOG,KAAKiB,SAASpB,EAAOE,UACzC,IAAY,GAAPnE,KAAiB+D,EAEpB,MAAO,CAAC,EAAG,EAAGG,GAEhB,MAAMgC,EAAQlG,GAAQ,EACtB,IAAIsE,EACJ,IAAI4B,EAIF,MAAM,IAAIhF,MAAM,oCAElB,OAJEoD,EAAMJ,EAAUgC,EAIX,CAACX,EAAevF,GAAOsE,EAAKA,EACrC,CC9KO6B,eAAeC,EAAqBC,GAAavG,QAAEA,EAAOwG,iBAAEA,EAAmBC,QAA4B,IAChH,KAAKF,GAAiBA,EAAYhC,YAAc,GAAI,MAAM,IAAInD,MAAM,gCAGpE,MAAMsF,EAAeC,KAAKC,IAAI,EAAGL,EAAYhC,WAAaiC,GACpDK,QAAqBN,EAAYvC,MAAM0C,EAAcH,EAAYhC,YAGjEuC,EAAa,IAAIC,SAASF,GAChC,GAAgE,YAA5DC,EAAWE,UAAUH,EAAatC,WAAa,GAAG,GACpD,MAAM,IAAInD,MAAM,yCAKlB,MAAM6F,EAAiBH,EAAWE,UAAUH,EAAatC,WAAa,GAAG,GACzE,GAAI0C,EAAiBV,EAAYhC,WAAa,EAC5C,MAAM,IAAInD,MAAM,2BAA2B6F,8BAA2CV,EAAYhC,WAAa,KAIjH,GAAI0C,EAAiB,EAAIT,EAAkB,CAEzC,MAAMU,EAAiBX,EAAYhC,WAAa0C,EAAiB,EAC3DE,QAAuBZ,EAAYvC,MAAMkD,EAAgBR,GAEzDU,EAAiB,IAAIC,YAAYJ,EAAiB,GAClDK,EAAe,IAAI5H,WAAW0H,GAGpC,OAFAE,EAAaC,IAAI,IAAI7H,WAAWyH,IAChCG,EAAaC,IAAI,IAAI7H,WAAWmH,GAAeH,EAAeQ,GACvDM,EAAgBJ,EAAgB,CAAEpH,WAC3C,CAEE,OAAOwH,EAAgBX,EAAc,CAAE7G,WAE3C,CASO,SAASwH,EAAgBC,GAAazH,QAAEA,GAAY,CAAA,GACzD,KAAMyH,aAAuBJ,aAAc,MAAM,IAAIjG,MAAM,gCAC3D,MAAMkD,EAAO,IAAIyC,SAASU,GAM1B,GAHAzH,EAAU,IAAKxB,KAAoBwB,GAG/BsE,EAAKC,WAAa,EACpB,MAAM,IAAInD,MAAM,6BAElB,GAAkD,YAA9CkD,EAAK0C,UAAU1C,EAAKC,WAAa,GAAG,GACtC,MAAM,IAAInD,MAAM,yCAKlB,MAAMsG,EAAuBpD,EAAKC,WAAa,EACzC0C,EAAiB3C,EAAK0C,UAAUU,GAAsB,GAC5D,GAAIT,EAAiB3C,EAAKC,WAAa,EAErC,MAAM,IAAInD,MAAM,2BAA2B6F,8BAA2C3C,EAAKC,WAAa,KAG1G,MAEMoD,EAAWzD,EADF,CAAEI,OAAMD,OADAqD,EAAuBT,IAGxCpG,EAAU,IAAIC,YACpB,SAASK,EAAiCgB,GACxC,OAAOA,GAAStB,EAAQM,OAAOgB,EACjC,CAGA,MAAMyF,EAAUD,EAASE,QAEnBhF,EAAS8E,EAASG,QAAQ/G,IAAwBgH,IAAK,CAC3D7H,KAAMhC,EAAY6J,EAAMF,SACxBG,YAAaD,EAAMD,QACnBhE,gBAAiB1F,EAAoB2J,EAAME,SAC3C5E,KAAMlC,EAAO4G,EAAMG,SACnBhF,aAAc6E,EAAMI,QACpBhI,eAAgB9B,EAAc0J,EAAMK,SACpC5H,MAAOuH,EAAMM,QACbC,UAAWP,EAAMQ,QACjBC,SAAUT,EAAMU,QAChBpI,aAAcqI,EAAYX,EAAMY,aAG5BC,EAAe/F,EAAOgG,OAAOC,GAAKA,EAAE5I,MACpC6I,EAAWpB,EAASM,QACpBe,EAAarB,EAASO,QAAQnH,IAAwBkI,IAAQ,CAClEC,QAASD,EAASpB,QAAQ9G,IAAI,CAAoBoI,EAA8BC,KAAW,CACzFC,UAAWlI,EAAOgI,EAAOtB,SACzByB,YAAaH,EAAOrB,QACpByB,UAAWJ,EAAOlB,SAAW,CAC3B/H,KAAMhC,EAAYiL,EAAOlB,QAAQJ,SACjC2B,UAAWL,EAAOlB,QAAQH,SAAS/G,IAA2B+H,GAAM3K,EAAS2K,IAC7EW,eAAgBN,EAAOlB,QAAQA,QAAQlH,IAAII,GAC3CuI,MAAOpL,EAAiB6K,EAAOlB,QAAQC,SACvCyB,WAAYR,EAAOlB,QAAQE,QAC3ByB,wBAAyBT,EAAOlB,QAAQG,QACxCyB,sBAAuBV,EAAOlB,QAAQI,QACtCyB,mBAAoBX,EAAOlB,QAAQM,QACnCwB,iBAAkBZ,EAAOlB,QAAQQ,QACjCuB,kBAAmBb,EAAOlB,QAAQU,SAClCsB,uBAAwBd,EAAOlB,QAAQiC,SACvCC,WAAYC,EAAajB,EAAOlB,QAAQoC,SAAUzB,EAAaQ,GAAcpJ,GAC7EsK,eAAgBnB,EAAOlB,QAAQsC,UAAUxJ,IAAwByJ,IAAY,CAC3EC,UAAWlM,EAASiM,EAAa3C,SACjCvI,SAAUnB,EAASqM,EAAa1C,SAChC7E,MAAOuH,EAAavC,WAEtByC,oBAAqBvB,EAAOlB,QAAQ0C,SACpCC,oBAAqBzB,EAAOlB,QAAQ4C,SACpCC,gBAAiB3B,EAAOlB,QAAQ8C,UAAY,CAC1CC,gCAAiC7B,EAAOlB,QAAQ8C,SAASlD,QACzDoD,2BAA4B9B,EAAOlB,QAAQ8C,SAASjD,QACpDoD,2BAA4B/B,EAAOlB,QAAQ8C,SAAS9C,UAGxDkD,oBAAqBhC,EAAOjB,QAC5BkD,oBAAqBjC,EAAOhB,QAC5BkD,oBAAqBlC,EAAOf,QAC5BkD,oBAAqBnC,EAAOd,QAC5BkD,gBAAiBpC,EAAOZ,QACxBiD,0BAA2BrC,EAAOV,WAEpCgD,gBAAiBxC,EAASnB,QAC1BiB,SAAUE,EAAShB,QACnByD,gBAAiBzC,EAASf,SAASnH,IAAwB4K,IAAa,CACtEC,WAAYD,EAAc9D,QAC1BgE,WAAYF,EAAc7D,QAC1BgE,YAAaH,EAAc1D,WAE7BqB,YAAaL,EAASd,QACtB0B,sBAAuBZ,EAASb,QAChC2D,QAAS9C,EAASZ,WAEdyB,EAAqBnC,EAASQ,SAASpH,IAAwBiL,IAAQ,CAC3EC,IAAK9K,EAAO6K,EAASnE,SACrB1F,MAAOhB,EAAO6K,EAASlE,YAIzB,MAAO,CACLF,UACA/E,SACAkG,WACAC,aACAc,qBACAoC,WARiB/K,EAAOwG,EAASS,SASjC+D,gBAAiBlF,EAErB,CAgBA,SAASyB,EAAYA,GACnB,OAAIA,GAAab,QAAgB,CAAE3H,KAAM,UACrCwI,GAAaZ,QAAgB,CAAE5H,KAAM,OACrCwI,GAAaT,QAAgB,CAAE/H,KAAM,QACrCwI,GAAaR,QAAgB,CAAEhI,KAAM,QACrCwI,GAAaP,QAAgB,CAC/BjI,KAAM,UACNM,MAAOkI,EAAYP,QAAQN,QAC3BS,UAAWI,EAAYP,QAAQL,SAE7BY,GAAaN,QAAgB,CAAElI,KAAM,QACrCwI,GAAaL,QAAgB,CAC/BnI,KAAM,OACNkM,gBAAiB1D,EAAYL,QAAQR,QACrC7F,KAAMqK,EAAS3D,EAAYL,QAAQP,UAEjCY,GAAaH,QAAgB,CAC/BrI,KAAM,YACNkM,gBAAiB1D,EAAYH,QAAQV,QACrC7F,KAAMqK,EAAS3D,EAAYH,QAAQT,UAEjCY,GAAaC,SAAiB,CAChCzI,KAAM,UACNmB,SAAUqH,EAAYC,SAASd,QAC/BvG,SAAUoH,EAAYC,SAASb,SAE7BY,GAAawB,SAAiB,CAAEhK,KAAM,QACtCwI,GAAa2B,SAAiB,CAAEnK,KAAM,QACtCwI,GAAa6B,SAAiB,CAAErK,KAAM,QACtCwI,GAAaiC,SAAiB,CAAEzK,KAAM,QACtCwI,GAAamC,SAAiB,CAAE3K,KAAM,WACnCwI,CACT,CAMA,SAAS2D,EAASrK,GAChB,GAAIA,EAAK6F,QAAS,MAAO,SACzB,GAAI7F,EAAK8F,QAAS,MAAO,SACzB,GAAI9F,EAAKiG,QAAS,MAAO,QACzB,MAAM,IAAI7G,MAAM,6BAClB,CAWA,SAASgJ,EAAakC,EAAOzJ,EAAQ7C,GACnC,OAAOsM,GAAS,CACd1F,IAAK2F,EAAgBD,EAAMzE,QAAShF,EAAQ7C,GAC5CwM,IAAKD,EAAgBD,EAAMxE,QAASjF,EAAQ7C,GAC5CyM,WAAYH,EAAMrE,QAClByE,eAAgBJ,EAAMpE,QACtByE,UAAWJ,EAAgBD,EAAMnE,QAAStF,EAAQ7C,GAClD4M,UAAWL,EAAgBD,EAAMlE,QAASvF,EAAQ7C,GAClD6M,mBAAoBP,EAAMjE,QAC1ByE,mBAAoBR,EAAM/D,QAE9B,CAQO,SAASgE,EAAgBpK,EAAOU,EAAQ7C,GAC7C,MAAME,KAAEA,EAAIC,eAAEA,EAAcE,aAAEA,GAAiBwC,EAC/C,QAAckK,IAAV5K,EAAqB,OAAOA,EAChC,GAAa,YAATjC,EAAoB,OAAoB,IAAbiC,EAAM,GACrC,GAAa,eAATjC,EAAuB,OAAO,IAAIY,aAAcK,OAAOgB,GAC3D,MAAMmC,EAAO,IAAIyC,SAAS5E,EAAMV,OAAQU,EAAMT,WAAYS,EAAMoC,YAChE,MAAa,UAATrE,GAAwC,IAApBoE,EAAKC,WAAyBD,EAAK0I,WAAW,GAAG,GAC5D,WAAT9M,GAAyC,IAApBoE,EAAKC,WAAyBD,EAAKW,WAAW,GAAG,GAC7D,UAAT/E,GAAuC,SAAnBC,EAAkCH,EAAQf,aAAaqF,EAAK2I,SAAS,GAAG,IACnF,UAAT/M,GAAuC,qBAAnBC,EAA8CH,EAAQvB,0BAA0B6F,EAAK4I,YAAY,GAAG,IAC/G,UAAThN,GAAuC,qBAAnBC,EAA8CH,EAAQnB,0BAA0ByF,EAAK4I,YAAY,GAAG,IAC/G,UAAThN,GAA2C,cAAvBG,GAAcH,MAA+C,UAAvBG,GAAc2B,KAAyBhC,EAAQjB,yBAAyBuF,EAAK4I,YAAY,GAAG,IAC7I,UAAThN,GAA2C,cAAvBG,GAAcH,MAA+C,WAAvBG,GAAc2B,KAA0BhC,EAAQnB,0BAA0ByF,EAAK4I,YAAY,GAAG,IAC/I,UAAThN,GAA2C,cAAvBG,GAAcH,KAA6BF,EAAQvB,0BAA0B6F,EAAK4I,YAAY,GAAG,IAC5G,UAAThN,GAAwC,IAApBoE,EAAKC,WAAyBD,EAAK2I,SAAS,GAAG,GAC1D,UAAT/M,GAAwC,IAApBoE,EAAKC,WAAyBD,EAAK4I,YAAY,GAAG,GACnD,YAAnB/M,EAAqCQ,EAAawB,GAAS,MAAQU,EAAOrC,OAAS,GAC5D,YAAvBH,GAAcH,KAA2B6B,EAAaI,GACdA,CAG9C,CC1QO,SAASgL,EAAOC,EAAKC,GAE1B,IAAK,IAAIxN,EAAI,EAAGA,EAAIwN,EAAIzN,OAAQC,GADlB,IAEZuN,EAAI9J,QAAQ+J,EAAIrJ,MAAMnE,EAAGA,EAFb,KAIhB,CAmDOwG,eAAeiH,GAAmBC,IAAEA,EAAGhJ,WAAEA,EAAUiJ,YAAEA,EAAaC,MAAOC,IAE9E,MAAMD,EAAQC,GAAeC,WAAWF,MAQxC,IAAIhM,EANJ8C,UA5BK8B,eAAiCkH,EAAKC,EAAaE,GACxD,MAAMD,EAAQC,GAAeC,WAAWF,MACxC,aAAaA,EAAMF,EAAK,IAAKC,EAAaI,OAAQ,SAC/CC,KAAKC,IACJ,IAAKA,EAAIC,GAAI,MAAM,IAAI3M,MAAM,qBAAqB0M,EAAIE,UACtD,MAAMpO,EAASkO,EAAIG,QAAQC,IAAI,kBAC/B,IAAKtO,EAAQ,MAAM,IAAIwB,MAAM,0BAC7B,OAAO+M,SAASvO,IAEtB,CAmBuBwO,CAAkBb,EAAKC,EAAaC,GAOzD,MAAMY,EAAOb,GAAe,CAAA,EAE5B,MAAO,CACLjJ,aACA,WAAMP,CAAMsK,EAAOC,GACjB,GAAI9M,EACF,OAAOA,EAAOoM,KAAKpM,GAAUA,EAAOuC,MAAMsK,EAAOC,IAGnD,MAAMN,EAAU,IAAIO,QAAQH,EAAKJ,SAC3BQ,OAAiB1B,IAARwB,EAAoB,GAAKA,EAAM,EAC9CN,EAAQ1G,IAAI,QAAS,SAAS+G,KAASG,KAEvC,MAAMX,QAAYL,EAAMF,EAAK,IAAKc,EAAMJ,YACxC,IAAKH,EAAIC,KAAOD,EAAIY,KAAM,MAAM,IAAItN,MAAM,gBAAgB0M,EAAIE,UAE9D,GAAmB,MAAfF,EAAIE,OAGN,OADAvM,EAASqM,EAAIrG,cACNhG,EAAOoM,KAAKpM,GAAUA,EAAOuC,MAAMsK,EAAOC,IAC5C,GAAmB,MAAfT,EAAIE,OAEb,OAAOF,EAAIrG,cAEX,MAAM,IAAIrG,MAAM,yCAAyC0M,EAAIE,SAEjE,EAEJ,CAUO,SAASW,GAAkBpK,WAAEA,EAAUP,MAAEA,IAAS4K,QAAEA,EAAUnI,QAA4B,IAC/F,GAAIlC,EAAaqK,EAAS,CAExB,MAAMnN,EAASuC,EAAM,EAAGO,GACxB,MAAO,CACLA,aACA8B,MAAW,MAACiI,EAAOC,WACH9M,GAAQuC,MAAMsK,EAAOC,GAGzC,CACA,MAAMM,EAAQ,IAAIC,IAClB,MAAO,CACLvK,aAMA,KAAAP,CAAMsK,EAAOC,GACX,MAAMtC,EAsBZ,SAAkBqC,EAAOC,EAAK/I,GAC5B,GAAI8I,EAAQ,EAAG,CACb,QAAYvB,IAARwB,EAAmB,MAAM,IAAInN,MAAM,yBAAyBkN,MAAUC,MAC1E,YAAaxB,IAATvH,EAA2B,GAAG8I,KAC3B,GAAG9I,EAAO8I,KAAS9I,GAC5B,CAAO,QAAYuH,IAARwB,EAAmB,CAC5B,GAAID,EAAQC,EAAK,MAAM,IAAInN,MAAM,wBAAwBkN,MAAUC,MACnE,MAAO,GAAGD,KAASC,GACrB,CAAO,YAAaxB,IAATvH,EACF,GAAG8I,KAEH,GAAGA,KAAS9I,GAEvB,CAnCkBuJ,CAAST,EAAOC,EAAKhK,GAC3ByK,EAASH,EAAMX,IAAIjC,GACzB,GAAI+C,EAAQ,OAAOA,EAEnB,MAAMC,EAAUjL,EAAMsK,EAAOC,GAE7B,OADAM,EAAMtH,IAAI0E,EAAKgD,GACRA,CACT,EAEJ,CAkCO,SAASC,EAAQC,GACtB,IAAKA,EAAQ,MAAO,GACpB,GAAsB,IAAlBA,EAAOvP,OAAc,OAAOuP,EAAO,GAEvC,MAAM1P,EAAS,GACf,IAAK,MAAM2P,KAASD,EAClBhC,EAAO1N,EAAQ2P,GAEjB,OAAO3P,CACT,CC3IO,SAAS4P,GAAepF,uBAAEA,EAAsBF,iBAAEA,EAAgBF,sBAAEA,IACzE,MAAMyF,EAAerF,GAA0BF,EAC/C,MAAO,CACLwF,UAAW3Q,OAAO0Q,GAClBE,QAAS5Q,OAAO0Q,EAAezF,GAEnC,CC/DO,SAAS4F,EAAchQ,EAAQiQ,EAAkBC,EAAkB/J,EAAQhC,GAChF,MAAMgM,EAAIF,GAAkB9P,QAAU+P,EAAiB/P,OACvD,IAAKgQ,EAAG,OAAOhK,EACf,MAAMiK,EAAqB9L,EAAsBH,GAC3CkM,EAAiBlM,EAAW7C,IAAI,EAAGhB,aAAcA,EAAQ+D,iBAC/D,IAAIiM,EAAa,EAGjB,MAAMC,EAAiB,CAACvQ,GACxB,IAAIwQ,EAAmBxQ,EACnByQ,EAAe,EACfC,EAAkB,EAClBC,EAAkB,EAEtB,GAAIT,EAAiB,GAEnB,KAAOO,EAAeJ,EAAelQ,OAAS,GAAKwQ,EAAkBT,EAAiB,IACpFO,IACqC,aAAjCJ,EAAeI,KAEjBD,EAAmBA,EAAiBI,IAAG,GACvCL,EAAe1M,KAAK2M,GACpBE,KAEmC,aAAjCL,EAAeI,IAA8BE,IAIrD,IAAK,IAAIvQ,EAAI,EAAGA,EAAI+P,EAAG/P,IAAK,CAE1B,MAAMyQ,EAAMZ,GAAkB9P,OAAS8P,EAAiB7P,GAAKgQ,EACvDU,EAAMZ,EAAiB9P,GAG7B,KAAOqQ,IAAiBK,EAAMH,GAAoD,aAAjCN,EAAeI,KACzB,aAAjCJ,EAAeI,KACjBF,EAAeQ,MACfL,KAEmC,aAAjCL,EAAeI,IAA8BE,IACjDF,IAMF,IAHAD,EAAmBD,EAAeK,IAAG,IAIlCH,EAAeJ,EAAelQ,OAAS,GAA0C,aAArCkQ,EAAeI,EAAe,MAC1EC,EAAkBG,GAA4C,aAArCR,EAAeI,EAAe,KACxD,CAEA,GADAA,IACqC,aAAjCJ,EAAeI,GAA8B,CAE/C,MAAMO,EAAU,GAChBR,EAAiB3M,KAAKmN,GACtBR,EAAmBQ,EACnBT,EAAe1M,KAAKmN,GACpBN,GACF,CACqC,aAAjCL,EAAeI,IAA8BE,GACnD,CAGIE,IAAQT,EAEVI,EAAiB3M,KAAKsC,EAAOmK,MACpBG,IAAiBJ,EAAelQ,OAAS,EAClDqQ,EAAiB3M,KAAK,MAEtB2M,EAAiB3M,KAAK,GAE1B,CAGA,IAAK7D,EAAOG,OAEV,IAAK,IAAIC,EAAI,EAAGA,EAAIgQ,EAAoBhQ,IAAK,CAE3C,MAAM4Q,EAAU,GAChBR,EAAiB3M,KAAKmN,GACtBR,EAAmBQ,CACrB,CAGF,OAAOhR,CACT,CAUO,SAASiR,EAAeC,EAAe9N,EAAQ+N,EAAQ,GAC5D,MAAM7N,EAAOF,EAAOE,KAAK8N,KAAK,KACxBC,EAA8C,aAAnCjO,EAAO9C,QAAQ+D,gBAC1BiN,EAAYD,EAAWF,EAAQ,EAAIA,EAEzC,GL7BK,SAAoB/N,GACzB,IAAKA,EAAQ,OAAO,EACpB,GAAsC,SAAlCA,EAAO9C,QAAQI,eAA2B,OAAO,EACrD,GAAI0C,EAAOG,SAASpD,OAAS,EAAG,OAAO,EAEvC,MAAMoR,EAAanO,EAAOG,SAAS,GACnC,QAAIgO,EAAWhO,SAASpD,OAAS,IACU,aAAvCoR,EAAWjR,QAAQ+D,eAGzB,CKmBMmN,CAAWpO,GAAS,CACtB,IAAIqO,EAAUrO,EAAOG,SAAS,GAC1BmO,EAAWJ,EACiB,IAA5BG,EAAQlO,SAASpD,SACnBsR,EAAUA,EAAQlO,SAAS,GAC3BmO,KAEFT,EAAeC,EAAeO,EAASC,GAEvC,MAAMC,EAAYF,EAAQnO,KAAK8N,KAAK,KAC9BjL,EAAS+K,EAAczC,IAAIkD,GACjC,IAAKxL,EAAQ,MAAM,IAAIxE,MAAM,sCAI7B,OAHI0P,GAAUO,EAAezL,EAAQgL,GACrCD,EAAcpJ,IAAIxE,EAAM6C,QACxB+K,EAAcW,OAAOF,EAEvB,CAEA,GL7BK,SAAmBvO,GACxB,IAAKA,EAAQ,OAAO,EACpB,GAAsC,QAAlCA,EAAO9C,QAAQI,eAA0B,OAAO,EACpD,GAAI0C,EAAOG,SAASpD,OAAS,EAAG,OAAO,EAEvC,MAAMoR,EAAanO,EAAOG,SAAS,GACnC,GAAmC,IAA/BgO,EAAWhO,SAASpD,OAAc,OAAO,EAC7C,GAA2C,aAAvCoR,EAAWjR,QAAQ+D,gBAAgC,OAAO,EAE9D,MAAMyN,EAAWP,EAAWhO,SAASU,KAAKN,GAAgC,QAAvBA,EAAMrD,QAAQsD,MACjE,GAA0C,aAAtCkO,GAAUxR,QAAQ+D,gBAAgC,OAAO,EAE7D,MAAM0N,EAAaR,EAAWhO,SAASU,KAAKN,GAAgC,UAAvBA,EAAMrD,QAAQsD,MACnE,MAA4C,aAAxCmO,GAAYzR,QAAQ+D,eAG1B,CKaM2N,CAAU5O,GAAS,CACrB,MAAM6O,EAAU7O,EAAOG,SAAS,GAAGjD,QAAQsD,KAG3CqN,EAAeC,EAAe9N,EAAOG,SAAS,GAAGA,SAAS,GAAI+N,EAAY,GAC1EL,EAAeC,EAAe9N,EAAOG,SAAS,GAAGA,SAAS,GAAI+N,EAAY,GAE1E,MAAMY,EAAOhB,EAAczC,IAAI,GAAGnL,KAAQ2O,SACpC9L,EAAS+K,EAAczC,IAAI,GAAGnL,KAAQ2O,WAE5C,IAAKC,EAAM,MAAM,IAAIvQ,MAAM,mCAC3B,IAAKwE,EAAQ,MAAM,IAAIxE,MAAM,qCAC7B,GAAIuQ,EAAK/R,SAAWgG,EAAOhG,OACzB,MAAM,IAAIwB,MAAM,gDAGlB,MAAMwQ,EAAMC,EAAaF,EAAM/L,EAAQmL,GAMvC,OALID,GAAUO,EAAeO,EAAKhB,GAElCD,EAAcW,OAAO,GAAGvO,KAAQ2O,SAChCf,EAAcW,OAAO,GAAGvO,KAAQ2O,gBAChCf,EAAcpJ,IAAIxE,EAAM6O,EAE1B,CAGA,GAAI/O,EAAOG,SAASpD,OAAQ,CAE1B,MAAMkS,EAAiD,aAAnCjP,EAAO9C,QAAQ+D,gBAAiC8M,EAAQA,EAAQ,EAE9EmB,EAAS,CAAA,EACf,IAAK,MAAM3O,KAASP,EAAOG,SAAU,CACnC0N,EAAeC,EAAevN,EAAO0O,GACrC,MAAME,EAAYrB,EAAczC,IAAI9K,EAAML,KAAK8N,KAAK,MACpD,IAAKmB,EAAW,MAAM,IAAI5Q,MAAM,qCAChC2Q,EAAO3O,EAAMrD,QAAQsD,MAAQ2O,CAC/B,CAEA,IAAK,MAAM5O,KAASP,EAAOG,SACzB2N,EAAcW,OAAOlO,EAAML,KAAK8N,KAAK,MAGvC,MAAMoB,EAAWC,EAAaH,EAAQD,GAClChB,GAAUO,EAAeY,EAAUrB,GACvCD,EAAcpJ,IAAIxE,EAAMkP,EAC1B,CACF,CAOA,SAASZ,EAAe5Q,EAAKmQ,GAC3B,IAAK,IAAI/Q,EAAI,EAAGA,EAAIY,EAAIb,OAAQC,IAC1B+Q,EACFS,EAAe5Q,EAAIZ,GAAI+Q,EAAQ,GAE/BnQ,EAAIZ,GAAKY,EAAIZ,GAAG,EAGtB,CAQA,SAASgS,EAAaF,EAAM/L,EAAQgL,GAClC,MAAMgB,EAAM,GACZ,IAAK,IAAI/R,EAAI,EAAGA,EAAI8R,EAAK/R,OAAQC,IAC/B,GAAI+Q,EACFgB,EAAItO,KAAKuO,EAAaF,EAAK9R,GAAI+F,EAAO/F,GAAI+Q,EAAQ,SAElD,GAAIe,EAAK9R,GAAI,CAEX,MAAMsS,EAAM,CAAA,EACZ,IAAK,IAAIC,EAAI,EAAGA,EAAIT,EAAK9R,GAAGD,OAAQwS,IAAK,CACvC,MAAMjQ,EAAQyD,EAAO/F,GAAGuS,GACxBD,EAAIR,EAAK9R,GAAGuS,SAAgBrF,IAAV5K,EAAsB,KAAOA,CACjD,CACAyP,EAAItO,KAAK6O,EACX,MACEP,EAAItO,UAAKyJ,GAIf,OAAO6E,CACT,CASA,SAASM,EAAaH,EAAQnB,GAC5B,MAAMe,EAAOU,OAAOV,KAAKI,GACnBnS,EAASmS,EAAOJ,EAAK,KAAK/R,OAC1BgS,EAAM,GACZ,IAAK,IAAI/R,EAAI,EAAGA,EAAID,EAAQC,IAAK,CAE/B,MAAMsS,EAAM,CAAA,EACZ,IAAK,MAAMlG,KAAO0F,EAAM,CACtB,GAAII,EAAO9F,GAAKrM,SAAWA,EAAQ,MAAM,IAAIwB,MAAM,gCACnD+Q,EAAIlG,GAAO8F,EAAO9F,GAAKpM,EACzB,CACI+Q,EACFgB,EAAItO,KAAK4O,EAAaC,EAAKvB,EAAQ,IAEnCgB,EAAItO,KAAK6O,EAEb,CACA,OAAOP,CACT,CC/OO,SAASU,EAAkBnO,EAAQlB,EAAOxD,GAC/C,MAAM8S,EAAQ9S,aAAkBmC,WAC1B4Q,EAAY1N,EAAWX,GACvBsO,EAAoB3N,EAAWX,GACrCW,EAAWX,GACX,IAAIhC,EAAQ6C,EAAiBb,GACzBuO,EAAc,EAClBjT,EAAOiT,KAAiBH,EAAQ3T,OAAOuD,GAASA,EAEhD,MAAMwQ,EAAqBH,EAAYC,EAEvC,KAAOC,EAAczP,GAAO,CAE1B,MAAM2P,EAAW5N,EAAiBb,GAC5B0O,EAAY,IAAInT,WAAW+S,GACjC,IAAK,IAAI5S,EAAI,EAAGA,EAAI4S,EAAmB5S,IACrCgT,EAAUhT,GAAKsE,EAAOG,KAAKiB,SAASpB,EAAOE,UAG7C,IAAK,IAAIxE,EAAI,EAAGA,EAAI4S,GAAqBC,EAAczP,EAAOpD,IAAK,CAEjE,MAAMwB,EAAWM,OAAOkR,EAAUhT,IAClC,GAAIwB,EAAU,CACZ,IAAIyR,EAAa,GACbC,EAAiBJ,EACrB,MAAMK,GAAQ,IAAM3R,GAAY,GAChC,KAAO0R,GAAkBL,EAAczP,GAAO,CAC5C,IAAIZ,EAAOV,OAAOwC,EAAOG,KAAKiB,SAASpB,EAAOE,UAAYyO,EAAaE,EAEvE,IADAF,GAAczR,EACPyR,GAAc,GACnBA,GAAc,GACd3O,EAAOE,SACHyO,IACFzQ,GAAQV,OAAOwC,EAAOG,KAAKiB,SAASpB,EAAOE,UAAYhD,EAAWyR,EAAaE,GAInF7Q,GADcyQ,EAAWvQ,EAEzB5C,EAAOiT,KAAiBH,EAAQ3T,OAAOuD,GAASA,EAChD4Q,GACF,CACIA,IAEF5O,EAAOE,QAAUsC,KAAKsM,MAAMF,EAAiBnU,OAAOyC,GAAYzC,OAAOkU,IAAe,GAE1F,MACE,IAAK,IAAIV,EAAI,EAAGA,EAAIO,GAAsBD,EAAczP,EAAOmP,IAC7DjQ,GAASyQ,EACTnT,EAAOiT,KAAiBH,EAAQ3T,OAAOuD,GAASA,CAGtD,CACF,CACF,CAOO,SAAS+Q,EAAqB/O,EAAQlB,EAAOxD,GAClD,MAAM0T,EAAU,IAAIvR,WAAWqB,GAC/BqP,EAAkBnO,EAAQlB,EAAOkQ,GACjC,IAAK,IAAItT,EAAI,EAAGA,EAAIoD,EAAOpD,IACzBJ,EAAOI,GAAK,IAAIH,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQ8O,EAAQtT,IAC/FsE,EAAOE,QAAU8O,EAAQtT,EAE7B,CCnEO,SAASwB,EAASc,GACvB,OAAO,GAAKwE,KAAKyM,MAAMjR,EACzB,CAYO,SAASkR,EAAuBlP,EAAQmP,EAAO7T,EAAQG,QAC7CmN,IAAXnN,IACFA,EAASuE,EAAOG,KAAK0C,UAAU7C,EAAOE,QAAQ,GAC9CF,EAAOE,QAAU,GAEnB,MAAMkP,EAAcpP,EAAOE,OAC3B,IAAImP,EAAO,EACX,KAAOA,EAAO/T,EAAOG,QAAQ,CAC3B,MAAM6T,EAAS3O,EAAWX,GAC1B,GAAa,EAATsP,EAEFD,EAAOE,GAAcvP,EAAQsP,EAAQH,EAAO7T,EAAQ+T,OAC/C,CAEL,MAAMvQ,EAAQwQ,IAAW,EACzBE,EAAQxP,EAAQlB,EAAOqQ,EAAO7T,EAAQ+T,GACtCA,GAAQvQ,CACV,CACF,CACAkB,EAAOE,OAASkP,EAAc3T,CAChC,CAWA,SAAS+T,EAAQxP,EAAQlB,EAAO5B,EAAU5B,EAAQ+T,GAChD,MAAMF,EAAQjS,EAAW,GAAK,EAC9B,IAAIc,EAAQ,EACZ,IAAK,IAAItC,EAAI,EAAGA,EAAIyT,EAAOzT,IACzBsC,GAASgC,EAAOG,KAAKiB,SAASpB,EAAOE,YAAcxE,GAAK,GAK1D,IAAK,IAAIA,EAAI,EAAGA,EAAIoD,EAAOpD,IACzBJ,EAAO+T,EAAO3T,GAAKsC,CAEvB,CAaA,SAASuR,GAAcvP,EAAQsP,EAAQpS,EAAU5B,EAAQ+T,GACvD,IAAIvQ,EAAQwQ,GAAU,GAAK,EAC3B,MAAMT,GAAQ,GAAK3R,GAAY,EAE/B,IAAIjC,EAAO,EACX,GAAI+E,EAAOE,OAASF,EAAOG,KAAKC,WAC9BnF,EAAO+E,EAAOG,KAAKiB,SAASpB,EAAOE,eAC9B,GAAI2O,EAET,MAAM,IAAI5R,MAAM,0BAA0B+C,EAAOE,uBAEnD,IAAIuP,EAAO,EACPC,EAAQ,EAGZ,KAAO5Q,GAED4Q,EAAQ,GACVA,GAAS,EACTD,GAAQ,EACRxU,KAAU,GACDwU,EAAOC,EAAQxS,GAExBjC,GAAQ+E,EAAOG,KAAKiB,SAASpB,EAAOE,SAAWuP,EAC/CzP,EAAOE,SACPuP,GAAQ,IAEJJ,EAAO/T,EAAOG,SAEhBH,EAAO+T,KAAUpU,GAAQyU,EAAQb,GAEnC/P,IACA4Q,GAASxS,GAIb,OAAOmS,CACT,CASO,SAASM,GAAgB3P,EAAQlB,EAAO/C,EAAM6T,GACnD,MAAMT,EA6BR,SAAmBpT,EAAM6T,GACvB,OAAQ7T,GACR,IAAK,QACL,IAAK,QACH,OAAO,EACT,IAAK,QACL,IAAK,SACH,OAAO,EACT,IAAK,uBACH,IAAK6T,EAAY,MAAM,IAAI3S,MAAM,yCACjC,OAAO2S,EACT,QACE,MAAM,IAAI3S,MAAM,6BAA6BlB,KAEjD,CA3CgB8T,CAAU9T,EAAM6T,GACxB7R,EAAQ,IAAIxC,WAAWuD,EAAQqQ,GACrC,IAAK,IAAIW,EAAI,EAAGA,EAAIX,EAAOW,IACzB,IAAK,IAAIpU,EAAI,EAAGA,EAAIoD,EAAOpD,IACzBqC,EAAMrC,EAAIyT,EAAQW,GAAK9P,EAAOG,KAAKiB,SAASpB,EAAOE,UAIvD,GAAa,UAATnE,EAAkB,OAAO,IAAIgU,aAAahS,EAAMT,QAC/C,GAAa,WAATvB,EAAmB,OAAO,IAAIiU,aAAajS,EAAMT,QACrD,GAAa,UAATvB,EAAkB,OAAO,IAAI0B,WAAWM,EAAMT,QAClD,GAAa,UAATvB,EAAkB,OAAO,IAAIqB,cAAcW,EAAMT,QACrD,GAAa,yBAATvB,EAAiC,CAExC,MAAMkU,EAAQ,IAAI1T,MAAMuC,GACxB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IACzBuU,EAAMvU,GAAKqC,EAAMmS,SAASxU,EAAIyT,GAAQzT,EAAI,GAAKyT,GAEjD,OAAOc,CACT,CACA,MAAM,IAAIhT,MAAM,+CAA+ClB,IACjE,CCzIO,SAASoU,GAAUnQ,EAAQjE,EAAM+C,EAAOsR,GAC7C,GAAc,IAAVtR,EAAa,MAAO,GACxB,GAAa,YAAT/C,EACF,OA4BJ,SAA0BiE,EAAQlB,GAChC,MAAM2C,EAAS,IAAIlF,MAAMuC,GACzB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IAAK,CAC9B,MAAM6B,EAAayC,EAAOE,QAAUxE,EAAI,EAAI,GACtC2U,EAAY3U,EAAI,EAChBuC,EAAO+B,EAAOG,KAAKiB,SAAS7D,GAClCkE,EAAO/F,MAAMuC,EAAO,GAAKoS,EAC3B,CAEA,OADArQ,EAAOE,QAAUsC,KAAKsM,KAAKhQ,EAAQ,GAC5B2C,CACT,CAtCW6O,CAAiBtQ,EAAQlB,GAC3B,GAAa,UAAT/C,EACT,OA6CJ,SAAwBiE,EAAQlB,GAC9B,MAAM2C,GAAUzB,EAAOG,KAAK5C,WAAayC,EAAOE,QAAU,EACtD,IAAIzC,WAAW8S,GAAMvQ,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAgB,EAARpB,IACjF,IAAIrB,WAAWuC,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQpB,GAE/E,OADAkB,EAAOE,QAAkB,EAARpB,EACV2C,CACT,CAnDW+O,CAAexQ,EAAQlB,GACzB,GAAa,UAAT/C,EACT,OA0DJ,SAAwBiE,EAAQlB,GAC9B,MAAM2C,GAAUzB,EAAOG,KAAK5C,WAAayC,EAAOE,QAAU,EACtD,IAAI9C,cAAcmT,GAAMvQ,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAgB,EAARpB,IACpF,IAAI1B,cAAc4C,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQpB,GAElF,OADAkB,EAAOE,QAAkB,EAARpB,EACV2C,CACT,CAhEWgP,CAAezQ,EAAQlB,GACzB,GAAa,UAAT/C,EACT,OAuEJ,SAAwBiE,EAAQlB,GAC9B,MAAM2C,EAAS,IAAIlF,MAAMuC,GACzB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IAAK,CAC9B,MAAMgV,EAAM1Q,EAAOG,KAAK4I,YAAY/I,EAAOE,OAAa,GAAJxE,GAAQ,GACtDiV,EAAO3Q,EAAOG,KAAK2I,SAAS9I,EAAOE,OAAa,GAAJxE,EAAS,GAAG,GAC9D+F,EAAO/F,GAAK8B,OAAOmT,IAAS,IAAMD,CACpC,CAEA,OADA1Q,EAAOE,QAAkB,GAARpB,EACV2C,CACT,CAhFWmP,CAAe5Q,EAAQlB,GACzB,GAAa,UAAT/C,EACT,OAuFJ,SAAwBiE,EAAQlB,GAC9B,MAAM2C,GAAUzB,EAAOG,KAAK5C,WAAayC,EAAOE,QAAU,EACtD,IAAI6P,aAAaQ,GAAMvQ,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAgB,EAARpB,IACnF,IAAIiR,aAAa/P,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQpB,GAEjF,OADAkB,EAAOE,QAAkB,EAARpB,EACV2C,CACT,CA7FWoP,CAAe7Q,EAAQlB,GACzB,GAAa,WAAT/C,EACT,OAoGJ,SAAyBiE,EAAQlB,GAC/B,MAAM2C,GAAUzB,EAAOG,KAAK5C,WAAayC,EAAOE,QAAU,EACtD,IAAI8P,aAAaO,GAAMvQ,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAgB,EAARpB,IACnF,IAAIkR,aAAahQ,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQpB,GAEjF,OADAkB,EAAOE,QAAkB,EAARpB,EACV2C,CACT,CA1GWqP,CAAgB9Q,EAAQlB,GAC1B,GAAa,eAAT/C,EACT,OAiHJ,SAA4BiE,EAAQlB,GAClC,MAAM2C,EAAS,IAAIlF,MAAMuC,GACzB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IAAK,CAC9B,MAAMD,EAASuE,EAAOG,KAAK0C,UAAU7C,EAAOE,QAAQ,GACpDF,EAAOE,QAAU,EACjBuB,EAAO/F,GAAK,IAAIH,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQzE,GACvFuE,EAAOE,QAAUzE,CACnB,CACA,OAAOgG,CACT,CA1HWsP,CAAmB/Q,EAAQlB,GAC7B,GAAa,yBAAT/C,EAAiC,CAC1C,IAAKqU,EAAa,MAAM,IAAInT,MAAM,gCAClC,OAiIJ,SAAiC+C,EAAQlB,EAAOsR,GAE9C,MAAM3O,EAAS,IAAIlF,MAAMuC,GACzB,IAAK,IAAIpD,EAAI,EAAGA,EAAIoD,EAAOpD,IACzB+F,EAAO/F,GAAK,IAAIH,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQkQ,GACvFpQ,EAAOE,QAAUkQ,EAEnB,OAAO3O,CACT,CAzIWuP,CAAwBhR,EAAQlB,EAAOsR,EAChD,CACE,MAAM,IAAInT,MAAM,2BAA2BlB,IAE/C,CAgJA,SAASwU,GAAMjT,EAAQ4C,EAAQmB,GAC7B,MAAM4P,EAAU,IAAI/N,YAAY7B,GAEhC,OADA,IAAI9F,WAAW0V,GAAS7N,IAAI,IAAI7H,WAAW+B,EAAQ4C,EAAQmB,IACpD4P,CACT,CC7KA,MAAMC,GAAY,CAAC,EAAG,IAAM,MAAQ,SAAU,YAW9C,SAASC,GAAUC,EAAWC,EAASC,EAASC,EAAO9V,GACrD,IAAK,IAAIC,EAAI,EAAGA,EAAID,EAAQC,IAC1B4V,EAAQC,EAAQ7V,GAAK0V,EAAUC,EAAU3V,EAE7C,CCPO,SAAS8V,GAAazT,EAAO0T,GAAM1V,KAAEA,EAAIH,QAAEA,EAAO6D,WAAEA,IACzD,MAAMU,EAAO,IAAIyC,SAAS7E,EAAMT,OAAQS,EAAMR,WAAYQ,EAAMqC,YAC1DJ,EAAS,CAAEG,OAAMD,OAAQ,GAE/B,IAAIwR,EAGJ,MAAMlG,EAkDR,SAA8BxL,EAAQyR,EAAMhS,GAC1C,GAAIA,EAAWhE,OAAS,EAAG,CACzB,MAAMkW,EAAqBnS,EAAsBC,GACjD,GAAIkS,EAAoB,CACtB,MAAMlQ,EAAS,IAAIlF,MAAMkV,EAAKjM,YAE9B,OADA0J,EAAuBlP,EAAQ9C,EAASyU,GAAqBlQ,GACtDA,CACT,CACF,CACA,MAAO,EACT,CA5D2BmQ,CAAqB5R,EAAQyR,EAAMhS,IAEtD8L,iBAAEA,EAAgBsG,SAAEA,GAkE5B,SAA8B7R,EAAQyR,EAAMhS,GAC1C,MAAMiM,EAAqB9L,EAAsBH,GACjD,IAAKiM,EAAoB,MAAO,CAAEH,iBAAkB,GAAIsG,SAAU,GAElE,MAAMtG,EAAmB,IAAIhP,MAAMkV,EAAKjM,YACxC0J,EAAuBlP,EAAQ9C,EAASwO,GAAqBH,GAG7D,IAAIsG,EAAWJ,EAAKjM,WACpB,IAAK,MAAM2G,KAAOZ,EACZY,IAAQT,GAAoBmG,IAEjB,IAAbA,IAAgBtG,EAAiB9P,OAAS,GAE9C,MAAO,CAAE8P,mBAAkBsG,WAC7B,CAjFyCC,CAAqB9R,EAAQyR,EAAMhS,GAIpEsS,EAAUN,EAAKjM,WAAaqM,EAClC,GAAsB,UAAlBJ,EAAKtW,SACPuW,EAAWvB,GAAUnQ,EAAQjE,EAAMgW,EAASnW,EAAQiI,kBAC/C,GACa,qBAAlB4N,EAAKtW,UACa,mBAAlBsW,EAAKtW,UACa,QAAlBsW,EAAKtW,SACL,CACA,MAAM+B,EAAoB,YAATnB,EAAqB,EAAIoE,EAAKiB,SAASpB,EAAOE,UAC3DhD,GACFwU,EAAW,IAAInV,MAAMwV,GACR,YAAThW,GACFmT,EAAuBlP,EAAQ9C,EAAUwU,GACzCA,EAAWA,EAAS9U,IAAIoV,KAAOA,IAG/B9C,EAAuBlP,EAAQ9C,EAAUwU,EAAUvR,EAAKC,WAAaJ,EAAOE,SAG9EwR,EAAW,IAAInW,WAAWwW,EAE9B,MAAO,GAAsB,sBAAlBN,EAAKtW,SACduW,EAAW/B,GAAgB3P,EAAQ+R,EAAShW,EAAMH,EAAQiI,kBACrD,GAAsB,wBAAlB4N,EAAKtW,SAAoC,CAElDuW,EADuB,UAAT3V,EACK,IAAI0B,WAAWsU,GAAW,IAAI3U,cAAc2U,GAC/D5D,EAAkBnO,EAAQ+R,EAASL,EACrC,KAAO,IAAsB,4BAAlBD,EAAKtW,SAId,MAAM,IAAI8B,MAAM,iCAAiCwU,EAAKtW,YAHtDuW,EAAW,IAAInV,MAAMwV,GACrBhD,EAAqB/O,EAAQ+R,EAASL,EAGxC,CAEA,MAAO,CAAEnG,mBAAkBC,mBAAkBkG,WAC/C,CAmDO,SAASO,GAAeC,EAAiBC,EAAwB5M,EAAO6M,GAE7E,IAAIC,EACJ,MAAMC,EAAqBF,IAAc7M,GACzC,GAAc,iBAAVA,EACF8M,EAAOH,OACF,GAAII,EACTD,EAAOC,EAAmBJ,EAAiBC,OACtC,IAAc,WAAV5M,EAIT,MAAM,IAAItI,MAAM,0CAA0CsI,KAH1D8M,EAAO,IAAI9W,WAAW4W,GD5FnB,SAA0BI,EAAOjX,GACtC,MAAMkX,EAAcD,EAAMnS,WACpBqS,EAAenX,EAAO8E,WAC5B,IAAIsS,EAAM,EACNC,EAAS,EAGb,KAAOD,EAAMF,GAAa,CACxB,MAAMI,EAAIL,EAAMG,GAEhB,GADAA,IACIE,EAAI,IACN,KAEJ,CACA,GAAIH,GAAgBC,GAAOF,EACzB,MAAM,IAAIvV,MAAM,gCAGlB,KAAOyV,EAAMF,GAAa,CACxB,MAAMI,EAAIL,EAAMG,GAChB,IAAIG,EAAM,EAGV,GAFAH,IAEIA,GAAOF,EACT,MAAM,IAAIvV,MAAM,sBAIlB,GAAS,EAAJ2V,EAsBE,CAEL,IAAI1S,EAAS,EACb,OAAY,EAAJ0S,GACR,KAAK,EAEHC,EAAwB,GAAjBD,IAAM,EAAI,GACjB1S,EAASqS,EAAMG,IAAQE,IAAM,GAAK,GAClCF,IACA,MACF,KAAK,EAEH,GAAIF,GAAeE,EAAM,EACvB,MAAM,IAAIzV,MAAM,6BAElB4V,GAAOD,IAAM,GAAK,EAClB1S,EAASqS,EAAMG,IAAQH,EAAMG,EAAM,IAAM,GACzCA,GAAO,EACP,MACF,KAAK,EAEH,GAAIF,GAAeE,EAAM,EACvB,MAAM,IAAIzV,MAAM,6BAElB4V,GAAOD,IAAM,GAAK,EAClB1S,EAASqS,EAAMG,IACVH,EAAMG,EAAM,IAAM,IAClBH,EAAMG,EAAM,IAAM,KAClBH,EAAMG,EAAM,IAAM,IACvBA,GAAO,EAKT,GAAe,IAAXxS,GAAgB4S,MAAM5S,GACxB,MAAM,IAAIjD,MAAM,kBAAkBiD,SAAcwS,iBAAmBF,KAErE,GAAItS,EAASyS,EACX,MAAM,IAAI1V,MAAM,2CAElBkU,GAAU7V,EAAQqX,EAASzS,EAAQ5E,EAAQqX,EAAQE,GACnDF,GAAUE,CACZ,KAhEqB,CAEnB,IAAIA,GAAOD,IAAM,GAAK,EAEtB,GAAIC,EAAM,GAAI,CACZ,GAAIH,EAAM,GAAKF,EACb,MAAM,IAAIvV,MAAM,+CAElB,MAAM8V,EAAaF,EAAM,GACzBA,EAAMN,EAAMG,IACPH,EAAMG,EAAM,IAAM,IAClBH,EAAMG,EAAM,IAAM,KAClBH,EAAMG,EAAM,IAAM,IACvBG,EAAsC,GAA/BA,EAAM3B,GAAU6B,IACvBL,GAAOK,CACT,CACA,GAAIL,EAAMG,EAAML,EACd,MAAM,IAAIvV,MAAM,6CAElBkU,GAAUoB,EAAOG,EAAKpX,EAAQqX,EAAQE,GACtCH,GAAOG,EACPF,GAAUE,CACZ,CA2CF,CAEA,GAAIF,IAAWF,EAAc,MAAM,IAAIxV,MAAM,yBAC/C,CCHI+V,CAAiBd,EAAiBG,EAGpC,CACA,GAAIA,GAAM5W,SAAW0W,EACnB,MAAM,IAAIlV,MAAM,oCAAoCoV,GAAM5W,gCAAgC0W,KAE5F,OAAOE,CACT,CAWO,SAASY,GAAef,EAAiBgB,EAAI9X,GAClD,MACM4E,EAAS,CAAEG,KADJ,IAAIyC,SAASsP,EAAgB5U,OAAQ4U,EAAgB3U,WAAY2U,EAAgB9R,YACvEF,OAAQ,IACzBnE,KAAEA,EAAIH,QAAEA,EAAO6D,WAAEA,EAAU8F,MAAEA,EAAK6M,YAAEA,GAAgBhX,EACpD+X,EAAQD,EAAGE,oBACjB,IAAKD,EAAO,MAAM,IAAIlW,MAAM,4CAG5B,MAAMuO,EA2DR,SAAgCxL,EAAQmT,EAAO1T,GAC7C,MAAMkS,EAAqBnS,EAAsBC,GACjD,IAAKkS,EAAoB,MAAO,GAEhC,MAAMlQ,EAAS,IAAIlF,MAAM4W,EAAM3N,YAE/B,OADA0J,EAAuBlP,EAAQ9C,EAASyU,GAAqBlQ,EAAQ0R,EAAME,+BACpE5R,CACT,CAlE2B6R,CAAuBtT,EAAQmT,EAAO1T,GAC/DO,EAAOE,OAASiT,EAAME,8BAGtB,MAAM9H,EAsER,SAAgCvL,EAAQmT,EAAO1T,GAC7C,MAAMiM,EAAqB9L,EAAsBH,GACjD,GAAIiM,EAAoB,CAEtB,MAAMjK,EAAS,IAAIlF,MAAM4W,EAAM3N,YAE/B,OADA0J,EAAuBlP,EAAQ9C,EAASwO,GAAqBjK,EAAQ0R,EAAMI,+BACpE9R,CACT,CACF,CA9E2B+R,CAAuBxT,EAAQmT,EAAO1T,GAGzDgU,EAAuBP,EAAGf,uBAAyBgB,EAAMI,8BAAgCJ,EAAME,8BAErG,IAAIhB,EAAOH,EAAgBhC,SAASlQ,EAAOE,SACf,IAAxBiT,EAAMO,gBACRrB,EAAOJ,GAAeI,EAAMoB,EAAsBlO,EAAO6M,IAE3D,MAAMuB,EAAW,IAAI/Q,SAASyP,EAAK/U,OAAQ+U,EAAK9U,WAAY8U,EAAKjS,YAC3DwT,EAAa,CAAEzT,KAAMwT,EAAUzT,OAAQ,GAI7C,IAAIwR,EACJ,MAAMK,EAAUoB,EAAM3N,WAAa2N,EAAMU,UACzC,GAAuB,UAAnBV,EAAMhY,SACRuW,EAAWvB,GAAUyD,EAAY7X,EAAMgW,EAASnW,EAAQiI,kBACnD,GAAuB,QAAnBsP,EAAMhY,SAEfuW,EAAW,IAAInV,MAAMwV,GACrB7C,EAAuB0E,EAAY,EAAGlC,GACtCA,EAAWA,EAAS9U,IAAIoV,KAAOA,QAC1B,GACc,qBAAnBmB,EAAMhY,UACa,mBAAnBgY,EAAMhY,SACN,CACA,MAAM+B,EAAWyW,EAASvS,SAASwS,EAAW1T,UAC9CwR,EAAW,IAAInV,MAAMwV,GACrB7C,EAAuB0E,EAAY1W,EAAUwU,EAAU+B,EAAuB,EAChF,MAAO,GAAuB,wBAAnBN,EAAMhY,SAAoC,CAEnDuW,EADuB,UAAT3V,EACK,IAAI0B,WAAWsU,GAAW,IAAI3U,cAAc2U,GAC/D5D,EAAkByF,EAAY7B,EAASL,EACzC,MAAO,GAAuB,4BAAnByB,EAAMhY,SACfuW,EAAW,IAAInV,MAAMwV,GACrBhD,EAAqB6E,EAAY7B,EAASL,QACrC,GAAuB,qBAAnByB,EAAMhY,SACfuW,EAAW,IAAInV,MAAMwV,GJ9GlB,SAAwB/R,EAAQlB,EAAOxD,GAC5C,MAAMwY,EAAa,IAAIrW,WAAWqB,GAClCqP,EAAkBnO,EAAQlB,EAAOgV,GACjC,MAAMC,EAAa,IAAItW,WAAWqB,GAClCqP,EAAkBnO,EAAQlB,EAAOiV,GAEjC,IAAK,IAAIrY,EAAI,EAAGA,EAAIoD,EAAOpD,IAAK,CAC9B,MAAMsY,EAAS,IAAIzY,WAAWyE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQ6T,EAAWrY,IACjGoY,EAAWpY,IAEbJ,EAAOI,GAAK,IAAIH,WAAWuY,EAAWpY,GAAKqY,EAAWrY,IACtDJ,EAAOI,GAAG0H,IAAI9H,EAAOI,EAAI,GAAGwU,SAAS,EAAG4D,EAAWpY,KACnDJ,EAAOI,GAAG0H,IAAI4Q,EAAQF,EAAWpY,KAEjCJ,EAAOI,GAAKsY,EAEdhU,EAAOE,QAAU6T,EAAWrY,EAC9B,CACF,CI6FIuY,CAAeL,EAAY7B,EAASL,OAC/B,IAAuB,sBAAnByB,EAAMhY,SAGf,MAAM,IAAI8B,MAAM,iCAAiCkW,EAAMhY,YAFvDuW,EAAW/B,GAAgB3P,EAAQ+R,EAAShW,EAAMH,EAAQiI,YAG5D,CAEA,MAAO,CAAE0H,mBAAkBC,mBAAkBkG,WAC/C,CCxLO,SAASwC,GAAWlU,GAAQmU,WAAEA,EAAUC,YAAEA,EAAWC,UAAEA,GAAajZ,EAAekZ,GACxF,MAAMC,WAAEA,GAAenZ,EAEjB4P,EAAS,GAEf,IAAI9P,EAEAsZ,EACAC,EAAW,EAEf,MAAMC,EAAgBJ,SACpBE,GAAaF,EAAO,CAClBC,aACAI,WAAYH,EACZI,SAAUT,EAAaM,EAAWD,EAAU/Y,OAC5CoZ,OAAQV,EAAaM,GAExB,GAED,KAAOA,EAAWJ,KACZrU,EAAOE,QAAUF,EAAOG,KAAKC,WAAa,IADnB,CAI3B,MAAMkP,EAASwF,GAAc9U,GAC7B,GAAoB,oBAAhBsP,EAAOvT,KAETb,EAAa6Z,GAAS/U,EAAQsP,EAAQlU,EAAeF,OAAY0N,EAAW,GAC5E1N,EAAaS,EAAQT,EAAYE,OAC5B,CACL,MAAM4Z,EAAkBR,GAAW/Y,QAAU,EACvCgG,EAASsT,GAAS/U,EAAQsP,EAAQlU,EAAeF,EAAYsZ,EAAWJ,EAAcK,GACxFD,IAAc/S,EAEhBgT,GAAYhT,EAAOhG,OAASuZ,GAE5BN,MACA1J,EAAO7L,KAAKsC,GACZgT,GAAYhT,EAAOhG,OACnB+Y,EAAY/S,EAEhB,CACF,CAOA,OANAiT,MAEID,EAAWJ,GAAaG,IAE1BxJ,EAAOA,EAAOvP,OAAS,GAAK+Y,EAAU3U,MAAM,EAAGwU,GAAaI,EAAWD,EAAU/Y,UAE5EuP,CACT,CAaO,SAAS+J,GAAS/U,EAAQsP,EAAQlU,EAAeF,EAAY+Z,EAAeC,GACjF,MAAMnZ,KAAEA,EAAIH,QAAEA,EAAO6D,WAAEA,EAAU8F,MAAEA,EAAK6M,YAAEA,GAAgBhX,EAEpD8W,EAAkB,IAAI3W,WAC1ByE,EAAOG,KAAK7C,OAAQ0C,EAAOG,KAAK5C,WAAayC,EAAOE,OAAQoP,EAAO6F,sBAKrE,GAHAnV,EAAOE,QAAUoP,EAAO6F,qBAGJ,cAAhB7F,EAAOvT,KAAsB,CAC/B,MAAM0V,EAAOnC,EAAO8F,iBACpB,IAAK3D,EAAM,MAAM,IAAIxU,MAAM,yCAG3B,GAAIiY,EAAYzD,EAAKjM,YXiClB,SAAsB/F,GAC3B,GAA0B,IAAtBA,EAAWhE,OAAc,OAAO,EACpC,MAAM,CAAGuJ,GAAUvF,EACnB,MAAuC,aAAnCuF,EAAOpJ,QAAQ+D,kBACfqF,EAAOnG,SAASpD,MAEtB,CWvCuC4Z,CAAa5V,GAC9C,OAAO,IAAIlD,MAAMkV,EAAKjM,YAGxB,MAAM6M,EAAOJ,GAAeC,EAAiBzX,OAAO6U,EAAO6C,wBAAyB5M,EAAO6M,IACrF7G,iBAAEA,EAAgBC,iBAAEA,EAAgBkG,SAAEA,GAAaF,GAAaa,EAAMZ,EAAMrW,GAIlF,IAAIqG,EAASzG,EAAsB0W,EAAUxW,EAAYuW,EAAKtW,SAAUC,GACxE,GAAIoQ,EAAiB/P,QAAU8P,GAAkB9P,OAAQ,CAEvD,OAAO6P,EADQ/O,MAAM+Y,QAAQL,GAAiBA,EAAgB,GACjC1J,EAAkBC,EAAkB/J,EAAQhC,EAC3E,CAEE,IAAK,IAAI/D,EAAI,EAAGA,EAAI+D,EAAWhE,OAAQC,IACS,aAA1C+D,EAAW/D,GAAGE,QAAQ+D,kBACxB8B,EAASlF,MAAMoB,KAAK8D,EAAQkD,GAAK,CAACA,KAGtC,OAAOlD,CAEX,CAAO,GAAoB,iBAAhB6N,EAAOvT,KAAyB,CACzC,MAAMoX,EAAQ7D,EAAO8D,oBACrB,IAAKD,EAAO,MAAM,IAAIlW,MAAM,4CAG5B,GAAIiY,EAAY/B,EAAMvO,SACpB,OAAO,IAAIrI,MAAM4W,EAAM3N,YAGzB,MAAM+F,iBAAEA,EAAgBC,iBAAEA,EAAgBkG,SAAEA,GAC1CuB,GAAef,EAAiB5C,EAAQlU,GAGpCqG,EAASzG,EAAsB0W,EAAUxW,EAAYiY,EAAMhY,SAAUC,GAE3E,OAAOkQ,EADQ/O,MAAM+Y,QAAQL,GAAiBA,EAAgB,GACjC1J,EAAkBC,EAAkB/J,EAAQhC,EAC3E,CAAO,GAAoB,oBAAhB6P,EAAOvT,KAA4B,CAC5C,MAAMwZ,EAAOjG,EAAOkG,uBACpB,IAAKD,EAAM,MAAM,IAAItY,MAAM,+CAE3B,MAAMoV,EAAOJ,GACXC,EAAiBzX,OAAO6U,EAAO6C,wBAAyB5M,EAAO6M,GAIjE,OAAOjC,GADQ,CAAEhQ,KAAM,IAAIyC,SAASyP,EAAK/U,OAAQ+U,EAAK9U,WAAY8U,EAAKjS,YAAaF,OAAQ,GACnEnE,EAAMwZ,EAAK/P,WAAY5J,EAAQiI,YAC1D,CACE,MAAM,IAAI5G,MAAM,kCAAkCqS,EAAOvT,OAE7D,CASA,SAAS+Y,GAAc9U,GACrB,MAAMsP,EAASvP,EAA4BC,GAsC3C,MAAO,CACLjE,KApCW3B,EAASkV,EAAO5L,SAqC3ByO,uBApC6B7C,EAAO3L,QAqCpCwR,qBApC2B7F,EAAOxL,QAqClC2R,IApCUnG,EAAOvL,QAqCjBqR,iBApCuB9F,EAAOtL,SAAW,CACzCwB,WAAY8J,EAAOtL,QAAQN,QAC3BvI,SAAUnB,EAASsV,EAAOtL,QAAQL,SAClC+R,0BAA2B1b,EAASsV,EAAOtL,QAAQF,SACnD6R,0BAA2B3b,EAASsV,EAAOtL,QAAQD,SACnDiC,WAAYsJ,EAAOtL,QAAQA,SAAW,CACpCvB,IAAK6M,EAAOtL,QAAQA,QAAQN,QAC5B2E,IAAKiH,EAAOtL,QAAQA,QAAQL,QAC5B2E,WAAYgH,EAAOtL,QAAQA,QAAQF,QACnCyE,eAAgB+G,EAAOtL,QAAQA,QAAQD,QACvCyE,UAAW8G,EAAOtL,QAAQA,QAAQA,QAClCyE,UAAW6G,EAAOtL,QAAQA,QAAQC,UA0BpC2R,kBAvBwBtG,EAAOrL,QAwB/BuR,uBAvB6BlG,EAAOpL,SAAW,CAC/CsB,WAAY8J,EAAOpL,QAAQR,QAC3BvI,SAAUnB,EAASsV,EAAOpL,QAAQP,SAClCkS,UAAWvG,EAAOpL,QAAQJ,SAqB1BsP,oBAnB0B9D,EAAOlL,SAAW,CAC5CoB,WAAY8J,EAAOlL,QAAQV,QAC3BmQ,UAAWvE,EAAOlL,QAAQT,QAC1BiB,SAAU0K,EAAOlL,QAAQN,QACzB3I,SAAUnB,EAASsV,EAAOlL,QAAQL,SAClCwP,8BAA+BjE,EAAOlL,QAAQJ,QAC9CqP,8BAA+B/D,EAAOlL,QAAQH,QAC9CyP,mBAA0C9K,IAA3B0G,EAAOlL,QAAQF,SAA+BoL,EAAOlL,QAAQF,QAC5E8B,WAAYsJ,EAAOlL,QAAQA,SAa/B,CCvHOlC,eAAe4T,IAAiBC,aAAEA,GAAgB3B,EAAaC,EAAWtP,EAASiR,GACxF,MAAMC,EAAY,IAAI1Z,MAAM8X,GAItB6B,QAAoBC,QAAQC,IAAIL,EAAanZ,IAAI,EAAG3B,UAAWA,EAAKyO,KAAKqB,KAGzEsL,EAAsBN,EACzBnZ,IAAIqC,GAASA,EAAMqX,aAAa,IAChC5R,OAAOxF,IAAS6F,GAAWA,EAAQwR,SAASrX,IACzCsX,EAAczR,GAAWsR,EACzBI,EAAgBD,EAAY5Z,IAAIsC,GAAQ6W,EAAaW,UAAU1R,GAAUA,EAAOsR,aAAa,KAAOpX,IAG1G,IAAK,IAAIyX,EAAMvC,EAAauC,EAAMtC,EAAWsC,IAC3C,GAAkB,WAAdX,EAAwB,CAG1B,MAAMY,EAAU,CAAA,EAChB,IAAK,IAAIlb,EAAI,EAAGA,EAAIqa,EAAata,OAAQC,IACvCkb,EAAQb,EAAara,GAAG4a,aAAa,IAAMJ,EAAYxa,GAAGib,GAE5DV,EAAUU,GAAOC,CACnB,KAAO,CAEL,MAAMA,EAAU,IAAIra,MAAMwZ,EAAata,QACvC,IAAK,IAAIC,EAAI,EAAGA,EAAI8a,EAAY/a,OAAQC,IAClC+a,EAAc/a,IAAM,IACtBkb,EAAQlb,GAAKwa,EAAYO,EAAc/a,IAAIib,IAG/CV,EAAUU,GAAOC,CACnB,CAEF,OAAOX,CACT,CCnGO/T,eAAe2U,GAAYC,GAEhCA,EAAQtT,iBAAmBrB,EAAqB2U,EAAQC,MAGxD,MAAMC,EA6DD,SAA0BF,GAC/B,IAAKA,EAAQtT,SAAU,MAAM,IAAIvG,MAAM,6BAIvC,MAAMga,ET5ED,UAAqBzT,SAAEA,EAAQoR,SAAEA,EAAW,EAACC,OAAEA,EAASrW,IAAQuG,QAAEA,IACvE,IAAKvB,EAAU,MAAM,IAAIvG,MAAM,iCAE/B,MAAMia,EAAS,GAETC,EAAU,GAGhB,IAAIhD,EAAa,EACjB,IAAK,MAAMrP,KAAYtB,EAASqB,WAAY,CAC1C,MAAMuS,EAAY3c,OAAOqK,EAASF,UAC5ByS,EAAWlD,EAAaiD,EAE9B,GAAIA,EAAY,GAAKC,GAAYzC,GAAYT,EAAaU,EAAQ,CAEhE,MAAMyC,EAAS,GAEf,IAAK,MAAMpS,UAAEA,EAASE,UAAEA,KAAeN,EAASC,QAAS,CACvD,GAAIG,EAAW,MAAM,IAAIjI,MAAM,mCAC/B,IAAKmI,EAAW,MAAM,IAAInI,MAAM,wCAE3B8H,IAAWA,EAAQwR,SAASnR,EAAUE,eAAe,KACxDgS,EAAOnY,KAAK+L,EAAe9F,GAE/B,CACA,MAAMgP,EAAc5R,KAAKC,IAAImS,EAAWT,EAAY,GAC9CE,EAAY7R,KAAK6F,IAAIwM,EAASV,EAAYiD,GAChDF,EAAO/X,KAAK,CAAEmY,SAAQxS,WAAUqP,aAAYiD,YAAWhD,cAAaC,cAGpE,MAAMkD,EAAYD,EAAOA,EAAO7b,OAAS,IAAI4P,QAAUiM,EAAO,IAAIlM,UAClE,IAAKrG,GAAWwS,EA3CS,SA6CvBJ,EAAQhY,KAAK,CACXiM,UAAWkM,EAAO,GAAGlM,UACrBC,QAASiM,EAAOA,EAAO7b,OAAS,GAAG4P,eAEhC,GAAIiM,EAAO7b,OAChBuN,EAAOmO,EAASG,QACX,GAAIvS,GAAStJ,OAClB,MAAM,IAAIwB,MAAM,8BAA8B8H,EAAQ2H,KAAK,QAE/D,CAEAyH,EAAakD,CACf,CAGA,OAFKG,SAAS3C,KAASA,EAASV,GAEzB,CAAE3Q,WAAUoR,WAAUC,SAAQ9P,UAASoS,UAASD,SACzD,CS2BeO,CAAYX,GAIzB,OAHAA,EAAQC,KTPH,SAA6BA,GAAMI,QAAEA,IAE1C,MAAMO,EAAWP,EAAQva,IAAI,EAAGwO,YAAWC,aAAc0L,EAAKlX,MAAMuL,EAAWC,IAC/E,MAAO,CACLjL,WAAY2W,EAAK3W,WACjB,KAAAP,CAAMsK,EAAOC,EAAM2M,EAAK3W,YAEtB,MAAMuX,EAAQR,EAAQT,UAAU,EAAGtL,YAAWC,aAAcD,GAAajB,GAASC,GAAOiB,GACzF,GAAIsM,EAAQ,EAAG,MAAM,IAAI1a,MAAM,0BAA0BkN,MAAUC,MACnE,GAAI+M,EAAQQ,GAAOvM,YAAcjB,GAASgN,EAAQQ,GAAOtM,UAAYjB,EAAK,CAExE,MAAMgF,EAAcjF,EAAQgN,EAAQQ,GAAOvM,UACrCwM,EAAYxN,EAAM+M,EAAQQ,GAAOvM,UACvC,OAAIsM,EAASC,aAAkBxB,QACtBuB,EAASC,GAAOjO,KAAKpM,GAAUA,EAAOuC,MAAMuP,EAAawI,IAEzDF,EAASC,GAAO9X,MAAMuP,EAAawI,EAE9C,CACE,OAAOF,EAASC,EAEpB,EAEJ,CShBiBE,CAAoBf,EAAQC,KAAME,GAG1CA,EAAKC,OAAOta,IAAIkb,GD7ElB,SAAsBhB,GAAStT,SAAEA,EAAQuB,QAAEA,GAAW+S,GAC3D,MAAMf,KAAEA,EAAI3E,YAAEA,EAAWtW,KAAEA,GAASgb,EAG9Bf,EAAe,GAEfla,EAAU,IAAKxB,KAAoByc,EAAQjb,SAGjD,IAAK,MAAMqJ,UAAEA,EAASE,UAAEA,KAAe0S,EAAUhT,SAASC,QAAS,CACjE,GAAIG,EAAW,MAAM,IAAIjI,MAAM,mCAC/B,IAAKmI,EAAW,MAAM,IAAInI,MAAM,wCAGhC,MAAMsX,EAAanP,EAAUE,eAAe,GAC5C,GAAIP,IAAYA,EAAQwR,SAAShC,GAAa,SAE9C,MAAMnJ,UAAEA,EAASC,QAAEA,GAAYH,EAAe9F,GACxC2S,EAAc1M,EAAUD,EAI9B,GAAI2M,EAAc,GAAK,GAAI,CACzBC,QAAQC,KAAK,iCAAiC7S,EAAUE,mBAAmByS,WAE3E,QACF,CAIA,MAAMza,EAAS6Y,QAAQ+B,QAAQnB,EAAKlX,MAAMuL,EAAWC,IAGrD0K,EAAa5W,KAAK,CAChBmX,aAAclR,EAAUE,eACxBrK,KAAMqC,EAAOoM,KAAKpG,IAChB,MAAM7D,EAAaL,EAAcoE,EAAS9E,OAAQ0G,EAAUE,gBACtDtF,EAAS,CAAEG,KAAM,IAAIyC,SAASU,GAAcpD,OAAQ,GAEpD9E,EAAgB,CACpBmZ,WAFgBnP,EAAUE,eAAeoH,KAAK,KAG9C3Q,KAAMqJ,EAAUrJ,KAChBH,QAAS6D,EAAWA,EAAWhE,OAAS,GAAGG,QAC3C6D,aACA8F,MAAOH,EAAUG,MACjB1J,UACAuW,cACAtW,QAEF,OAAOoY,GAAWlU,EAAQ8X,EAAW1c,EAAe0b,EAAQxC,WAGlE,CAEA,MAAO,CAAEH,WAAY2D,EAAU3D,WAAYiD,UAAWU,EAAUV,UAAWrB,eAC7E,CCsBsCoC,CAAarB,EAASG,EAAMa,GAClE,CAvEsBM,CAAiBtB,IAE/BlC,SAAEA,EAAW,EAACC,OAAEA,EAAM9P,QAAEA,EAAOsT,QAAEA,EAAOC,WAAEA,EAAUtC,UAAEA,GAAcc,EAG1E,IAAKwB,IAAeD,EAAS,CAC3B,IAAK,MAAMtC,aAAEA,KAAkBiB,EAC7B,IAAK,MAAM/b,KAAEA,KAAU8a,QAAoB9a,EAE7C,MACF,CAGA,MAAMwD,EX8JD,UAAuBC,OAAEA,IAC9B,OAAOU,EAAcV,EAAQ,IAAI,EACnC,CWhKqB6Z,CAAczB,EAAQtT,UACnCgV,EAAYxB,EAAYpa,IAAI6b,GDyF7B,SAAuBC,EAAeja,GAC3C,MAAMsX,aAAEA,GAAiB2C,EAEnBF,EAAY,GAClB,IAAK,MAAMvZ,KAASR,EAAWI,SAC7B,GAAII,EAAMJ,SAASpD,OAAQ,CACzB,MAAMkd,EAAe5C,EAAarR,OAAOM,GAAUA,EAAOsR,aAAa,KAAOrX,EAAMrD,QAAQsD,MAC5F,IAAKyZ,EAAald,OAAQ,SAI1B,MAAMmd,EAAW,IAAIjO,IACf1P,EAAOkb,QAAQC,IAAIuC,EAAa/b,IAAIoI,GACjCA,EAAO/J,KAAKyO,KAAKiL,IACtBiE,EAASxV,IAAI4B,EAAOsR,aAAa5J,KAAK,KAAM3B,EAAQ4J,QAEpDjL,KAAK,KAEP6C,EAAeqM,EAAU3Z,GACzB,MAAM4Z,EAAaD,EAAS7O,IAAI9K,EAAML,KAAK8N,KAAK,MAChD,IAAKmM,EAAY,MAAM,IAAI5b,MAAM,qCACjC,MAAO,CAAC4b,KAGVL,EAAUrZ,KAAK,CAAEmX,aAAcrX,EAAML,KAAM3D,QAC7C,KAAO,CAEL,MAAM6d,EAAc/C,EAAaxW,KAAKyF,GAAUA,EAAOsR,aAAa,KAAOrX,EAAMrD,QAAQsD,MACrF4Z,GACFN,EAAUrZ,KAAK2Z,EAEnB,CAEF,MAAO,IAAKJ,EAAe3C,aAAcyC,EAC3C,CC3H2CO,CAAcN,EAAKha,IAG5D,GAAI4Z,EACF,IAAK,MAAMW,KAAcR,EACvB,IAAK,MAAMM,KAAeE,EAAWjD,aACnC+C,EAAY7d,KAAKyO,KAAKwM,IACpB,IAAItB,EAAWoE,EAAW7E,WAC1B,IAAK,MAAMQ,KAAcuB,EACvBmC,EAAQ,CACN9D,WAAYuE,EAAYxC,aAAa,GACrC3B,aACAC,WACAC,OAAQD,EAAWD,EAAWlZ,SAEhCmZ,GAAYD,EAAWlZ,SAQjC,GAAI6c,EAAY,CAEd,MAAMW,EAAO,GACb,IAAK,MAAMD,KAAcR,EAAW,CAElC,MAAMpE,EAAc5R,KAAKC,IAAImS,EAAWoE,EAAW7E,WAAY,GACzDE,EAAY7R,KAAK6F,KAAKwM,GAAUrW,KAAYwa,EAAW7E,WAAY6E,EAAW5B,WAGpFpO,EAAOiQ,SADiBnD,GAAiBkD,EAAY5E,EAAaC,EAAWtP,EAASiR,IAC/DnW,MAAMuU,EAAaC,GAC5C,CACAiE,EAAWW,EACb,MAEE,IAAK,MAAMlD,aAAEA,KAAkByC,EAC7B,IAAK,MAAMvd,KAAEA,KAAU8a,QAAoB9a,CAGjD,CC/CO,SAASie,GAAUC,GACxB,MAAMC,EAAK,IAAIxW,SAASuW,EAAI7b,OAAQ6b,EAAI5b,WAAY4b,EAAI/Y,YACxD,IAAIF,EAAS,EAGb,MAAMmZ,EAAYF,EAAIjZ,GAASA,GAAU,EACzC,MAAMoZ,EAA+B,IAAdD,EAGjBE,EAAeH,EAAGvW,UAAU3C,EAAQoZ,GAI1C,GAHApZ,GAAU,EA1Cc,IA6CpBqZ,EAAoC,CAEtC,MAAMvH,EAAIoH,EAAGtY,WAAWZ,EAAQoZ,GAAiBpZ,GAAU,EAC3D,MAAMsZ,EAAIJ,EAAGtY,WAAWZ,EAAQoZ,GAChC,OADiDpZ,GAAU,EACpD,CAAEnE,KAAM,QAAS0d,YAAa,CAACzH,EAAGwH,GAC3C,CAAO,GAjDsB,IAiDlBD,EAAyC,CAElD,MAAMG,EAAYN,EAAGvW,UAAU3C,EAAQoZ,GAAiBpZ,GAAU,EAClE,MAAMyZ,EAAS,GACf,IAAK,IAAIje,EAAI,EAAGA,EAAIge,EAAWhe,IAAK,CAClC,MAAMsW,EAAIoH,EAAGtY,WAAWZ,EAAQoZ,GAAiBpZ,GAAU,EAC3D,MAAMsZ,EAAIJ,EAAGtY,WAAWZ,EAAQoZ,GAAiBpZ,GAAU,EAC3DyZ,EAAOxa,KAAK,CAAC6S,EAAGwH,GAClB,CACA,MAAO,CAAEzd,KAAM,aAAc0d,YAAaE,EAC5C,CAAO,GA1DmB,IA0DfJ,EAAsC,CAE/C,MAAMK,EAAWR,EAAGvW,UAAU3C,EAAQoZ,GAAiBpZ,GAAU,EACjE,MAAMyZ,EAAS,GACf,IAAK,IAAIE,EAAI,EAAGA,EAAID,EAAUC,IAAK,CACjC,MAAMH,EAAYN,EAAGvW,UAAU3C,EAAQoZ,GAAiBpZ,GAAU,EAClE,MAAM4Z,EAAO,GACb,IAAK,IAAIC,EAAI,EAAGA,EAAIL,EAAWK,IAAK,CAClC,MAAM/H,EAAIoH,EAAGtY,WAAWZ,EAAQoZ,GAAiBpZ,GAAU,EAC3D,MAAMsZ,EAAIJ,EAAGtY,WAAWZ,EAAQoZ,GAAiBpZ,GAAU,EAC3D4Z,EAAK3a,KAAK,CAAC6S,EAAGwH,GAChB,CACAG,EAAOxa,KAAK2a,EACd,CACA,MAAO,CAAE/d,KAAM,UAAW0d,YAAaE,EACzC,CAAO,GAtEwB,IAsEpBJ,EAA2C,CAEpD,MAAMS,EAAcZ,EAAGvW,UAAU3C,EAAQoZ,GAAiBpZ,GAAU,EACpE,MAAM+Z,EAAW,GACjB,IAAK,IAAIve,EAAI,EAAGA,EAAIse,EAAate,IAAK,CAEpC,MAAMwe,EAAqC,IAAhBf,EAAIjZ,GAAeA,GAAU,EACxD,MAAMia,EAAWf,EAAGvW,UAAU3C,EAAQga,GACtC,GAD2Dha,GAAU,EAhF/C,IAiFlBia,EACF,MAAM,IAAIld,MAAM,yCAAyCkd,KAE3D,MAAMP,EAAWR,EAAGvW,UAAU3C,EAAQga,GAAqBha,GAAU,EAErE,MAAMka,EAAW,GACjB,IAAK,IAAIP,EAAI,EAAGA,EAAID,EAAUC,IAAK,CACjC,MAAMH,EAAYN,EAAGvW,UAAU3C,EAAQga,GAAqBha,GAAU,EACtE,MAAM4Z,EAAO,GACb,IAAK,IAAIC,EAAI,EAAGA,EAAIL,EAAWK,IAAK,CAClC,MAAM/H,EAAIoH,EAAGtY,WAAWZ,EAAQga,GAAqBha,GAAU,EAC/D,MAAMsZ,EAAIJ,EAAGtY,WAAWZ,EAAQga,GAAqBha,GAAU,EAC/D4Z,EAAK3a,KAAK,CAAC6S,EAAGwH,GAChB,CACAY,EAASjb,KAAK2a,EAChB,CACAG,EAAS9a,KAAKib,EAChB,CACA,MAAO,CAAEre,KAAM,eAAgB0d,YAAaQ,EAC9C,CAAO,GAnGsB,IAmGlBV,EAAyC,CAElD,MAAMG,EAAYN,EAAGvW,UAAU3C,EAAQoZ,GAAiBpZ,GAAU,EAClE,MAAMma,EAAS,GACf,IAAK,IAAI3e,EAAI,EAAGA,EAAIge,EAAWhe,IAAK,CAElC,MAAM4e,EAAsC,IAAhBnB,EAAIjZ,GAAeA,GAAU,EACzD,MAAMqa,EAAYnB,EAAGvW,UAAU3C,EAAQoa,GACvC,GAD6Dpa,GAAU,EA7GnD,IA8GhBqa,EACF,MAAM,IAAItd,MAAM,qCAAqCsd,KAEvD,MAAMvI,EAAIoH,EAAGtY,WAAWZ,EAAQoa,GAAsBpa,GAAU,EAChE,MAAMsZ,EAAIJ,EAAGtY,WAAWZ,EAAQoa,GAAsBpa,GAAU,EAChEma,EAAOlb,KAAK,CAAC6S,EAAGwH,GAClB,CACA,MAAO,CAAEzd,KAAM,aAAc0d,YAAaY,EAC5C,CAAO,GAlH2B,IAkHvBd,EAA8C,CAEvD,MAAMiB,EAAiBpB,EAAGvW,UAAU3C,EAAQoZ,GAAiBpZ,GAAU,EACvE,MAAMua,EAAc,GACpB,IAAK,IAAI/e,EAAI,EAAGA,EAAI8e,EAAgB9e,IAAK,CAEvC,MAAMgf,EAAqC,IAAhBvB,EAAIjZ,GAAeA,GAAU,EACxD,MAAMya,EAAWvB,EAAGvW,UAAU3C,EAAQwa,GACtC,GAD2Dxa,GAAU,EA5H5C,IA6HrBya,EACF,MAAM,IAAI1d,MAAM,+CAA+C0d,KAEjE,MAAMjB,EAAYN,EAAGvW,UAAU3C,EAAQoZ,GAAiBpZ,GAAU,EAClE,MAAMyZ,EAAS,GACf,IAAK,IAAII,EAAI,EAAGA,EAAIL,EAAWK,IAAK,CAClC,MAAM/H,EAAIoH,EAAGtY,WAAWZ,EAAQwa,GAAqBxa,GAAU,EAC/D,MAAMsZ,EAAIJ,EAAGtY,WAAWZ,EAAQwa,GAAqBxa,GAAU,EAC/DyZ,EAAOxa,KAAK,CAAC6S,EAAGwH,GAClB,CACAiB,EAAYtb,KAAKwa,EACnB,CACA,MAAO,CAAE5d,KAAM,kBAAmB0d,YAAagB,EACjD,CACE,MAAM,IAAIxd,MAAM,8BAA8Bsc,IAElD,CChIOrX,eAAe0Y,IAAU7D,KAAEA,EAAI3E,YAAEA,IACtC,MAAM5O,QAAiBrB,EAAqB4U,GACtC8D,EAAcrX,EAASmC,oBAAoBpG,KAAKub,GAAiB,QAAXA,EAAGhT,KAC/D,IAAK+S,EACH,MAAM,IAAI5d,MAAM,mDAIlB,MAAM8d,EAAYje,KAAKC,MAAM8d,EAAY7c,OAAS,MAG5C/C,QFwG2B6b,EExGK,CAAEC,OAAMvT,WAAU1H,MAAM,EAAOsW,eFyG9D,IAAI+D,QAAQ,CAACmC,EAAY0C,KAC9BnE,GAAY,CACVb,UAAW,YACRc,EACHwB,eACC2C,MAAMD,MANN,IAA4BlE,EErGjC,MAAMoE,EAAW,GACXC,EAAgBJ,EAAUK,gBAAkB,WAClD,IAAK,MAAMzE,KAAO1b,EAAM,CACtB,MAAMke,EAAMxC,EAAIwE,GAChB,IAAKhC,EAEH,SAGF,MAAMkC,EAAWnC,GAAUC,GAIrBmC,EAAa,CAAA,EACnB,IAAK,MAAMxT,KAAOoG,OAAOV,KAAKmJ,GAAM,CAClC,MAAM3Y,EAAQ2Y,EAAI7O,GACdA,IAAQqT,GAA2B,OAAVnd,IAC3Bsd,EAAWxT,GAAO9J,EAEtB,CAGA,MAAMud,EAAU,CACdxf,KAAM,UACNsf,WACAC,cAGFJ,EAAS/b,KAAKoc,EAChB,CAEA,MAAO,CACLxf,KAAM,oBACNmf,WAEJ,EC7DAhZ,iBAEE,MAAMyI,IAAEA,SAAc6Q,OAAOC,KAAKC,cAAc,QAG1C9e,EAAM,IAAI+N,EAFsBgR,SAASC,eAAe,OAErC,CACvBC,OAAQ,CAAEC,IAAK,GAAIC,KAAK,IACxBC,KAAM,IAMR,IAEE,MAAMjF,EAAOvM,QACLrB,EAAmB,CAAEC,IALZ,0DAK6BhJ,WAAY,SAE1D4X,QAAQiE,IAAI,mBAAoBlF,GAChC,MAAMmF,QAAgBtB,GAAU,CAAE7D,SAElCiB,QAAQiE,IAAI,WAAYC,GAGxBtf,EAAI3B,KAAKkhB,WAAWD,EACtB,CAAE,MAAOE,GACPpE,QAAQoE,MAAM,4CAA6CA,EAC7D,CACF,CACAC","x_google_ignoreList":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]}